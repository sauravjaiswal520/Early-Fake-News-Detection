{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EarlyFakeNewsDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2epcw6DNCC3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from itertools import chain\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8fLIb6pNGej"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn.utils as utils"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoSEjCSsNJWH"
      },
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd6dj3fONedR"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA5VYvMANfCU"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBAhdUpeNgdj"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbD1OueSNh8q"
      },
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(RNNEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_encoder = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, h_0: torch.Tensor)->torch.Tensor:\n",
        "        # x.shape: Batch x sequence length x input_size\n",
        "        # h_0.shape:\n",
        "        # output.shape: batch x sequence length x input_size\n",
        "        output, h_n = self.rnn_encoder(x, h_0)\n",
        "\n",
        "        # output.shape: batch x input_size\n",
        "        output = output.mean(dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, out_channels: int, kernel_size: tuple):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        self.cnn_encoder = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=kernel_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x.shape: batch x sequence length x kernel_size[1](input_size)\n",
        "        # x.shape: batch x 1 x sequence length x kernel_size[1]\n",
        "        x = x.unsqueeze(dim=1)\n",
        "        # output.shape: batch x out_channels x sequence length - kernel_size[0] + 1\n",
        "        output = F.relu(self.cnn_encoder(x))\n",
        "        # output.shape: batch x out_channels\n",
        "        output = output.mean(dim=2)\n",
        "        return output\n",
        "\n",
        "\n",
        "class DetectModel(nn.Module):\n",
        "    def __init__(self, input_size,\n",
        "                 hidden_size, rnn_layers,\n",
        "                 out_channels, height, cnn_layers,\n",
        "                 linear_hidden_size, linear_layers, output_size):\n",
        "        super(DetectModel, self).__init__()\n",
        "        self.rnn_encoder = RNNEncoder(input_size=input_size, hidden_size=hidden_size, num_layers=rnn_layers)\n",
        "        self.cnn_encoder = CNNEncoder(out_channels=out_channels, kernel_size=(height, input_size))\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(hidden_size + out_channels, linear_hidden_size), nn.ReLU(inplace=True),\n",
        "            *chain(*[(nn.Linear(linear_hidden_size, linear_hidden_size), nn.ReLU(inplace=True)) for i in range(linear_layers - 2)]),\n",
        "            nn.Linear(linear_hidden_size, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, h0):\n",
        "        # h0 for rnn_encoder\n",
        "        \n",
        "        rnn_output = self.rnn_encoder(x, h0)\n",
        "        cnn_output = self.cnn_encoder(x)\n",
        "        cnn_output = cnn_output.squeeze()\n",
        "       \n",
        "        # output.shape: batch x (hidden_size + out_channels)\n",
        "        # output.shape: batch x output_size\n",
        "        output = torch.cat([rnn_output, cnn_output], dim=1)\n",
        "        output = self.linear(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqEGl0u1NkCW"
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.best_acc = 0\n",
        "        self.patience = 0\n",
        "        self.init_clip_max_norm = 5.0\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def fit(self, X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred,\n",
        "            X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev):\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "\n",
        "        batch_size = self.config['batch_size']\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.config['lr'], weight_decay=self.config['reg'], amsgrad=True) #\n",
        "        # self.optimizer = torch.optim.Adadelta(self.parameters(), weight_decay=self.config['reg'])\n",
        "\n",
        "        X_train_source_wid = torch.LongTensor(X_train_source_wid)\n",
        "        X_train_source_id = torch.LongTensor(X_train_source_id)\n",
        "        X_train_user_id = torch.LongTensor(X_train_user_id)\n",
        "        X_train_ruid = torch.LongTensor(X_train_ruid)\n",
        "        y_train = torch.LongTensor(y_train)\n",
        "        y_train_cred = torch.LongTensor(y_train_cred)\n",
        "        y_train_rucred = torch.LongTensor(y_train_rucred)\n",
        " \n",
        "        dataset = TensorDataset(X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # UPDATE\n",
        "\n",
        "        model1 = DetectModel(config['input_size'],\n",
        "                      config['hidden_size'], config['rnn_layers'],\n",
        "                      config['out_channels'], config['height'], config['cnn_layers'],\n",
        "                      config['linear_hidden_size'], config['linear_layers'], config['output_size'])\n",
        "        \n",
        "        \n",
        "        \n",
        "        model1 = model1.to(self.device)\n",
        "        \n",
        "        self.optimizer1 = torch.optim.Adam(params= model1.parameters(), lr=config['lr'])\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=3)\n",
        "        self.initial_hidden_state = torch.zeros(config['rnn_layers'], config['batch_size'], config['hidden_size'], dtype=torch.float, requires_grad=False).to(self.device)\n",
        "\n",
        "        \n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        loss_func2 = nn.CrossEntropyLoss(ignore_index=3) # kya h \n",
        "        for epoch in range(1, self.config['epochs']+1):\n",
        "            print(\"\\nEpoch \", epoch, \"/\", self.config['epochs'])\n",
        "            self.train()\n",
        "            avg_loss = 0\n",
        "            avg_acc = 0\n",
        "            for i, data in enumerate(dataloader):\n",
        "                with torch.no_grad():\n",
        "                    X_source_wid, X_source_id, X_user_id, X_ruid, batch_y, batch_y_cred, batch_y_rucred = (item.cuda(device=self.device) for item in data)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                logit, ulogit, rulogit, r = self.forward(X_source_wid, X_source_id, X_user_id, X_ruid)\n",
        "\n",
        "                output = model1(r,  self.initial_hidden_state[:, :r.shape[0], :])\n",
        "                loss3 = self.criterion(output,  batch_y)\n",
        "                \n",
        "                \n",
        "                loss1 = loss_func(logit, batch_y)\n",
        "                pub_loss = loss_func(ulogit, batch_y_cred)\n",
        "                uloss = loss_func2(rulogit.view(-1, rulogit.size(-1)), batch_y_rucred.view(-1))\n",
        "                loss = loss1 + pub_loss + uloss + loss3\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                corrects = (torch.max(logit, 1)[1].view(batch_y.size()).data == batch_y.data).sum()\n",
        "                accuracy = 100*corrects/len(batch_y)\n",
        "                print('Batch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(i, loss.item(), accuracy, corrects, batch_y.size(0)))\n",
        "\n",
        "                avg_loss += loss.item()\n",
        "                avg_acc += accuracy\n",
        "\n",
        "                if self.init_clip_max_norm is not None:\n",
        "                    utils.clip_grad_norm_(self.parameters(), max_norm=self.init_clip_max_norm)\n",
        "\n",
        "            cnt = y_train.size(0) // batch_size + 1\n",
        "            print(\"Average loss:{:.6f} average acc:{:.6f}%\".format(avg_loss/cnt, avg_acc/cnt))\n",
        "            if epoch > self.config['epochs']//2 and self.patience > 2: #\n",
        "                print(\"Reload the best model...\")\n",
        "                self.load_state_dict(torch.load(self.config['save_path']))\n",
        "                now_lr = self.adjust_learning_rate(self.optimizer)\n",
        "                print(now_lr)\n",
        "                self.patience = 0\n",
        "            self.evaluate(X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev, epoch)\n",
        "\n",
        "\n",
        "    def adjust_learning_rate(self, optimizer, decay_rate=.5):\n",
        "        now_lr = 0\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * decay_rate\n",
        "            now_lr = param_group['lr']\n",
        "        return now_lr\n",
        "\n",
        "\n",
        "    def evaluate(self, X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev, epoch):\n",
        "        y_pred = self.predict(X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid)\n",
        "        acc = accuracy_score(y_dev, y_pred)\n",
        "        print(\"Val set acc:\", acc)\n",
        "        print(\"Best val set acc:\", self.best_acc)\n",
        "\n",
        "        if epoch >= self.config['epochs']//2 and acc > self.best_acc:  #\n",
        "            self.best_acc = acc\n",
        "            self.patience = 0\n",
        "            torch.save(self.state_dict(), self.config['save_path'])\n",
        "            print(classification_report(y_dev, y_pred, target_names=self.config['target_names'], digits=5))\n",
        "            print(\"save model!!!\")\n",
        "        else:\n",
        "            self.patience += 1\n",
        "\n",
        "\n",
        "    def predict(self, X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid):\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "\n",
        "        self.eval()\n",
        "        y_pred = []\n",
        "        X_dev_source_wid = torch.LongTensor(X_dev_source_wid)\n",
        "        X_dev_source_id = torch.LongTensor(X_dev_source_id)\n",
        "        X_dev_user_id = torch.LongTensor(X_dev_user_id)\n",
        "        X_dev_ruid = torch.LongTensor(X_dev_ruid)\n",
        "\n",
        "        dataset = TensorDataset(X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid)\n",
        "        dataloader = DataLoader(dataset, batch_size=32)\n",
        "\n",
        "        for i, data in enumerate(dataloader):\n",
        "            with torch.no_grad():\n",
        "                X_source_wid, X_source_id, X_user_id, \\\n",
        "                X_ruid = (item.cuda(device=self.device) for item in data)\n",
        "\n",
        "            \n",
        "            logits, _, _, r = self.forward(X_source_wid, X_source_id, X_user_id, X_ruid)\n",
        "            \n",
        "            predicted = torch.max(logits, dim=1)[1]\n",
        "            y_pred += predicted.data.cpu().numpy().tolist()\n",
        "        return y_pred\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dGqsVczNkFN"
      },
      "source": [
        "class PGAN(NeuralNetwork):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(PGAN, self).__init__()\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.config = config\n",
        "        embedding_weights = config['embedding_weights']\n",
        "        V, D = embedding_weights.shape\n",
        "        self.n_heads = config['n_heads']\n",
        "\n",
        "        self.A_us = config['A_us']\n",
        "        self.A_uu = config['A_uu']\n",
        "        embeding_size = config['embeding_size']\n",
        "\n",
        "        self.word_embedding = nn.Embedding(V, D, padding_idx=0, _weight=torch.from_numpy(embedding_weights)) # V embeddings of 300 Size.\n",
        "        self.user_embedding = nn.Embedding(config['A_us'].shape[0], embeding_size, padding_idx=0) # Number of users embedding of 100 size.\n",
        "        self.source_embedding = nn.Embedding(config['A_us'].shape[1], embeding_size)\n",
        "        self.up_embed = nn.Embedding(config['A_us'].shape[0], 100, padding_idx=0)\n",
        "\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(300, 100, kernel_size=K) for K in config['kernel_sizes']])\n",
        "        self.max_poolings = nn.ModuleList([nn.MaxPool1d(kernel_size=config['maxlen'] - K + 1) for K in config['kernel_sizes']])\n",
        "\n",
        "        self.Wcm = [nn.Parameter(torch.FloatTensor(embeding_size, embeding_size)).cuda() for _ in\n",
        "                    range(self.n_heads)] # 10 Layers of size 100 * 100\n",
        "        self.Wam = [nn.Parameter(torch.FloatTensor(embeding_size, embeding_size)).cuda() for _ in\n",
        "                    range(self.n_heads)]\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([embeding_size])).cuda() #  // self.n_heads\n",
        "\n",
        "        self.W1 = nn.Parameter(torch.FloatTensor(embeding_size * self.n_heads, embeding_size)) # 1000 * 100\n",
        "        self.W2 = nn.Parameter(torch.FloatTensor(embeding_size * self.n_heads, embeding_size))   # 1000 * 100\n",
        "        self.linear = nn.Linear(400, 200)  # 400 Input features and gives 200 Output Features.\n",
        "        # Activation Function\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        self.relu = nn.ReLU()\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(300 + 2 * embeding_size, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config['dropout']),\n",
        "            nn.Linear(100, config[\"num_classes\"])\n",
        "        )\n",
        "        self.fc_user_out = nn.Sequential(\n",
        "            nn.Linear(embeding_size, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config['dropout']),\n",
        "            nn.Linear(100, 3)\n",
        "        )\n",
        "        self.fc_ruser_out = nn.Sequential(\n",
        "            nn.Linear(embeding_size, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config['dropout']),\n",
        "            nn.Linear(100, 3)\n",
        "        )\n",
        "        print(self)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init.xavier_normal_(self.user_embedding.weight)\n",
        "        init.xavier_normal_(self.source_embedding.weight)\n",
        "        for i in range(self.n_heads):\n",
        "            init.xavier_normal_(self.Wcm[i])\n",
        "            init.xavier_normal_(self.Wam[i])\n",
        "\n",
        "        init.xavier_normal_(self.W1)\n",
        "        init.xavier_normal_(self.W2)\n",
        "        init.xavier_normal_(self.linear.weight)\n",
        "        for name, param in self.fc_out.named_parameters():\n",
        "            if name.__contains__(\"weight\"):\n",
        "                init.xavier_normal_(param)\n",
        "        for name, param in self.fc_user_out.named_parameters():\n",
        "            if name.__contains__(\"weight\"):\n",
        "                init.xavier_normal_(param)\n",
        "        for name, param in self.fc_ruser_out.named_parameters():\n",
        "            if name.__contains__(\"weight\"):\n",
        "                init.xavier_normal_(param)\n",
        "\n",
        "    def user_multi_head(self, X_user, X_user_id, Wcm):\n",
        "        M = self.source_embedding.weight\n",
        "        linear1 = torch.einsum(\"bd,dd,sd->bs\", X_user, Wcm, M) / self.scale # Doubt h \n",
        "        linear1 = self.relu(linear1) # (batch, |S|)\n",
        "\n",
        "        A_us = self.A_us[X_user_id.cpu(), :].todense()\n",
        "        A_us = torch.FloatTensor(A_us).cuda()   # (batch, |S|)\n",
        "\n",
        "        alpha = F.softmax(linear1 * A_us, dim=-1)\n",
        "        alpha = self.dropout(alpha)\n",
        "        return alpha.matmul(M)\n",
        "\n",
        "    def retweet_user_multi_head(self, X_ruser, X_ruser_id, Wam):\n",
        "        M = self.user_embedding.weight\n",
        "        linear1 = torch.einsum(\"bnd,dd,md->bnm\", X_ruser, Wam, M) / self.scale # m x bsz\n",
        "        linear1 = self.relu(linear1)\n",
        "\n",
        "        s1, s2 = X_ruser_id.size()\n",
        "        idx = X_ruser_id.view(-1).cpu()\n",
        "        A_uu = self.A_uu[idx, :].todense()\n",
        "        A_uu = torch.FloatTensor(A_uu).view(s1, s2, -1).cuda()\n",
        "\n",
        "        alpha = F.softmax(linear1 * A_uu, dim=-1)  # m x bsz\n",
        "        alpha = self.dropout(alpha)\n",
        "        return alpha.matmul(M)\n",
        "\n",
        "    def publisher_encoder(self, X_user, X_user_id):\n",
        "        m_hat = []\n",
        "        for i in range(self.n_heads):\n",
        "            m_hat.append(self.user_multi_head(X_user, X_user_id, self.Wcm[i]))\n",
        "\n",
        "        m_hat = torch.cat(m_hat, dim=-1).matmul(self.W1)\n",
        "        m_hat = self.elu(m_hat)\n",
        "        m_hat = self.dropout(m_hat)\n",
        "\n",
        "        U_hat = m_hat + X_user  # bsz x d\n",
        "        return U_hat\n",
        "\n",
        "    def retweet_user_encoder(self, X_ruser, X_ruser_id):  # 0.854  0.878\n",
        "        '''\n",
        "        :param X_ruser:  (bsz, num_users, d)\n",
        "        :param X_ruser_id: (bsz, num_users)\n",
        "        :return:\n",
        "        '''\n",
        "        m_hat = []\n",
        "        for i in range(self.n_heads):\n",
        "            m_hat.append(self.retweet_user_multi_head(X_ruser, X_ruser_id, self.Wam[i]))\n",
        "        m_hat = torch.cat(m_hat, dim=-1).matmul(self.W2)\n",
        "        m_hat = self.elu(m_hat)\n",
        "        m_hat = self.dropout(m_hat)\n",
        "\n",
        "        a_hat = m_hat + X_ruser  # bsz x 20 x d\n",
        "        return a_hat\n",
        "\n",
        "    def source_encoder(self, X_source, r_user_rep, user_rep):  #\n",
        "        linear1 = torch.einsum(\"bd,bnd->bn\", X_source, r_user_rep) # / self.scale\n",
        "        alpha = F.softmax(linear1, dim=-1)\n",
        "        retweet_rep = torch.einsum(\"bn,bnd->bd\", alpha, r_user_rep)\n",
        "\n",
        "        # beta = 0.5\n",
        "        source_rep = torch.cat([retweet_rep, user_rep,\n",
        "                                retweet_rep * user_rep,\n",
        "                                retweet_rep - user_rep], dim=-1)  # .mm(self.W) #\n",
        "        source_rep = self.linear(source_rep)\n",
        "        source_rep = self.dropout(source_rep)\n",
        "        return source_rep\n",
        "\n",
        "    def text_representation(self, X_word):\n",
        "        X_word = X_word.permute(0, 2, 1)\n",
        "        conv_block = []\n",
        "        for Conv, max_pooling in zip(self.convs, self.max_poolings):\n",
        "            act = self.relu(Conv(X_word))\n",
        "            pool = max_pooling(act).squeeze()\n",
        "            conv_block.append(pool)\n",
        "\n",
        "        features = torch.cat(conv_block, dim=1)\n",
        "        features = self.dropout(features)\n",
        "        return features\n",
        "\n",
        "    def forward(self, X_source_wid, X_source_id, X_user_id, X_ruser_id):  # , X_composer_id, X_reviewer_id\n",
        "        '''\n",
        "        :param X_source_wid size: (batch_size, max_words)\n",
        "                X_source_id size: (batch_size, )\n",
        "                X_user_id  size: (batch_size, )\n",
        "                X_retweet_id  size: (batch_size, max_retweets)\n",
        "                X_retweet_uid  size: (batch_size, max_retweets)\n",
        "\n",
        "        :return:\n",
        "        '''\n",
        "        X_word = self.word_embedding(X_source_wid)\n",
        "        X_user = self.user_embedding(X_user_id)\n",
        "        X_ruser = self.user_embedding(X_ruser_id)\n",
        "        X_source = self.source_embedding(X_source_id)\n",
        "        R = self.up_embed(X_ruser_id)\n",
        "        X_text = self.text_representation(X_word)\n",
        "\n",
        "        user_rep = self.publisher_encoder(X_user, X_user_id)\n",
        "        r_user_rep = self.retweet_user_encoder(X_ruser, X_ruser_id)  #\n",
        "        source_rep = self.source_encoder(X_source, r_user_rep, user_rep)  #\n",
        "        r_rep = self.retweet_user_encoder(R, X_ruser_id)\n",
        "        tweet_rep = torch.cat([X_text, source_rep], dim=-1)\n",
        "\n",
        "        Xt_logit = self.fc_out(tweet_rep)\n",
        "        Xu_logit = self.fc_user_out(user_rep)\n",
        "        Xru_logit = self.fc_ruser_out(r_user_rep)\n",
        "\n",
        "        return Xt_logit, Xu_logit, Xru_logit, r_rep\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V7bENcQNkH9",
        "outputId": "98c7e475-a7f9-4fcd-e797-2af3bacf1950"
      },
      "source": [
        "def load_dataset(task):\n",
        "    print(\"task: \", task)\n",
        "\n",
        "    A_us, A_uu = pickle.load(open(\"/content/drive/MyDrive/twitter15/relations.pkl\", 'rb'))\n",
        "    X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred, word_embeddings = pickle.load(open(\"/content/drive/MyDrive/twitter15/train.pkl\", 'rb'))\n",
        "    X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev = pickle.load(open(\"/content/drive/MyDrive/twitter15/dev.pkl\", 'rb'))\n",
        "    X_test_source_wid, X_test_source_id, X_test_user_id, X_test_ruid, y_test = pickle.load(open(\"/content/drive/MyDrive/twitter15/test.pkl\", 'rb'))\n",
        "    config['maxlen'] = len(X_train_source_wid[0])\n",
        "    if task == 'twitter15':\n",
        "        config['n_heads'] = 10\n",
        "    elif task == 'twitter16':\n",
        "        config['n_heads'] = 8\n",
        "    else:\n",
        "        config['n_heads'] = 7\n",
        "        config['batch_size'] = 128\n",
        "        config['num_classes'] = 2\n",
        "        config['target_names'] = ['NR', 'FR']\n",
        "    print(config)\n",
        "\n",
        "    config['embedding_weights'] = word_embeddings\n",
        "    config['A_us'] = A_us\n",
        "    config['A_uu'] = A_uu\n",
        "    return X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred, \\\n",
        "           X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev, \\\n",
        "           X_test_source_wid, X_test_source_id, X_test_user_id, X_test_ruid, y_test\n",
        "\n",
        "\n",
        "def train_and_test(model, task):\n",
        "    model_suffix = model.__name__.lower().strip(\"text\")\n",
        "    config['save_path'] = '/content/drive/MyDrive/twitter15/weights.best.' + task + \".\" + model_suffix\n",
        "\n",
        "    X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred, \\\n",
        "    X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev, \\\n",
        "    X_test_source_wid, X_test_source_id, X_test_user_id, X_test_ruid, y_test = load_dataset(task)\n",
        "\n",
        "    nn = model(config)\n",
        "    nn.fit(X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred,\n",
        "          X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev)  #\n",
        "\n",
        "    print(\"================================\")\n",
        "    nn.load_state_dict(torch.load(config['save_path']))\n",
        "    y_pred = nn.predict(X_test_source_wid, X_test_source_id, X_test_user_id, X_test_ruid)\n",
        "    print(classification_report(y_test, y_pred, target_names=config['target_names'], digits=3))\n",
        "\n",
        "\n",
        "config = {\n",
        "    'lr':1e-3,\n",
        "    'reg':1e-6,\n",
        "    'embeding_size': 100,\n",
        "    'batch_size':16,\n",
        "    'nb_filters':100,\n",
        "    'kernel_sizes':[3, 4, 5],\n",
        "    'dropout':0.5,\n",
        "    'epochs':18,\n",
        "    'num_classes':4,\n",
        "    'target_names':['NR', 'FR', 'TR', 'UR']\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    task = 'twitter15'\n",
        "    # task = 'twitter16'\n",
        "    # task = 'weibo'\n",
        "    model = PGAN\n",
        "    train_and_test(model, task)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "task:  twitter15\n",
            "{'lr': 0.001, 'reg': 1e-06, 'embeding_size': 100, 'batch_size': 16, 'nb_filters': 100, 'kernel_sizes': [3, 4, 5], 'dropout': 0.5, 'epochs': 18, 'num_classes': 4, 'target_names': ['NR', 'FR', 'TR', 'UR'], 'save_path': '/content/drive/MyDrive/twitter15/weights.best.twitter15.pgan', 'maxlen': 50, 'n_heads': 10}\n",
            "PGAN(\n",
            "  (word_embedding): Embedding(2246, 300, padding_idx=0)\n",
            "  (user_embedding): Embedding(2213, 100, padding_idx=0)\n",
            "  (source_embedding): Embedding(1490, 100)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (max_poolings): ModuleList(\n",
            "    (0): MaxPool1d(kernel_size=48, stride=48, padding=0, dilation=1, ceil_mode=False)\n",
            "    (1): MaxPool1d(kernel_size=47, stride=47, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): MaxPool1d(kernel_size=46, stride=46, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear): Linear(in_features=400, out_features=200, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (relu): ReLU()\n",
            "  (elu): ELU(alpha=1.0)\n",
            "  (fc_out): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=4, bias=True)\n",
            "  )\n",
            "  (fc_user_out): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=3, bias=True)\n",
            "  )\n",
            "  (fc_ruser_out): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch  1 / 18\n",
            "Batch[0] - loss: 3.595103  acc: 37.5000%(6/16)\n",
            "Batch[1] - loss: 3.615829  acc: 31.2500%(5/16)\n",
            "Batch[2] - loss: 3.671012  acc: 25.0000%(4/16)\n",
            "Batch[3] - loss: 3.526528  acc: 18.7500%(3/16)\n",
            "Batch[4] - loss: 3.473754  acc: 31.2500%(5/16)\n",
            "Batch[5] - loss: 3.492289  acc: 37.5000%(6/16)\n",
            "Batch[6] - loss: 3.367414  acc: 43.7500%(7/16)\n",
            "Batch[7] - loss: 3.555242  acc: 0.0000%(0/16)\n",
            "Batch[8] - loss: 3.424837  acc: 12.5000%(2/16)\n",
            "Batch[9] - loss: 3.352225  acc: 18.7500%(3/16)\n",
            "Batch[10] - loss: 3.349624  acc: 25.0000%(4/16)\n",
            "Batch[11] - loss: 3.309285  acc: 31.2500%(5/16)\n",
            "Batch[12] - loss: 3.228601  acc: 31.2500%(5/16)\n",
            "Batch[13] - loss: 3.219707  acc: 31.2500%(5/16)\n",
            "Batch[14] - loss: 3.102484  acc: 18.7500%(3/16)\n",
            "Batch[15] - loss: 3.410487  acc: 12.5000%(2/16)\n",
            "Batch[16] - loss: 3.227612  acc: 25.0000%(4/16)\n",
            "Batch[17] - loss: 3.348881  acc: 25.0000%(4/16)\n",
            "Batch[18] - loss: 3.736888  acc: 31.2500%(5/16)\n",
            "Batch[19] - loss: 3.305054  acc: 18.7500%(3/16)\n",
            "Batch[20] - loss: 3.162096  acc: 43.7500%(7/16)\n",
            "Batch[21] - loss: 3.328502  acc: 18.7500%(3/16)\n",
            "Batch[22] - loss: 4.566483  acc: 25.0000%(4/16)\n",
            "Batch[23] - loss: 2.761069  acc: 37.5000%(6/16)\n",
            "Batch[24] - loss: 3.488739  acc: 12.5000%(2/16)\n",
            "Batch[25] - loss: 3.170054  acc: 25.0000%(4/16)\n",
            "Batch[26] - loss: 3.541875  acc: 37.5000%(6/16)\n",
            "Batch[27] - loss: 3.541574  acc: 25.0000%(4/16)\n",
            "Batch[28] - loss: 3.219711  acc: 43.7500%(7/16)\n",
            "Batch[29] - loss: 3.424436  acc: 0.0000%(0/16)\n",
            "Batch[30] - loss: 3.285275  acc: 37.5000%(6/16)\n",
            "Batch[31] - loss: 3.546314  acc: 18.7500%(3/16)\n",
            "Batch[32] - loss: 3.141983  acc: 31.2500%(5/16)\n",
            "Batch[33] - loss: 3.552561  acc: 37.5000%(6/16)\n",
            "Batch[34] - loss: 3.206097  acc: 0.0000%(0/16)\n",
            "Batch[35] - loss: 3.284786  acc: 31.2500%(5/16)\n",
            "Batch[36] - loss: 3.647080  acc: 31.2500%(5/16)\n",
            "Batch[37] - loss: 3.588190  acc: 43.7500%(7/16)\n",
            "Batch[38] - loss: 3.223601  acc: 18.7500%(3/16)\n",
            "Batch[39] - loss: 3.555290  acc: 43.7500%(7/16)\n",
            "Batch[40] - loss: 3.160167  acc: 18.7500%(3/16)\n",
            "Batch[41] - loss: 3.324958  acc: 50.0000%(8/16)\n",
            "Batch[42] - loss: 3.291472  acc: 18.7500%(3/16)\n",
            "Batch[43] - loss: 3.524660  acc: 25.0000%(4/16)\n",
            "Batch[44] - loss: 3.272284  acc: 56.2500%(9/16)\n",
            "Batch[45] - loss: 3.130549  acc: 43.7500%(7/16)\n",
            "Batch[46] - loss: 3.516337  acc: 37.5000%(6/16)\n",
            "Batch[47] - loss: 3.199815  acc: 56.2500%(9/16)\n",
            "Batch[48] - loss: 3.099083  acc: 31.2500%(5/16)\n",
            "Batch[49] - loss: 3.073289  acc: 50.0000%(8/16)\n",
            "Batch[50] - loss: 3.388999  acc: 31.2500%(5/16)\n",
            "Batch[51] - loss: 3.169447  acc: 31.2500%(5/16)\n",
            "Batch[52] - loss: 3.181914  acc: 37.5000%(6/16)\n",
            "Batch[53] - loss: 3.336225  acc: 43.7500%(7/16)\n",
            "Batch[54] - loss: 2.987711  acc: 37.5000%(6/16)\n",
            "Batch[55] - loss: 3.210062  acc: 37.5000%(6/16)\n",
            "Batch[56] - loss: 3.201298  acc: 18.7500%(3/16)\n",
            "Batch[57] - loss: 3.083380  acc: 25.0000%(4/16)\n",
            "Batch[58] - loss: 3.222072  acc: 18.7500%(3/16)\n",
            "Batch[59] - loss: 3.169169  acc: 18.7500%(3/16)\n",
            "Batch[60] - loss: 2.992745  acc: 50.0000%(8/16)\n",
            "Batch[61] - loss: 3.506868  acc: 18.7500%(3/16)\n",
            "Batch[62] - loss: 2.841189  acc: 38.4615%(5/13)\n",
            "Average loss:3.340195 average acc:29.578756%\n",
            "Val set acc: 0.4429530201342282\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  2 / 18\n",
            "Batch[0] - loss: 2.788508  acc: 56.2500%(9/16)\n",
            "Batch[1] - loss: 2.941967  acc: 50.0000%(8/16)\n",
            "Batch[2] - loss: 2.959486  acc: 43.7500%(7/16)\n",
            "Batch[3] - loss: 2.925905  acc: 25.0000%(4/16)\n",
            "Batch[4] - loss: 3.135437  acc: 56.2500%(9/16)\n",
            "Batch[5] - loss: 3.184695  acc: 50.0000%(8/16)\n",
            "Batch[6] - loss: 3.063709  acc: 43.7500%(7/16)\n",
            "Batch[7] - loss: 3.050515  acc: 43.7500%(7/16)\n",
            "Batch[8] - loss: 2.816947  acc: 25.0000%(4/16)\n",
            "Batch[9] - loss: 3.110178  acc: 18.7500%(3/16)\n",
            "Batch[10] - loss: 2.803176  acc: 50.0000%(8/16)\n",
            "Batch[11] - loss: 3.300589  acc: 56.2500%(9/16)\n",
            "Batch[12] - loss: 2.620634  acc: 37.5000%(6/16)\n",
            "Batch[13] - loss: 3.105718  acc: 37.5000%(6/16)\n",
            "Batch[14] - loss: 3.295628  acc: 62.5000%(10/16)\n",
            "Batch[15] - loss: 2.716957  acc: 43.7500%(7/16)\n",
            "Batch[16] - loss: 3.331562  acc: 37.5000%(6/16)\n",
            "Batch[17] - loss: 3.341867  acc: 43.7500%(7/16)\n",
            "Batch[18] - loss: 2.739604  acc: 68.7500%(11/16)\n",
            "Batch[19] - loss: 2.417362  acc: 62.5000%(10/16)\n",
            "Batch[20] - loss: 2.685071  acc: 62.5000%(10/16)\n",
            "Batch[21] - loss: 2.688528  acc: 50.0000%(8/16)\n",
            "Batch[22] - loss: 2.893498  acc: 50.0000%(8/16)\n",
            "Batch[23] - loss: 2.855388  acc: 50.0000%(8/16)\n",
            "Batch[24] - loss: 2.921846  acc: 31.2500%(5/16)\n",
            "Batch[25] - loss: 2.775052  acc: 50.0000%(8/16)\n",
            "Batch[26] - loss: 2.447836  acc: 50.0000%(8/16)\n",
            "Batch[27] - loss: 3.034608  acc: 50.0000%(8/16)\n",
            "Batch[28] - loss: 3.036213  acc: 43.7500%(7/16)\n",
            "Batch[29] - loss: 2.764128  acc: 43.7500%(7/16)\n",
            "Batch[30] - loss: 3.312454  acc: 18.7500%(3/16)\n",
            "Batch[31] - loss: 2.654648  acc: 43.7500%(7/16)\n",
            "Batch[32] - loss: 2.626980  acc: 50.0000%(8/16)\n",
            "Batch[33] - loss: 2.804450  acc: 50.0000%(8/16)\n",
            "Batch[34] - loss: 2.559762  acc: 56.2500%(9/16)\n",
            "Batch[35] - loss: 2.912067  acc: 56.2500%(9/16)\n",
            "Batch[36] - loss: 3.128544  acc: 50.0000%(8/16)\n",
            "Batch[37] - loss: 2.788383  acc: 37.5000%(6/16)\n",
            "Batch[38] - loss: 2.836472  acc: 56.2500%(9/16)\n",
            "Batch[39] - loss: 2.747285  acc: 43.7500%(7/16)\n",
            "Batch[40] - loss: 2.726834  acc: 62.5000%(10/16)\n",
            "Batch[41] - loss: 2.887221  acc: 37.5000%(6/16)\n",
            "Batch[42] - loss: 2.806989  acc: 50.0000%(8/16)\n",
            "Batch[43] - loss: 2.472371  acc: 75.0000%(12/16)\n",
            "Batch[44] - loss: 2.516217  acc: 62.5000%(10/16)\n",
            "Batch[45] - loss: 2.344285  acc: 68.7500%(11/16)\n",
            "Batch[46] - loss: 2.759217  acc: 50.0000%(8/16)\n",
            "Batch[47] - loss: 2.550516  acc: 62.5000%(10/16)\n",
            "Batch[48] - loss: 2.748740  acc: 75.0000%(12/16)\n",
            "Batch[49] - loss: 2.196450  acc: 75.0000%(12/16)\n",
            "Batch[50] - loss: 2.840291  acc: 56.2500%(9/16)\n",
            "Batch[51] - loss: 2.803680  acc: 43.7500%(7/16)\n",
            "Batch[52] - loss: 2.757068  acc: 50.0000%(8/16)\n",
            "Batch[53] - loss: 2.605223  acc: 50.0000%(8/16)\n",
            "Batch[54] - loss: 2.334785  acc: 56.2500%(9/16)\n",
            "Batch[55] - loss: 2.300706  acc: 75.0000%(12/16)\n",
            "Batch[56] - loss: 3.730182  acc: 56.2500%(9/16)\n",
            "Batch[57] - loss: 2.685599  acc: 31.2500%(5/16)\n",
            "Batch[58] - loss: 2.425410  acc: 62.5000%(10/16)\n",
            "Batch[59] - loss: 2.392429  acc: 56.2500%(9/16)\n",
            "Batch[60] - loss: 2.356634  acc: 56.2500%(9/16)\n",
            "Batch[61] - loss: 3.144812  acc: 56.2500%(9/16)\n",
            "Batch[62] - loss: 2.083272  acc: 76.9231%(10/13)\n",
            "Average loss:2.803057 average acc:50.824181%\n",
            "Val set acc: 0.6442953020134228\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  3 / 18\n",
            "Batch[0] - loss: 2.147453  acc: 50.0000%(8/16)\n",
            "Batch[1] - loss: 2.023031  acc: 75.0000%(12/16)\n",
            "Batch[2] - loss: 2.529761  acc: 50.0000%(8/16)\n",
            "Batch[3] - loss: 1.817966  acc: 87.5000%(14/16)\n",
            "Batch[4] - loss: 2.388788  acc: 75.0000%(12/16)\n",
            "Batch[5] - loss: 1.789947  acc: 87.5000%(14/16)\n",
            "Batch[6] - loss: 2.101144  acc: 93.7500%(15/16)\n",
            "Batch[7] - loss: 2.272142  acc: 68.7500%(11/16)\n",
            "Batch[8] - loss: 2.251043  acc: 56.2500%(9/16)\n",
            "Batch[9] - loss: 1.812861  acc: 93.7500%(15/16)\n",
            "Batch[10] - loss: 2.316296  acc: 68.7500%(11/16)\n",
            "Batch[11] - loss: 1.850791  acc: 62.5000%(10/16)\n",
            "Batch[12] - loss: 1.478458  acc: 93.7500%(15/16)\n",
            "Batch[13] - loss: 1.905159  acc: 75.0000%(12/16)\n",
            "Batch[14] - loss: 2.349616  acc: 37.5000%(6/16)\n",
            "Batch[15] - loss: 1.850439  acc: 75.0000%(12/16)\n",
            "Batch[16] - loss: 2.198764  acc: 68.7500%(11/16)\n",
            "Batch[17] - loss: 1.948429  acc: 68.7500%(11/16)\n",
            "Batch[18] - loss: 1.848174  acc: 75.0000%(12/16)\n",
            "Batch[19] - loss: 1.834549  acc: 68.7500%(11/16)\n",
            "Batch[20] - loss: 2.142937  acc: 75.0000%(12/16)\n",
            "Batch[21] - loss: 1.984134  acc: 87.5000%(14/16)\n",
            "Batch[22] - loss: 1.786763  acc: 62.5000%(10/16)\n",
            "Batch[23] - loss: 1.401495  acc: 93.7500%(15/16)\n",
            "Batch[24] - loss: 1.858846  acc: 75.0000%(12/16)\n",
            "Batch[25] - loss: 1.575449  acc: 81.2500%(13/16)\n",
            "Batch[26] - loss: 2.382672  acc: 75.0000%(12/16)\n",
            "Batch[27] - loss: 1.866729  acc: 62.5000%(10/16)\n",
            "Batch[28] - loss: 1.867605  acc: 81.2500%(13/16)\n",
            "Batch[29] - loss: 1.264524  acc: 93.7500%(15/16)\n",
            "Batch[30] - loss: 1.609155  acc: 68.7500%(11/16)\n",
            "Batch[31] - loss: 1.735614  acc: 87.5000%(14/16)\n",
            "Batch[32] - loss: 1.815228  acc: 75.0000%(12/16)\n",
            "Batch[33] - loss: 1.523048  acc: 81.2500%(13/16)\n",
            "Batch[34] - loss: 1.685109  acc: 81.2500%(13/16)\n",
            "Batch[35] - loss: 1.739499  acc: 81.2500%(13/16)\n",
            "Batch[36] - loss: 1.339878  acc: 93.7500%(15/16)\n",
            "Batch[37] - loss: 1.936607  acc: 68.7500%(11/16)\n",
            "Batch[38] - loss: 1.861605  acc: 50.0000%(8/16)\n",
            "Batch[39] - loss: 1.763424  acc: 81.2500%(13/16)\n",
            "Batch[40] - loss: 1.852928  acc: 62.5000%(10/16)\n",
            "Batch[41] - loss: 1.844557  acc: 75.0000%(12/16)\n",
            "Batch[42] - loss: 1.734572  acc: 81.2500%(13/16)\n",
            "Batch[43] - loss: 1.535743  acc: 75.0000%(12/16)\n",
            "Batch[44] - loss: 1.697823  acc: 87.5000%(14/16)\n",
            "Batch[45] - loss: 1.522519  acc: 75.0000%(12/16)\n",
            "Batch[46] - loss: 1.454759  acc: 62.5000%(10/16)\n",
            "Batch[47] - loss: 1.719789  acc: 62.5000%(10/16)\n",
            "Batch[48] - loss: 1.340637  acc: 81.2500%(13/16)\n",
            "Batch[49] - loss: 1.482979  acc: 81.2500%(13/16)\n",
            "Batch[50] - loss: 1.570328  acc: 75.0000%(12/16)\n",
            "Batch[51] - loss: 1.516563  acc: 75.0000%(12/16)\n",
            "Batch[52] - loss: 1.356031  acc: 81.2500%(13/16)\n",
            "Batch[53] - loss: 1.591059  acc: 87.5000%(14/16)\n",
            "Batch[54] - loss: 1.111694  acc: 93.7500%(15/16)\n",
            "Batch[55] - loss: 1.470000  acc: 87.5000%(14/16)\n",
            "Batch[56] - loss: 1.271414  acc: 93.7500%(15/16)\n",
            "Batch[57] - loss: 1.193622  acc: 87.5000%(14/16)\n",
            "Batch[58] - loss: 1.307237  acc: 87.5000%(14/16)\n",
            "Batch[59] - loss: 0.892373  acc: 87.5000%(14/16)\n",
            "Batch[60] - loss: 0.971498  acc: 87.5000%(14/16)\n",
            "Batch[61] - loss: 1.838093  acc: 81.2500%(13/16)\n",
            "Batch[62] - loss: 1.041625  acc: 92.3077%(12/13)\n",
            "Average loss:1.732904 average acc:76.961235%\n",
            "Val set acc: 0.6778523489932886\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  4 / 18\n",
            "Batch[0] - loss: 1.106333  acc: 93.7500%(15/16)\n",
            "Batch[1] - loss: 1.124717  acc: 93.7500%(15/16)\n",
            "Batch[2] - loss: 1.031820  acc: 87.5000%(14/16)\n",
            "Batch[3] - loss: 0.998909  acc: 87.5000%(14/16)\n",
            "Batch[4] - loss: 1.519549  acc: 87.5000%(14/16)\n",
            "Batch[5] - loss: 0.966859  acc: 81.2500%(13/16)\n",
            "Batch[6] - loss: 1.132948  acc: 81.2500%(13/16)\n",
            "Batch[7] - loss: 0.987601  acc: 75.0000%(12/16)\n",
            "Batch[8] - loss: 0.697340  acc: 93.7500%(15/16)\n",
            "Batch[9] - loss: 1.049479  acc: 81.2500%(13/16)\n",
            "Batch[10] - loss: 0.831427  acc: 81.2500%(13/16)\n",
            "Batch[11] - loss: 0.834563  acc: 87.5000%(14/16)\n",
            "Batch[12] - loss: 1.060520  acc: 93.7500%(15/16)\n",
            "Batch[13] - loss: 0.667558  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.554190  acc: 93.7500%(15/16)\n",
            "Batch[15] - loss: 0.455468  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.833680  acc: 87.5000%(14/16)\n",
            "Batch[17] - loss: 1.569931  acc: 81.2500%(13/16)\n",
            "Batch[18] - loss: 0.948191  acc: 87.5000%(14/16)\n",
            "Batch[19] - loss: 0.897563  acc: 87.5000%(14/16)\n",
            "Batch[20] - loss: 0.688011  acc: 93.7500%(15/16)\n",
            "Batch[21] - loss: 0.719389  acc: 93.7500%(15/16)\n",
            "Batch[22] - loss: 0.889595  acc: 87.5000%(14/16)\n",
            "Batch[23] - loss: 0.977609  acc: 93.7500%(15/16)\n",
            "Batch[24] - loss: 1.033637  acc: 81.2500%(13/16)\n",
            "Batch[25] - loss: 0.530856  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.765379  acc: 87.5000%(14/16)\n",
            "Batch[27] - loss: 0.821577  acc: 87.5000%(14/16)\n",
            "Batch[28] - loss: 0.700374  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.888813  acc: 75.0000%(12/16)\n",
            "Batch[30] - loss: 1.203398  acc: 87.5000%(14/16)\n",
            "Batch[31] - loss: 0.611570  acc: 93.7500%(15/16)\n",
            "Batch[32] - loss: 1.094843  acc: 87.5000%(14/16)\n",
            "Batch[33] - loss: 0.735723  acc: 93.7500%(15/16)\n",
            "Batch[34] - loss: 0.700674  acc: 93.7500%(15/16)\n",
            "Batch[35] - loss: 0.566111  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.975532  acc: 81.2500%(13/16)\n",
            "Batch[37] - loss: 0.786278  acc: 87.5000%(14/16)\n",
            "Batch[38] - loss: 0.485114  acc: 93.7500%(15/16)\n",
            "Batch[39] - loss: 0.477235  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.758193  acc: 93.7500%(15/16)\n",
            "Batch[41] - loss: 1.121692  acc: 93.7500%(15/16)\n",
            "Batch[42] - loss: 0.483695  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.654123  acc: 93.7500%(15/16)\n",
            "Batch[44] - loss: 0.918987  acc: 93.7500%(15/16)\n",
            "Batch[45] - loss: 0.431423  acc: 93.7500%(15/16)\n",
            "Batch[46] - loss: 0.866320  acc: 81.2500%(13/16)\n",
            "Batch[47] - loss: 0.452614  acc: 93.7500%(15/16)\n",
            "Batch[48] - loss: 0.785070  acc: 93.7500%(15/16)\n",
            "Batch[49] - loss: 0.808947  acc: 87.5000%(14/16)\n",
            "Batch[50] - loss: 0.836183  acc: 93.7500%(15/16)\n",
            "Batch[51] - loss: 0.615331  acc: 93.7500%(15/16)\n",
            "Batch[52] - loss: 0.747540  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.966561  acc: 81.2500%(13/16)\n",
            "Batch[54] - loss: 0.701371  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.742790  acc: 87.5000%(14/16)\n",
            "Batch[56] - loss: 0.588444  acc: 87.5000%(14/16)\n",
            "Batch[57] - loss: 0.826820  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.812519  acc: 87.5000%(14/16)\n",
            "Batch[59] - loss: 0.745760  acc: 87.5000%(14/16)\n",
            "Batch[60] - loss: 0.561630  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.768511  acc: 93.7500%(15/16)\n",
            "Batch[62] - loss: 0.448928  acc: 92.3077%(12/13)\n",
            "Average loss:0.818473 average acc:90.651711%\n",
            "Val set acc: 0.7718120805369127\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  5 / 18\n",
            "Batch[0] - loss: 0.459127  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.467543  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.235946  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.597288  acc: 87.5000%(14/16)\n",
            "Batch[4] - loss: 0.446264  acc: 87.5000%(14/16)\n",
            "Batch[5] - loss: 0.537220  acc: 87.5000%(14/16)\n",
            "Batch[6] - loss: 0.324128  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.589868  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.322568  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.380647  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.628815  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.286759  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.477707  acc: 93.7500%(15/16)\n",
            "Batch[13] - loss: 0.326139  acc: 93.7500%(15/16)\n",
            "Batch[14] - loss: 0.467384  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.586828  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.237767  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.713876  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.246049  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.266588  acc: 93.7500%(15/16)\n",
            "Batch[20] - loss: 0.318060  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.310011  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.530880  acc: 93.7500%(15/16)\n",
            "Batch[23] - loss: 0.285783  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.303963  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.499632  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.178303  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.228162  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.429623  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.321437  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.249067  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.300866  acc: 93.7500%(15/16)\n",
            "Batch[32] - loss: 0.640182  acc: 87.5000%(14/16)\n",
            "Batch[33] - loss: 0.244030  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.541297  acc: 93.7500%(15/16)\n",
            "Batch[35] - loss: 0.285699  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.349604  acc: 93.7500%(15/16)\n",
            "Batch[37] - loss: 0.694844  acc: 93.7500%(15/16)\n",
            "Batch[38] - loss: 0.188873  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.287356  acc: 93.7500%(15/16)\n",
            "Batch[40] - loss: 0.213762  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.172720  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.329078  acc: 93.7500%(15/16)\n",
            "Batch[43] - loss: 0.369327  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.271695  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.223263  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.325413  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.323070  acc: 81.2500%(13/16)\n",
            "Batch[48] - loss: 0.168793  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.211366  acc: 93.7500%(15/16)\n",
            "Batch[50] - loss: 0.360568  acc: 93.7500%(15/16)\n",
            "Batch[51] - loss: 0.459232  acc: 93.7500%(15/16)\n",
            "Batch[52] - loss: 0.424609  acc: 93.7500%(15/16)\n",
            "Batch[53] - loss: 0.343646  acc: 93.7500%(15/16)\n",
            "Batch[54] - loss: 0.205564  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.230516  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.210745  acc: 93.7500%(15/16)\n",
            "Batch[57] - loss: 0.345155  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.286435  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.416780  acc: 93.7500%(15/16)\n",
            "Batch[60] - loss: 0.230046  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.418293  acc: 93.7500%(15/16)\n",
            "Batch[62] - loss: 0.224082  acc: 100.0000%(13/13)\n",
            "Average loss:0.357942 average acc:97.123024%\n",
            "Val set acc: 0.7785234899328859\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  6 / 18\n",
            "Batch[0] - loss: 0.094191  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.229395  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.137341  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.202888  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.089818  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.126451  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.305996  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.091271  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.167735  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.239068  acc: 93.7500%(15/16)\n",
            "Batch[10] - loss: 0.128376  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.162636  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.173117  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.246322  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.088543  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.229217  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.059906  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.043639  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.132471  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.185377  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.131545  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.288592  acc: 93.7500%(15/16)\n",
            "Batch[22] - loss: 0.117392  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.108160  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.124325  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.080323  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.255206  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.094300  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.304297  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.140008  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.306628  acc: 93.7500%(15/16)\n",
            "Batch[31] - loss: 0.106893  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.110991  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.146178  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.179699  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.141583  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.049512  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.077895  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.197615  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.305155  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.137148  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.100033  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.186354  acc: 93.7500%(15/16)\n",
            "Batch[43] - loss: 0.049665  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.260690  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.089394  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.244687  acc: 93.7500%(15/16)\n",
            "Batch[47] - loss: 0.213535  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.328041  acc: 87.5000%(14/16)\n",
            "Batch[49] - loss: 0.088849  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.232264  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.110280  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.102584  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.255300  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.384677  acc: 93.7500%(15/16)\n",
            "Batch[55] - loss: 0.120971  acc: 93.7500%(15/16)\n",
            "Batch[56] - loss: 0.248452  acc: 93.7500%(15/16)\n",
            "Batch[57] - loss: 0.164918  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.090619  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.090568  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.156838  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.140369  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.063551  acc: 100.0000%(13/13)\n",
            "Average loss:0.162855 average acc:99.007942%\n",
            "Val set acc: 0.7651006711409396\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  7 / 18\n",
            "Batch[0] - loss: 0.086431  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.075572  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.034725  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.158595  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.325211  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.052113  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.220631  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.230769  acc: 93.7500%(15/16)\n",
            "Batch[8] - loss: 0.075353  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.152091  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.113316  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.105299  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.107887  acc: 93.7500%(15/16)\n",
            "Batch[13] - loss: 0.087330  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.172409  acc: 93.7500%(15/16)\n",
            "Batch[15] - loss: 0.137221  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.100384  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.093417  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.089935  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.031376  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.065589  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.057976  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.090856  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.061923  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.120991  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.051221  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.030072  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.034346  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.067044  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.072030  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.071497  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.035535  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.056083  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.085199  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.072546  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.056639  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.124316  acc: 93.7500%(15/16)\n",
            "Batch[37] - loss: 0.050527  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.211110  acc: 93.7500%(15/16)\n",
            "Batch[39] - loss: 0.090455  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.193650  acc: 93.7500%(15/16)\n",
            "Batch[41] - loss: 0.054252  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.070911  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.078732  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.110872  acc: 93.7500%(15/16)\n",
            "Batch[45] - loss: 0.103306  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.091058  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.084749  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.069945  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.032986  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.039683  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.085661  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.056161  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.039818  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.036520  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.086073  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.051949  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.130441  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.080517  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.059953  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.104241  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.160946  acc: 93.7500%(15/16)\n",
            "Batch[62] - loss: 0.032541  acc: 100.0000%(13/13)\n",
            "Average loss:0.092238 average acc:99.206352%\n",
            "Val set acc: 0.7785234899328859\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  8 / 18\n",
            "Batch[0] - loss: 0.040354  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.024126  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.029612  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.076600  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.140962  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.073158  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.028833  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.130569  acc: 93.7500%(15/16)\n",
            "Batch[8] - loss: 0.038972  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.059818  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.041045  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.016078  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.019293  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.044035  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.047998  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.053183  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.034498  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.039441  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.035709  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.043808  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.033362  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.045836  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.023937  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.035077  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.061863  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.030783  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.058554  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.020297  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.055741  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.052115  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.088582  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.082249  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.085929  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.029803  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.026600  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.035491  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.017368  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.026977  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.028413  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.057877  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.022605  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.045180  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.086867  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.031265  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.027675  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.064600  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.034390  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.099078  acc: 93.7500%(15/16)\n",
            "Batch[48] - loss: 0.024671  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.135272  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.117630  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.026224  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.018744  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.055983  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.044544  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.034873  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.012568  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.073365  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.043980  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.015957  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.043081  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.036033  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.105219  acc: 100.0000%(13/13)\n",
            "Average loss:0.049440 average acc:99.801590%\n",
            "Val set acc: 0.7785234899328859\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  9 / 18\n",
            "Batch[0] - loss: 0.023441  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.035980  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.099704  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.033035  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.022931  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.046336  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.078321  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.031184  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.015623  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.021764  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.048532  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.023509  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.026690  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.016507  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.025700  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.023244  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.019228  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.037932  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.033688  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.039648  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.032082  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.048573  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.009240  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.027922  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.044165  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.023148  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.012793  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.036265  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.041246  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.014672  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.005875  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.024468  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.031527  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.013024  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.008897  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.044062  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.021783  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.043524  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.020295  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.019913  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.006422  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.053562  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.031169  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.079057  acc: 93.7500%(15/16)\n",
            "Batch[44] - loss: 0.029854  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.018796  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.022490  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.012634  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.023468  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.013385  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.017095  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.027950  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.007013  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.028471  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.091886  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.032172  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.023515  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.026839  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.021146  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.020058  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.024489  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.055582  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.018323  acc: 100.0000%(13/13)\n",
            "Average loss:0.030347 average acc:99.900803%\n",
            "Val set acc: 0.7919463087248322\n",
            "Best val set acc: 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.75000   0.71053   0.72973        38\n",
            "          FR    0.73684   0.75676   0.74667        37\n",
            "          TR    0.76923   0.81081   0.78947        37\n",
            "          UR    0.91667   0.89189   0.90411        37\n",
            "\n",
            "    accuracy                        0.79195       149\n",
            "   macro avg    0.79318   0.79250   0.79249       149\n",
            "weighted avg    0.79290   0.79195   0.79207       149\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  10 / 18\n",
            "Batch[0] - loss: 0.016781  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.021105  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.010134  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.038706  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.019488  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.018598  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.028357  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.021527  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.023597  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.011922  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.019857  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.011056  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.013212  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.009520  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.016522  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.014273  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.036849  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.017841  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.010565  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.042938  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.019043  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.034834  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.019861  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.013112  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.034623  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.027873  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.009148  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.019423  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.009543  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.010650  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.014216  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.041634  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.031591  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.015830  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.026671  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.014958  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.026261  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.022773  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.012972  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.017954  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.017679  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.010621  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.020727  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.017321  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.021264  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.042260  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.030082  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.015480  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.031170  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.019843  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.021145  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.023046  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.015935  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.018306  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.031134  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.024129  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.024253  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.020120  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.013815  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.009708  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.020744  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.010936  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.018970  acc: 100.0000%(13/13)\n",
            "Average loss:0.020706 average acc:100.000008%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0.7919463087248322\n",
            "\n",
            "Epoch  11 / 18\n",
            "Batch[0] - loss: 0.014557  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.026285  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.013586  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.023719  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.007914  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.028438  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.015309  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.065756  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.009651  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.015591  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.021974  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.016730  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.017265  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.008639  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.025815  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.011764  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.004481  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.020697  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.010498  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.023151  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.013456  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.013060  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.012676  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.007879  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.010049  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.011915  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.033405  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.015312  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.004710  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.008948  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.020361  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.026080  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.013494  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.061325  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.006652  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.027606  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.010976  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.047239  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.031313  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.006501  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.093829  acc: 93.7500%(15/16)\n",
            "Batch[41] - loss: 0.008576  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.006750  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.015552  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.013204  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.007060  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.011380  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.014316  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.052460  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.013118  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.020159  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.005716  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.010438  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.014086  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.015263  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.014272  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.042189  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.007920  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.009074  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.022220  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.005752  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.014138  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.003224  acc: 100.0000%(13/13)\n",
            "Average loss:0.018817 average acc:99.900803%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0.7919463087248322\n",
            "\n",
            "Epoch  12 / 18\n",
            "Batch[0] - loss: 0.019243  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.015942  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.009121  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.012690  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.011909  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.005645  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.011703  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.023719  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.010151  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.021233  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.022033  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.013340  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.014632  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.012014  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.020506  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.022504  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.015245  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.009014  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.015853  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.039434  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.013194  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.075936  acc: 93.7500%(15/16)\n",
            "Batch[22] - loss: 0.017244  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.007905  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.011221  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.016832  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.015168  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.009598  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.011495  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.006604  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.022495  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.014366  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.008477  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.019712  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.006204  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.047266  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.008765  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.011773  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.020882  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.005812  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.022374  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.019973  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.008642  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.008891  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.010964  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.008987  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.003741  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.014350  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.037711  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.019365  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.011872  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.022555  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.040269  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.049704  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.005401  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.006120  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.014280  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.016862  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.011604  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.013217  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.010886  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.006438  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.003388  acc: 100.0000%(13/13)\n",
            "Average loss:0.016579 average acc:99.900803%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0.7919463087248322\n",
            "\n",
            "Epoch  13 / 18\n",
            "Batch[0] - loss: 0.008145  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.006052  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.009921  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.037197  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.033790  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.013521  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.002629  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.004675  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.004113  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.020929  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.019705  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.005309  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.023498  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.027768  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.013837  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.011712  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.015841  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.007120  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.004475  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.018869  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.004594  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.006795  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.003988  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.013810  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.009522  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.007208  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.007927  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.024513  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.026475  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.018371  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.003516  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.008039  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.008916  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.010800  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.009040  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.010907  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.004776  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.017327  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.009227  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.009105  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.013263  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.005168  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.024630  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.007651  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.015270  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.015670  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.010618  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.005191  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.005503  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.004520  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.003796  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.011356  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.007587  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.048309  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.024715  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.009248  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.014294  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.010249  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.004350  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.013161  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.003436  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.004411  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.002302  acc: 100.0000%(13/13)\n",
            "Average loss:0.012201 average acc:100.000008%\n",
            "Reload the best model...\n",
            "0.0005\n",
            "Val set acc: 0.7919463087248322\n",
            "Best val set acc: 0.7919463087248322\n",
            "\n",
            "Epoch  14 / 18\n",
            "Batch[0] - loss: 0.026955  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.009230  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.011280  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.005889  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.010734  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.014894  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.017851  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.024409  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.011285  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.019061  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.017501  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.043650  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.006029  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.011745  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.011262  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.024200  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.028361  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.026016  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.023016  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.065062  acc: 93.7500%(15/16)\n",
            "Batch[20] - loss: 0.020948  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.043209  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.024761  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.040027  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.023281  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.035768  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.033856  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.036052  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.008476  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.024810  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.028904  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.047402  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.015215  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.015366  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.032272  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.023711  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.016190  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.012305  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.010973  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.022180  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.023077  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.009075  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.011433  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.016984  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.040640  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.024778  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.021641  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.011700  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.013796  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.005833  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.021838  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.054880  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.013499  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.015154  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.022723  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.023891  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.012149  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.023733  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.022067  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.029109  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.032054  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.012387  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.010632  acc: 100.0000%(13/13)\n",
            "Average loss:0.022178 average acc:99.900803%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0.7919463087248322\n",
            "\n",
            "Epoch  15 / 18\n",
            "Batch[0] - loss: 0.027500  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.008342  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.005883  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.009164  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.017139  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.020952  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.012900  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.007758  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.007543  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.028768  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.009177  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.014908  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.030618  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.035146  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.015695  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.008982  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.023613  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.019411  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.049829  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.022813  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.008266  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.016285  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.010506  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.016208  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.021875  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.018340  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.019391  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.008802  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.008874  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.018196  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.031976  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.013792  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.009540  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.012745  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.015731  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.007703  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.016937  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.027283  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.019254  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.019165  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.025161  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.010853  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.055841  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.016044  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.008465  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.018845  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.014416  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.010575  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.018212  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.011432  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.013883  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.014640  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.061161  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.006769  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.013422  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.006772  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.006914  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.008233  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.079289  acc: 93.7500%(15/16)\n",
            "Batch[59] - loss: 0.013702  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.018826  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.016914  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.016221  acc: 100.0000%(13/13)\n",
            "Average loss:0.018470 average acc:99.900803%\n",
            "Val set acc: 0.7785234899328859\n",
            "Best val set acc: 0.7919463087248322\n",
            "\n",
            "Epoch  16 / 18\n",
            "Batch[0] - loss: 0.028912  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.015278  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.009023  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.020404  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.025483  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.013298  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.029933  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.010232  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.010295  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.032513  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.010278  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.031902  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.007586  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.008329  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.008218  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.008386  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.024404  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.011885  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.012436  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.077300  acc: 93.7500%(15/16)\n",
            "Batch[20] - loss: 0.013409  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.009375  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.029481  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.030771  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.027187  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.016203  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.008861  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.019422  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.026252  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.020507  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.021781  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.017447  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.008045  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.010298  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.016640  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.012313  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.012505  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.006822  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.011860  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.025705  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.019303  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.032655  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.010508  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.026413  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.011233  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.021266  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.018222  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.015613  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.008278  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.003558  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.005362  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.019183  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.027911  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.028894  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.017438  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.008816  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.005643  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.006821  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.093183  acc: 93.7500%(15/16)\n",
            "Batch[59] - loss: 0.030497  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.012237  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.034388  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.007014  acc: 100.0000%(13/13)\n",
            "Average loss:0.019134 average acc:99.801590%\n",
            "Reload the best model...\n",
            "0.00025\n",
            "Val set acc: 0.7919463087248322\n",
            "Best val set acc: 0.7919463087248322\n",
            "\n",
            "Epoch  17 / 18\n",
            "Batch[0] - loss: 0.042989  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.041661  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.028425  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.009576  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.020542  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.073615  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.138078  acc: 93.7500%(15/16)\n",
            "Batch[7] - loss: 0.012321  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.024143  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.007102  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.034035  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.026851  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.006902  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.021265  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.037459  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.026825  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.013719  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.101841  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.020009  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.014292  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.039265  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.029003  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.038338  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.016521  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.020775  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.014806  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.037017  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.048186  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.008926  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.014189  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.016130  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.006721  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.082067  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.015046  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.009596  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.021970  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.010158  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.026865  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.013178  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.011561  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.022623  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.053209  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.022166  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.017085  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.020540  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.017872  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.016110  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.027760  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.009293  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.027263  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.016574  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.022972  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.023840  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.008623  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.025414  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.013232  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.043777  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.027702  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.008356  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.035308  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.018029  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.023732  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.028443  acc: 100.0000%(13/13)\n",
            "Average loss:0.027173 average acc:99.900803%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0.7919463087248322\n",
            "\n",
            "Epoch  18 / 18\n",
            "Batch[0] - loss: 0.015116  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.013868  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.018371  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.020335  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.022555  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.032314  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.027748  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.037365  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.034523  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.022234  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.029009  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.020488  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.013951  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.009352  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.018802  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.024937  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.011174  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.056555  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.008521  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.024089  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.022299  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.006399  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.009452  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.013287  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.016752  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.011003  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.014393  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.037710  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.006227  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.014139  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.007906  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.017995  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.016958  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.020675  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.008880  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.017258  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.013932  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.007836  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.014707  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.019302  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.041699  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.041433  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.029570  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.007971  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.015723  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.026838  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.022279  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.015912  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.022726  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.005238  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.049891  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.017217  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.010663  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.015203  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.012146  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.026077  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.009758  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.019673  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.059110  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.019979  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.022869  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.028444  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.044840  acc: 100.0000%(13/13)\n",
            "Average loss:0.021011 average acc:100.000008%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0.7919463087248322\n",
            "================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR      0.886     0.929     0.907        84\n",
            "          FR      0.876     0.929     0.902        84\n",
            "          TR      0.872     0.810     0.840        84\n",
            "          UR      0.938     0.905     0.921        84\n",
            "\n",
            "    accuracy                          0.893       336\n",
            "   macro avg      0.893     0.893     0.892       336\n",
            "weighted avg      0.893     0.893     0.892       336\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYVUihq_QRW1",
        "outputId": "c27feff0-a82b-47e3-9875-f83e07ddc772"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfiY6Xp2NkKk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl_zgeR9SJAl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32hECXUxSJEL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9f5DsgfSJHl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3No9uodSJKU"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knea6v27SNeM"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDbTTTVTSNhL"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMBJzuCRSNnN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}