              total        used        free      shared  buff/cache   available
Mem:          14877         493       12205           1        2178       14075
Swap:             0           0           0
Sun Aug  2 03:49:32 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:07.0 Off |                    0 |
| N/A   44C    P8    15W /  70W |      0MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
task:  twitter15
{'lr': 0.001, 'reg': 1e-06, 'embeding_size': 100, 'batch_size': 16, 'nb_filters': 100, 'kernel_sizes': [3, 4, 5], 'dropout': 0.5, 'epochs': 18, 'num_classes': 4, 'target_names': ['NR', 'FR', 'TR', 'UR'], 'save_path': 'checkpoint/weights.best.twitter15.pgan', 'maxlen': 50, 'n_heads': 10}
PGAN(
  (word_embedding): Embedding(2246, 300, padding_idx=0)
  (user_embedding): Embedding(2213, 100, padding_idx=0)
  (source_embedding): Embedding(1490, 100)
  (convs): ModuleList(
    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))
    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))
    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))
  )
  (max_poolings): ModuleList(
    (0): MaxPool1d(kernel_size=48, stride=48, padding=0, dilation=1, ceil_mode=False)
    (1): MaxPool1d(kernel_size=47, stride=47, padding=0, dilation=1, ceil_mode=False)
    (2): MaxPool1d(kernel_size=46, stride=46, padding=0, dilation=1, ceil_mode=False)
  )
  (linear): Linear(in_features=400, out_features=200, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
  (relu): ReLU()
  (elu): ELU(alpha=1.0)
  (fc_out): Sequential(
    (0): Linear(in_features=500, out_features=100, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=100, out_features=4, bias=True)
  )
  (fc_user_out): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=100, out_features=3, bias=True)
  )
  (fc_ruser_out): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=100, out_features=3, bias=True)
  )
)

Epoch  1 / 18
Batch[0] - loss: 3.517562  acc: 50.0000%(8/16)
Batch[1] - loss: 3.549713  acc: 31.0000%(5/16)
Batch[2] - loss: 3.566470  acc: 31.0000%(5/16)
Batch[3] - loss: 3.586085  acc: 25.0000%(4/16)
Batch[4] - loss: 3.494714  acc: 43.0000%(7/16)
Batch[5] - loss: 3.653662  acc: 0.0000%(0/16)
Batch[6] - loss: 3.598993  acc: 25.0000%(4/16)
Batch[7] - loss: 3.534913  acc: 25.0000%(4/16)
Batch[8] - loss: 3.504340  acc: 25.0000%(4/16)
Batch[9] - loss: 3.518017  acc: 43.0000%(7/16)
Batch[10] - loss: 3.489118  acc: 25.0000%(4/16)
Batch[11] - loss: 3.626562  acc: 25.0000%(4/16)
Batch[12] - loss: 3.409131  acc: 31.0000%(5/16)
Batch[13] - loss: 3.684928  acc: 43.0000%(7/16)
Batch[14] - loss: 3.471186  acc: 6.0000%(1/16)
Batch[15] - loss: 3.385142  acc: 25.0000%(4/16)
Batch[16] - loss: 3.373825  acc: 25.0000%(4/16)
Batch[17] - loss: 3.426932  acc: 31.0000%(5/16)
Batch[18] - loss: 3.416823  acc: 31.0000%(5/16)
Batch[19] - loss: 3.318706  acc: 25.0000%(4/16)
Batch[20] - loss: 3.285693  acc: 37.0000%(6/16)
Batch[21] - loss: 3.082248  acc: 31.0000%(5/16)
Batch[22] - loss: 3.418281  acc: 18.0000%(3/16)
Batch[23] - loss: 3.187191  acc: 18.0000%(3/16)
Batch[24] - loss: 2.879228  acc: 37.0000%(6/16)
Batch[25] - loss: 3.026863  acc: 37.0000%(6/16)
Batch[26] - loss: 3.302591  acc: 25.0000%(4/16)
Batch[27] - loss: 3.075629  acc: 37.0000%(6/16)
Batch[28] - loss: 3.568269  acc: 6.0000%(1/16)
Batch[29] - loss: 3.168745  acc: 31.0000%(5/16)
Batch[30] - loss: 3.461071  acc: 18.0000%(3/16)
Batch[31] - loss: 3.537784  acc: 12.0000%(2/16)
Batch[32] - loss: 2.867139  acc: 18.0000%(3/16)
Batch[33] - loss: 3.292357  acc: 56.0000%(9/16)
Batch[34] - loss: 3.441695  acc: 18.0000%(3/16)
Batch[35] - loss: 3.737926  acc: 18.0000%(3/16)
Batch[36] - loss: 3.006406  acc: 43.0000%(7/16)
Batch[37] - loss: 3.266996  acc: 31.0000%(5/16)
Batch[38] - loss: 3.182393  acc: 18.0000%(3/16)
Batch[39] - loss: 3.963676  acc: 18.0000%(3/16)
Batch[40] - loss: 2.750433  acc: 50.0000%(8/16)
Batch[41] - loss: 3.097413  acc: 18.0000%(3/16)
Batch[42] - loss: 2.977654  acc: 31.0000%(5/16)
Batch[43] - loss: 3.182430  acc: 37.0000%(6/16)
Batch[44] - loss: 3.359957  acc: 18.0000%(3/16)
Batch[45] - loss: 3.113508  acc: 37.0000%(6/16)
Batch[46] - loss: 3.290031  acc: 25.0000%(4/16)
Batch[47] - loss: 3.296722  acc: 56.0000%(9/16)
Batch[48] - loss: 3.093971  acc: 56.0000%(9/16)
Batch[49] - loss: 2.917853  acc: 37.0000%(6/16)
Batch[50] - loss: 3.108933  acc: 25.0000%(4/16)
Batch[51] - loss: 2.859064  acc: 62.0000%(10/16)
Batch[52] - loss: 3.621221  acc: 25.0000%(4/16)
Batch[53] - loss: 3.272081  acc: 31.0000%(5/16)
Batch[54] - loss: 3.843580  acc: 18.0000%(3/16)
Batch[55] - loss: 3.681035  acc: 18.0000%(3/16)
Batch[56] - loss: 3.661455  acc: 43.0000%(7/16)
Batch[57] - loss: 3.052638  acc: 31.0000%(5/16)
Batch[58] - loss: 3.014123  acc: 50.0000%(8/16)
Batch[59] - loss: 3.631382  acc: 31.0000%(5/16)
Batch[60] - loss: 3.060232  acc: 50.0000%(8/16)
Batch[61] - loss: 3.112282  acc: 18.0000%(3/16)
Batch[62] - loss: 3.098432  acc: 23.0000%(3/13)
Average loss:3.332943 average acc:29.000000%
Val set acc: 0.42953020134228187
Best val set acc: 0

Epoch  2 / 18
Batch[0] - loss: 2.787226  acc: 56.0000%(9/16)
Batch[1] - loss: 3.083077  acc: 50.0000%(8/16)
Batch[2] - loss: 3.566401  acc: 43.0000%(7/16)
Batch[3] - loss: 3.013176  acc: 68.0000%(11/16)
Batch[4] - loss: 2.923707  acc: 56.0000%(9/16)
Batch[5] - loss: 3.092705  acc: 25.0000%(4/16)
Batch[6] - loss: 3.147551  acc: 43.0000%(7/16)
Batch[7] - loss: 3.227448  acc: 50.0000%(8/16)
Batch[8] - loss: 2.935379  acc: 43.0000%(7/16)
Batch[9] - loss: 3.210436  acc: 68.0000%(11/16)
Batch[10] - loss: 2.990025  acc: 43.0000%(7/16)
Batch[11] - loss: 2.788479  acc: 62.0000%(10/16)
Batch[12] - loss: 2.897128  acc: 50.0000%(8/16)
Batch[13] - loss: 2.694127  acc: 62.0000%(10/16)
Batch[14] - loss: 2.797838  acc: 81.0000%(13/16)
Batch[15] - loss: 3.493310  acc: 25.0000%(4/16)
Batch[16] - loss: 2.958711  acc: 43.0000%(7/16)
Batch[17] - loss: 2.858369  acc: 43.0000%(7/16)
Batch[18] - loss: 2.740558  acc: 50.0000%(8/16)
Batch[19] - loss: 2.735094  acc: 62.0000%(10/16)
Batch[20] - loss: 3.004665  acc: 56.0000%(9/16)
Batch[21] - loss: 3.035736  acc: 25.0000%(4/16)
Batch[22] - loss: 2.859408  acc: 43.0000%(7/16)
Batch[23] - loss: 3.050169  acc: 62.0000%(10/16)
Batch[24] - loss: 2.715231  acc: 75.0000%(12/16)
Batch[25] - loss: 2.998129  acc: 56.0000%(9/16)
Batch[26] - loss: 2.821565  acc: 43.0000%(7/16)
Batch[27] - loss: 2.959039  acc: 31.0000%(5/16)
Batch[28] - loss: 2.598216  acc: 56.0000%(9/16)
Batch[29] - loss: 2.370274  acc: 68.0000%(11/16)
Batch[30] - loss: 3.305064  acc: 43.0000%(7/16)
Batch[31] - loss: 2.877110  acc: 68.0000%(11/16)
Batch[32] - loss: 2.685781  acc: 56.0000%(9/16)
Batch[33] - loss: 3.037159  acc: 56.0000%(9/16)
Batch[34] - loss: 2.385651  acc: 68.0000%(11/16)
Batch[35] - loss: 2.795619  acc: 37.0000%(6/16)
Batch[36] - loss: 2.527210  acc: 62.0000%(10/16)
Batch[37] - loss: 2.614360  acc: 56.0000%(9/16)
Batch[38] - loss: 2.954535  acc: 37.0000%(6/16)
Batch[39] - loss: 2.612258  acc: 56.0000%(9/16)
Batch[40] - loss: 2.779283  acc: 56.0000%(9/16)
Batch[41] - loss: 2.658202  acc: 50.0000%(8/16)
Batch[42] - loss: 2.724311  acc: 68.0000%(11/16)
Batch[43] - loss: 2.851038  acc: 56.0000%(9/16)
Batch[44] - loss: 2.448786  acc: 56.0000%(9/16)
Batch[45] - loss: 2.583814  acc: 56.0000%(9/16)
Batch[46] - loss: 2.558955  acc: 68.0000%(11/16)
Batch[47] - loss: 2.628619  acc: 50.0000%(8/16)
Batch[48] - loss: 2.221731  acc: 68.0000%(11/16)
Batch[49] - loss: 2.293081  acc: 56.0000%(9/16)
Batch[50] - loss: 2.354260  acc: 62.0000%(10/16)
Batch[51] - loss: 2.580088  acc: 50.0000%(8/16)
Batch[52] - loss: 2.386663  acc: 68.0000%(11/16)
Batch[53] - loss: 2.618145  acc: 68.0000%(11/16)
Batch[54] - loss: 2.274450  acc: 62.0000%(10/16)
Batch[55] - loss: 2.536459  acc: 56.0000%(9/16)
Batch[56] - loss: 2.625127  acc: 50.0000%(8/16)
Batch[57] - loss: 2.301284  acc: 68.0000%(11/16)
Batch[58] - loss: 2.419198  acc: 43.0000%(7/16)
Batch[59] - loss: 2.624097  acc: 75.0000%(12/16)
Batch[60] - loss: 2.600205  acc: 62.0000%(10/16)
Batch[61] - loss: 2.257991  acc: 43.0000%(7/16)
Batch[62] - loss: 2.702497  acc: 53.0000%(7/13)
Average loss:2.764702 average acc:54.000000%
Val set acc: 0.6644295302013423
Best val set acc: 0

Epoch  3 / 18
Batch[0] - loss: 1.813805  acc: 81.0000%(13/16)
Batch[1] - loss: 1.955720  acc: 68.0000%(11/16)
Batch[2] - loss: 2.214717  acc: 75.0000%(12/16)
Batch[3] - loss: 1.820100  acc: 81.0000%(13/16)
Batch[4] - loss: 1.913173  acc: 87.0000%(14/16)
Batch[5] - loss: 2.057174  acc: 87.0000%(14/16)
Batch[6] - loss: 1.901971  acc: 81.0000%(13/16)
Batch[7] - loss: 1.845811  acc: 93.0000%(15/16)
Batch[8] - loss: 2.191007  acc: 68.0000%(11/16)
Batch[9] - loss: 2.038446  acc: 62.0000%(10/16)
Batch[10] - loss: 1.576360  acc: 93.0000%(15/16)
Batch[11] - loss: 2.269737  acc: 75.0000%(12/16)
Batch[12] - loss: 1.852060  acc: 68.0000%(11/16)
Batch[13] - loss: 1.893263  acc: 68.0000%(11/16)
Batch[14] - loss: 1.837757  acc: 81.0000%(13/16)
Batch[15] - loss: 2.159027  acc: 75.0000%(12/16)
Batch[16] - loss: 2.024343  acc: 87.0000%(14/16)
Batch[17] - loss: 1.253802  acc: 87.0000%(14/16)
Batch[18] - loss: 1.555679  acc: 50.0000%(8/16)
Batch[19] - loss: 1.805386  acc: 68.0000%(11/16)
Batch[20] - loss: 1.522854  acc: 87.0000%(14/16)
Batch[21] - loss: 1.922012  acc: 68.0000%(11/16)
Batch[22] - loss: 2.104697  acc: 68.0000%(11/16)
Batch[23] - loss: 1.378781  acc: 87.0000%(14/16)
Batch[24] - loss: 1.791305  acc: 62.0000%(10/16)
Batch[25] - loss: 1.776826  acc: 75.0000%(12/16)
Batch[26] - loss: 1.696736  acc: 81.0000%(13/16)
Batch[27] - loss: 1.817388  acc: 81.0000%(13/16)
Batch[28] - loss: 1.825216  acc: 81.0000%(13/16)
Batch[29] - loss: 1.464502  acc: 75.0000%(12/16)
Batch[30] - loss: 1.807348  acc: 68.0000%(11/16)
Batch[31] - loss: 1.353750  acc: 93.0000%(15/16)
Batch[32] - loss: 1.786968  acc: 93.0000%(15/16)
Batch[33] - loss: 1.607728  acc: 93.0000%(15/16)
Batch[34] - loss: 1.787733  acc: 81.0000%(13/16)
Batch[35] - loss: 1.652030  acc: 75.0000%(12/16)
Batch[36] - loss: 1.599804  acc: 75.0000%(12/16)
Batch[37] - loss: 1.250056  acc: 87.0000%(14/16)
Batch[38] - loss: 1.840997  acc: 56.0000%(9/16)
Batch[39] - loss: 1.503559  acc: 87.0000%(14/16)
Batch[40] - loss: 1.533657  acc: 93.0000%(15/16)
Batch[41] - loss: 1.398222  acc: 87.0000%(14/16)
Batch[42] - loss: 1.785667  acc: 87.0000%(14/16)
Batch[43] - loss: 1.436053  acc: 81.0000%(13/16)
Batch[44] - loss: 1.573774  acc: 81.0000%(13/16)
Batch[45] - loss: 1.230503  acc: 93.0000%(15/16)
Batch[46] - loss: 1.322677  acc: 81.0000%(13/16)
Batch[47] - loss: 1.654806  acc: 81.0000%(13/16)
Batch[48] - loss: 1.265289  acc: 81.0000%(13/16)
Batch[49] - loss: 1.336248  acc: 93.0000%(15/16)
Batch[50] - loss: 1.539919  acc: 75.0000%(12/16)
Batch[51] - loss: 1.334095  acc: 81.0000%(13/16)
Batch[52] - loss: 1.339373  acc: 81.0000%(13/16)
Batch[53] - loss: 1.391965  acc: 75.0000%(12/16)
Batch[54] - loss: 1.371466  acc: 62.0000%(10/16)
Batch[55] - loss: 1.543904  acc: 81.0000%(13/16)
Batch[56] - loss: 1.326623  acc: 75.0000%(12/16)
Batch[57] - loss: 1.658577  acc: 81.0000%(13/16)
Batch[58] - loss: 1.193911  acc: 87.0000%(14/16)
Batch[59] - loss: 1.986611  acc: 68.0000%(11/16)
Batch[60] - loss: 1.620608  acc: 75.0000%(12/16)
Batch[61] - loss: 1.565377  acc: 81.0000%(13/16)
Batch[62] - loss: 1.716681  acc: 69.0000%(9/13)
Average loss:1.676121 average acc:78.000000%
Val set acc: 0.7181208053691275
Best val set acc: 0

Epoch  4 / 18
Batch[0] - loss: 1.271430  acc: 93.0000%(15/16)
Batch[1] - loss: 1.062439  acc: 93.0000%(15/16)
Batch[2] - loss: 0.882308  acc: 93.0000%(15/16)
Batch[3] - loss: 1.177258  acc: 87.0000%(14/16)
Batch[4] - loss: 0.852732  acc: 100.0000%(16/16)
Batch[5] - loss: 0.807416  acc: 100.0000%(16/16)
Batch[6] - loss: 1.280639  acc: 75.0000%(12/16)
Batch[7] - loss: 0.721891  acc: 93.0000%(15/16)
Batch[8] - loss: 0.913783  acc: 87.0000%(14/16)
Batch[9] - loss: 0.943599  acc: 87.0000%(14/16)
Batch[10] - loss: 1.164574  acc: 75.0000%(12/16)
Batch[11] - loss: 1.027445  acc: 100.0000%(16/16)
Batch[12] - loss: 0.813800  acc: 93.0000%(15/16)
Batch[13] - loss: 0.685607  acc: 100.0000%(16/16)
Batch[14] - loss: 0.735112  acc: 100.0000%(16/16)
Batch[15] - loss: 0.588324  acc: 100.0000%(16/16)
Batch[16] - loss: 0.985596  acc: 100.0000%(16/16)
Batch[17] - loss: 0.960069  acc: 87.0000%(14/16)
Batch[18] - loss: 0.562046  acc: 100.0000%(16/16)
Batch[19] - loss: 0.888657  acc: 87.0000%(14/16)
Batch[20] - loss: 0.705416  acc: 93.0000%(15/16)
Batch[21] - loss: 0.631026  acc: 100.0000%(16/16)
Batch[22] - loss: 0.982482  acc: 93.0000%(15/16)
Batch[23] - loss: 0.476872  acc: 93.0000%(15/16)
Batch[24] - loss: 0.486339  acc: 87.0000%(14/16)
Batch[25] - loss: 1.116134  acc: 81.0000%(13/16)
Batch[26] - loss: 0.725227  acc: 87.0000%(14/16)
Batch[27] - loss: 0.787658  acc: 93.0000%(15/16)
Batch[28] - loss: 0.740016  acc: 87.0000%(14/16)
Batch[29] - loss: 0.824878  acc: 93.0000%(15/16)
Batch[30] - loss: 0.591577  acc: 93.0000%(15/16)
Batch[31] - loss: 0.528996  acc: 100.0000%(16/16)
Batch[32] - loss: 0.427313  acc: 100.0000%(16/16)
Batch[33] - loss: 0.572188  acc: 100.0000%(16/16)
Batch[34] - loss: 0.429944  acc: 100.0000%(16/16)
Batch[35] - loss: 0.570619  acc: 87.0000%(14/16)
Batch[36] - loss: 1.061597  acc: 100.0000%(16/16)
Batch[37] - loss: 0.662656  acc: 93.0000%(15/16)
Batch[38] - loss: 0.796486  acc: 81.0000%(13/16)
Batch[39] - loss: 0.659775  acc: 87.0000%(14/16)
Batch[40] - loss: 0.612922  acc: 93.0000%(15/16)
Batch[41] - loss: 0.421071  acc: 100.0000%(16/16)
Batch[42] - loss: 1.072640  acc: 93.0000%(15/16)
Batch[43] - loss: 0.734593  acc: 93.0000%(15/16)
Batch[44] - loss: 0.605722  acc: 93.0000%(15/16)
Batch[45] - loss: 0.864592  acc: 81.0000%(13/16)
Batch[46] - loss: 1.134122  acc: 81.0000%(13/16)
Batch[47] - loss: 0.572436  acc: 93.0000%(15/16)
Batch[48] - loss: 0.738664  acc: 93.0000%(15/16)
Batch[49] - loss: 0.453918  acc: 100.0000%(16/16)
Batch[50] - loss: 0.600966  acc: 100.0000%(16/16)
Batch[51] - loss: 0.659530  acc: 87.0000%(14/16)
Batch[52] - loss: 0.508160  acc: 100.0000%(16/16)
Batch[53] - loss: 0.420146  acc: 93.0000%(15/16)
Batch[54] - loss: 0.409643  acc: 100.0000%(16/16)
Batch[55] - loss: 0.801419  acc: 93.0000%(15/16)
Batch[56] - loss: 0.524481  acc: 100.0000%(16/16)
Batch[57] - loss: 0.739163  acc: 100.0000%(16/16)
Batch[58] - loss: 1.373533  acc: 68.0000%(11/16)
Batch[59] - loss: 0.775466  acc: 93.0000%(15/16)
Batch[60] - loss: 0.924809  acc: 81.0000%(13/16)
Batch[61] - loss: 0.619195  acc: 100.0000%(16/16)
Batch[62] - loss: 0.895660  acc: 100.0000%(13/13)
Average loss:0.770869 average acc:92.000000%
Val set acc: 0.7583892617449665
Best val set acc: 0

Epoch  5 / 18
Batch[0] - loss: 0.291217  acc: 100.0000%(16/16)
Batch[1] - loss: 0.268232  acc: 93.0000%(15/16)
Batch[2] - loss: 0.230298  acc: 100.0000%(16/16)
Batch[3] - loss: 0.456404  acc: 100.0000%(16/16)
Batch[4] - loss: 0.296214  acc: 100.0000%(16/16)
Batch[5] - loss: 0.489133  acc: 93.0000%(15/16)
Batch[6] - loss: 0.481600  acc: 93.0000%(15/16)
Batch[7] - loss: 0.778313  acc: 100.0000%(16/16)
Batch[8] - loss: 0.572952  acc: 100.0000%(16/16)
Batch[9] - loss: 0.183157  acc: 100.0000%(16/16)
Batch[10] - loss: 0.160472  acc: 100.0000%(16/16)
Batch[11] - loss: 0.278043  acc: 100.0000%(16/16)
Batch[12] - loss: 0.334012  acc: 93.0000%(15/16)
Batch[13] - loss: 0.429800  acc: 93.0000%(15/16)
Batch[14] - loss: 0.311692  acc: 100.0000%(16/16)
Batch[15] - loss: 0.477065  acc: 93.0000%(15/16)
Batch[16] - loss: 0.303072  acc: 100.0000%(16/16)
Batch[17] - loss: 0.487897  acc: 100.0000%(16/16)
Batch[18] - loss: 0.443713  acc: 93.0000%(15/16)
Batch[19] - loss: 0.232192  acc: 100.0000%(16/16)
Batch[20] - loss: 0.372797  acc: 100.0000%(16/16)
Batch[21] - loss: 0.376484  acc: 100.0000%(16/16)
Batch[22] - loss: 0.209047  acc: 100.0000%(16/16)
Batch[23] - loss: 0.434439  acc: 100.0000%(16/16)
Batch[24] - loss: 0.452540  acc: 93.0000%(15/16)
Batch[25] - loss: 0.299652  acc: 100.0000%(16/16)
Batch[26] - loss: 0.330176  acc: 100.0000%(16/16)
Batch[27] - loss: 0.277184  acc: 87.0000%(14/16)
Batch[28] - loss: 0.435986  acc: 93.0000%(15/16)
Batch[29] - loss: 0.510200  acc: 93.0000%(15/16)
Batch[30] - loss: 0.184062  acc: 100.0000%(16/16)
Batch[31] - loss: 0.346892  acc: 100.0000%(16/16)
Batch[32] - loss: 0.285817  acc: 93.0000%(15/16)
Batch[33] - loss: 0.337847  acc: 93.0000%(15/16)
Batch[34] - loss: 0.295733  acc: 100.0000%(16/16)
Batch[35] - loss: 0.211592  acc: 100.0000%(16/16)
Batch[36] - loss: 0.242419  acc: 93.0000%(15/16)
Batch[37] - loss: 0.304783  acc: 93.0000%(15/16)
Batch[38] - loss: 0.205575  acc: 100.0000%(16/16)
Batch[39] - loss: 0.435738  acc: 87.0000%(14/16)
Batch[40] - loss: 0.454027  acc: 93.0000%(15/16)
Batch[41] - loss: 0.619555  acc: 87.0000%(14/16)
Batch[42] - loss: 0.147234  acc: 100.0000%(16/16)
Batch[43] - loss: 0.272982  acc: 100.0000%(16/16)
Batch[44] - loss: 0.267810  acc: 100.0000%(16/16)
Batch[45] - loss: 0.241579  acc: 100.0000%(16/16)
Batch[46] - loss: 0.348231  acc: 100.0000%(16/16)
Batch[47] - loss: 0.387448  acc: 93.0000%(15/16)
Batch[48] - loss: 0.382453  acc: 100.0000%(16/16)
Batch[49] - loss: 0.217895  acc: 100.0000%(16/16)
Batch[50] - loss: 0.452393  acc: 93.0000%(15/16)
Batch[51] - loss: 0.165621  acc: 100.0000%(16/16)
Batch[52] - loss: 0.298983  acc: 100.0000%(16/16)
Batch[53] - loss: 0.229350  acc: 100.0000%(16/16)
Batch[54] - loss: 0.333979  acc: 100.0000%(16/16)
Batch[55] - loss: 0.378348  acc: 100.0000%(16/16)
Batch[56] - loss: 0.185545  acc: 100.0000%(16/16)
Batch[57] - loss: 0.124561  acc: 100.0000%(16/16)
Batch[58] - loss: 0.334297  acc: 87.0000%(14/16)
Batch[59] - loss: 0.275409  acc: 100.0000%(16/16)
Batch[60] - loss: 0.244474  acc: 100.0000%(16/16)
Batch[61] - loss: 0.338417  acc: 93.0000%(15/16)
Batch[62] - loss: 0.860723  acc: 92.0000%(12/13)
Average loss:0.343107 average acc:97.000000%
Val set acc: 0.7516778523489933
Best val set acc: 0

Epoch  6 / 18
Batch[0] - loss: 0.211666  acc: 100.0000%(16/16)
Batch[1] - loss: 0.156863  acc: 100.0000%(16/16)
Batch[2] - loss: 0.128323  acc: 100.0000%(16/16)
Batch[3] - loss: 0.234462  acc: 93.0000%(15/16)
Batch[4] - loss: 0.215898  acc: 100.0000%(16/16)
Batch[5] - loss: 0.395553  acc: 93.0000%(15/16)
Batch[6] - loss: 0.116912  acc: 100.0000%(16/16)
Batch[7] - loss: 0.531466  acc: 93.0000%(15/16)
Batch[8] - loss: 0.183777  acc: 100.0000%(16/16)
Batch[9] - loss: 0.117158  acc: 100.0000%(16/16)
Batch[10] - loss: 0.129016  acc: 100.0000%(16/16)
Batch[11] - loss: 0.127007  acc: 100.0000%(16/16)
Batch[12] - loss: 0.126640  acc: 100.0000%(16/16)
Batch[13] - loss: 0.293574  acc: 93.0000%(15/16)
Batch[14] - loss: 0.213581  acc: 100.0000%(16/16)
Batch[15] - loss: 0.137616  acc: 93.0000%(15/16)
Batch[16] - loss: 0.076470  acc: 100.0000%(16/16)
Batch[17] - loss: 0.154136  acc: 100.0000%(16/16)
Batch[18] - loss: 0.102332  acc: 100.0000%(16/16)
Batch[19] - loss: 0.151785  acc: 100.0000%(16/16)
Batch[20] - loss: 0.090734  acc: 100.0000%(16/16)
Batch[21] - loss: 0.210024  acc: 100.0000%(16/16)
Batch[22] - loss: 0.128496  acc: 100.0000%(16/16)
Batch[23] - loss: 0.099205  acc: 100.0000%(16/16)
Batch[24] - loss: 0.232080  acc: 100.0000%(16/16)
Batch[25] - loss: 0.179466  acc: 100.0000%(16/16)
Batch[26] - loss: 0.074275  acc: 100.0000%(16/16)
Batch[27] - loss: 0.215574  acc: 100.0000%(16/16)
Batch[28] - loss: 0.147343  acc: 100.0000%(16/16)
Batch[29] - loss: 0.229970  acc: 100.0000%(16/16)
Batch[30] - loss: 0.164894  acc: 93.0000%(15/16)
Batch[31] - loss: 0.154904  acc: 100.0000%(16/16)
Batch[32] - loss: 0.138781  acc: 100.0000%(16/16)
Batch[33] - loss: 0.201509  acc: 93.0000%(15/16)
Batch[34] - loss: 0.060191  acc: 100.0000%(16/16)
Batch[35] - loss: 0.130991  acc: 100.0000%(16/16)
Batch[36] - loss: 0.378101  acc: 100.0000%(16/16)
Batch[37] - loss: 0.156337  acc: 100.0000%(16/16)
Batch[38] - loss: 0.148577  acc: 100.0000%(16/16)
Batch[39] - loss: 0.091356  acc: 100.0000%(16/16)
Batch[40] - loss: 0.120810  acc: 100.0000%(16/16)
Batch[41] - loss: 0.139414  acc: 100.0000%(16/16)
Batch[42] - loss: 0.129510  acc: 100.0000%(16/16)
Batch[43] - loss: 0.114552  acc: 100.0000%(16/16)
Batch[44] - loss: 0.432567  acc: 87.0000%(14/16)
Batch[45] - loss: 0.211397  acc: 100.0000%(16/16)
Batch[46] - loss: 0.037380  acc: 100.0000%(16/16)
Batch[47] - loss: 0.104296  acc: 100.0000%(16/16)
Batch[48] - loss: 0.081895  acc: 100.0000%(16/16)
Batch[49] - loss: 0.117256  acc: 100.0000%(16/16)
Batch[50] - loss: 0.141017  acc: 93.0000%(15/16)
Batch[51] - loss: 0.164919  acc: 100.0000%(16/16)
Batch[52] - loss: 0.182369  acc: 93.0000%(15/16)
Batch[53] - loss: 0.065695  acc: 100.0000%(16/16)
Batch[54] - loss: 0.201231  acc: 93.0000%(15/16)
Batch[55] - loss: 0.163299  acc: 100.0000%(16/16)
Batch[56] - loss: 0.210389  acc: 93.0000%(15/16)
Batch[57] - loss: 0.152620  acc: 93.0000%(15/16)
Batch[58] - loss: 0.124725  acc: 100.0000%(16/16)
Batch[59] - loss: 0.161749  acc: 100.0000%(16/16)
Batch[60] - loss: 0.179136  acc: 93.0000%(15/16)
Batch[61] - loss: 0.167923  acc: 100.0000%(16/16)
Batch[62] - loss: 0.118830  acc: 100.0000%(13/13)
Average loss:0.167619 average acc:98.000000%
Val set acc: 0.7785234899328859
Best val set acc: 0

Epoch  7 / 18
Batch[0] - loss: 0.069959  acc: 100.0000%(16/16)
Batch[1] - loss: 0.107970  acc: 100.0000%(16/16)
Batch[2] - loss: 0.163640  acc: 100.0000%(16/16)
Batch[3] - loss: 0.069682  acc: 100.0000%(16/16)
Batch[4] - loss: 0.097178  acc: 100.0000%(16/16)
Batch[5] - loss: 0.371467  acc: 87.0000%(14/16)
Batch[6] - loss: 0.097400  acc: 100.0000%(16/16)
Batch[7] - loss: 0.128184  acc: 100.0000%(16/16)
Batch[8] - loss: 0.064510  acc: 100.0000%(16/16)
Batch[9] - loss: 0.089811  acc: 100.0000%(16/16)
Batch[10] - loss: 0.113256  acc: 93.0000%(15/16)
Batch[11] - loss: 0.039633  acc: 100.0000%(16/16)
Batch[12] - loss: 0.083316  acc: 100.0000%(16/16)
Batch[13] - loss: 0.054243  acc: 100.0000%(16/16)
Batch[14] - loss: 0.092465  acc: 100.0000%(16/16)
Batch[15] - loss: 0.089466  acc: 100.0000%(16/16)
Batch[16] - loss: 0.086086  acc: 100.0000%(16/16)
Batch[17] - loss: 0.069765  acc: 100.0000%(16/16)
Batch[18] - loss: 0.085263  acc: 100.0000%(16/16)
Batch[19] - loss: 0.120425  acc: 100.0000%(16/16)
Batch[20] - loss: 0.124346  acc: 100.0000%(16/16)
Batch[21] - loss: 0.058545  acc: 100.0000%(16/16)
Batch[22] - loss: 0.052407  acc: 100.0000%(16/16)
Batch[23] - loss: 0.050802  acc: 100.0000%(16/16)
Batch[24] - loss: 0.029939  acc: 100.0000%(16/16)
Batch[25] - loss: 0.140304  acc: 100.0000%(16/16)
Batch[26] - loss: 0.065502  acc: 100.0000%(16/16)
Batch[27] - loss: 0.123846  acc: 100.0000%(16/16)
Batch[28] - loss: 0.177148  acc: 100.0000%(16/16)
Batch[29] - loss: 0.054259  acc: 100.0000%(16/16)
Batch[30] - loss: 0.058924  acc: 100.0000%(16/16)
Batch[31] - loss: 0.061166  acc: 100.0000%(16/16)
Batch[32] - loss: 0.082741  acc: 100.0000%(16/16)
Batch[33] - loss: 0.068074  acc: 100.0000%(16/16)
Batch[34] - loss: 0.055084  acc: 100.0000%(16/16)
Batch[35] - loss: 0.040086  acc: 100.0000%(16/16)
Batch[36] - loss: 0.083685  acc: 100.0000%(16/16)
Batch[37] - loss: 0.078350  acc: 100.0000%(16/16)
Batch[38] - loss: 0.073427  acc: 100.0000%(16/16)
Batch[39] - loss: 0.067313  acc: 100.0000%(16/16)
Batch[40] - loss: 0.020882  acc: 100.0000%(16/16)
Batch[41] - loss: 0.120279  acc: 100.0000%(16/16)
Batch[42] - loss: 0.048530  acc: 100.0000%(16/16)
Batch[43] - loss: 0.063541  acc: 100.0000%(16/16)
Batch[44] - loss: 0.097940  acc: 100.0000%(16/16)
Batch[45] - loss: 0.027381  acc: 100.0000%(16/16)
Batch[46] - loss: 0.075211  acc: 100.0000%(16/16)
Batch[47] - loss: 0.084314  acc: 93.0000%(15/16)
Batch[48] - loss: 0.095866  acc: 100.0000%(16/16)
Batch[49] - loss: 0.028603  acc: 100.0000%(16/16)
Batch[50] - loss: 0.094077  acc: 100.0000%(16/16)
Batch[51] - loss: 0.067081  acc: 100.0000%(16/16)
Batch[52] - loss: 0.051723  acc: 100.0000%(16/16)
Batch[53] - loss: 0.082511  acc: 100.0000%(16/16)
Batch[54] - loss: 0.072814  acc: 100.0000%(16/16)
Batch[55] - loss: 0.050033  acc: 100.0000%(16/16)
Batch[56] - loss: 0.058660  acc: 100.0000%(16/16)
Batch[57] - loss: 0.045111  acc: 100.0000%(16/16)
Batch[58] - loss: 0.117226  acc: 93.0000%(15/16)
Batch[59] - loss: 0.040669  acc: 100.0000%(16/16)
Batch[60] - loss: 0.031736  acc: 100.0000%(16/16)
Batch[61] - loss: 0.170001  acc: 93.0000%(15/16)
Batch[62] - loss: 0.036428  acc: 100.0000%(13/13)
Average loss:0.082862 average acc:99.000000%
Val set acc: 0.7583892617449665
Best val set acc: 0

Epoch  8 / 18
Batch[0] - loss: 0.057327  acc: 100.0000%(16/16)
Batch[1] - loss: 0.084016  acc: 100.0000%(16/16)
Batch[2] - loss: 0.068863  acc: 100.0000%(16/16)
Batch[3] - loss: 0.051493  acc: 100.0000%(16/16)
Batch[4] - loss: 0.039823  acc: 100.0000%(16/16)
Batch[5] - loss: 0.050159  acc: 100.0000%(16/16)
Batch[6] - loss: 0.056753  acc: 100.0000%(16/16)
Batch[7] - loss: 0.076319  acc: 100.0000%(16/16)
Batch[8] - loss: 0.077034  acc: 100.0000%(16/16)
Batch[9] - loss: 0.034431  acc: 100.0000%(16/16)
Batch[10] - loss: 0.045725  acc: 100.0000%(16/16)
Batch[11] - loss: 0.083705  acc: 100.0000%(16/16)
Batch[12] - loss: 0.016318  acc: 100.0000%(16/16)
Batch[13] - loss: 0.049135  acc: 100.0000%(16/16)
Batch[14] - loss: 0.042457  acc: 100.0000%(16/16)
Batch[15] - loss: 0.058260  acc: 100.0000%(16/16)
Batch[16] - loss: 0.047009  acc: 100.0000%(16/16)
Batch[17] - loss: 0.043933  acc: 100.0000%(16/16)
Batch[18] - loss: 0.063285  acc: 100.0000%(16/16)
Batch[19] - loss: 0.045509  acc: 100.0000%(16/16)
Batch[20] - loss: 0.075252  acc: 100.0000%(16/16)
Batch[21] - loss: 0.043200  acc: 100.0000%(16/16)
Batch[22] - loss: 0.081179  acc: 100.0000%(16/16)
Batch[23] - loss: 0.030389  acc: 100.0000%(16/16)
Batch[24] - loss: 0.076808  acc: 100.0000%(16/16)
Batch[25] - loss: 0.040553  acc: 100.0000%(16/16)
Batch[26] - loss: 0.014975  acc: 100.0000%(16/16)
Batch[27] - loss: 0.230391  acc: 87.0000%(14/16)
Batch[28] - loss: 0.030398  acc: 100.0000%(16/16)
Batch[29] - loss: 0.057345  acc: 100.0000%(16/16)
Batch[30] - loss: 0.018412  acc: 100.0000%(16/16)
Batch[31] - loss: 0.045537  acc: 100.0000%(16/16)
Batch[32] - loss: 0.042445  acc: 100.0000%(16/16)
Batch[33] - loss: 0.078483  acc: 100.0000%(16/16)
Batch[34] - loss: 0.035223  acc: 100.0000%(16/16)
Batch[35] - loss: 0.061059  acc: 100.0000%(16/16)
Batch[36] - loss: 0.023994  acc: 100.0000%(16/16)
Batch[37] - loss: 0.023778  acc: 100.0000%(16/16)
Batch[38] - loss: 0.042723  acc: 100.0000%(16/16)
Batch[39] - loss: 0.078788  acc: 100.0000%(16/16)
Batch[40] - loss: 0.051778  acc: 100.0000%(16/16)
Batch[41] - loss: 0.055860  acc: 100.0000%(16/16)
Batch[42] - loss: 0.053092  acc: 100.0000%(16/16)
Batch[43] - loss: 0.028732  acc: 100.0000%(16/16)
Batch[44] - loss: 0.028282  acc: 100.0000%(16/16)
Batch[45] - loss: 0.020725  acc: 100.0000%(16/16)
Batch[46] - loss: 0.038904  acc: 100.0000%(16/16)
Batch[47] - loss: 0.078094  acc: 100.0000%(16/16)
Batch[48] - loss: 0.033834  acc: 100.0000%(16/16)
Batch[49] - loss: 0.054014  acc: 100.0000%(16/16)
Batch[50] - loss: 0.031832  acc: 100.0000%(16/16)
Batch[51] - loss: 0.015261  acc: 100.0000%(16/16)
Batch[52] - loss: 0.041566  acc: 100.0000%(16/16)
Batch[53] - loss: 0.019093  acc: 100.0000%(16/16)
Batch[54] - loss: 0.040656  acc: 100.0000%(16/16)
Batch[55] - loss: 0.020466  acc: 100.0000%(16/16)
Batch[56] - loss: 0.039581  acc: 100.0000%(16/16)
Batch[57] - loss: 0.069868  acc: 93.0000%(15/16)
Batch[58] - loss: 0.044278  acc: 100.0000%(16/16)
Batch[59] - loss: 0.029263  acc: 100.0000%(16/16)
Batch[60] - loss: 0.034587  acc: 100.0000%(16/16)
Batch[61] - loss: 0.017437  acc: 100.0000%(16/16)
Batch[62] - loss: 0.005658  acc: 100.0000%(13/13)
Average loss:0.048815 average acc:99.000000%
Val set acc: 0.7785234899328859
Best val set acc: 0

Epoch  9 / 18
Batch[0] - loss: 0.041291  acc: 100.0000%(16/16)
Batch[1] - loss: 0.022170  acc: 100.0000%(16/16)
Batch[2] - loss: 0.045314  acc: 100.0000%(16/16)
Batch[3] - loss: 0.021861  acc: 100.0000%(16/16)
Batch[4] - loss: 0.015460  acc: 100.0000%(16/16)
Batch[5] - loss: 0.031321  acc: 100.0000%(16/16)
Batch[6] - loss: 0.036074  acc: 100.0000%(16/16)
Batch[7] - loss: 0.023365  acc: 100.0000%(16/16)
Batch[8] - loss: 0.038584  acc: 100.0000%(16/16)
Batch[9] - loss: 0.012472  acc: 100.0000%(16/16)
Batch[10] - loss: 0.034867  acc: 100.0000%(16/16)
Batch[11] - loss: 0.031845  acc: 100.0000%(16/16)
Batch[12] - loss: 0.047320  acc: 100.0000%(16/16)
Batch[13] - loss: 0.047594  acc: 100.0000%(16/16)
Batch[14] - loss: 0.058030  acc: 100.0000%(16/16)
Batch[15] - loss: 0.042649  acc: 100.0000%(16/16)
Batch[16] - loss: 0.011767  acc: 100.0000%(16/16)
Batch[17] - loss: 0.018134  acc: 100.0000%(16/16)
Batch[18] - loss: 0.020257  acc: 100.0000%(16/16)
Batch[19] - loss: 0.038452  acc: 100.0000%(16/16)
Batch[20] - loss: 0.037407  acc: 100.0000%(16/16)
Batch[21] - loss: 0.036112  acc: 100.0000%(16/16)
Batch[22] - loss: 0.037177  acc: 100.0000%(16/16)
Batch[23] - loss: 0.009779  acc: 100.0000%(16/16)
Batch[24] - loss: 0.031107  acc: 100.0000%(16/16)
Batch[25] - loss: 0.024974  acc: 100.0000%(16/16)
Batch[26] - loss: 0.043435  acc: 100.0000%(16/16)
Batch[27] - loss: 0.024262  acc: 100.0000%(16/16)
Batch[28] - loss: 0.027461  acc: 100.0000%(16/16)
Batch[29] - loss: 0.013276  acc: 100.0000%(16/16)
Batch[30] - loss: 0.077267  acc: 100.0000%(16/16)
Batch[31] - loss: 0.020887  acc: 100.0000%(16/16)
Batch[32] - loss: 0.084809  acc: 93.0000%(15/16)
Batch[33] - loss: 0.023516  acc: 100.0000%(16/16)
Batch[34] - loss: 0.009150  acc: 100.0000%(16/16)
Batch[35] - loss: 0.044508  acc: 100.0000%(16/16)
Batch[36] - loss: 0.016264  acc: 100.0000%(16/16)
Batch[37] - loss: 0.020053  acc: 100.0000%(16/16)
Batch[38] - loss: 0.044962  acc: 100.0000%(16/16)
Batch[39] - loss: 0.014113  acc: 100.0000%(16/16)
Batch[40] - loss: 0.022612  acc: 100.0000%(16/16)
Batch[41] - loss: 0.041713  acc: 100.0000%(16/16)
Batch[42] - loss: 0.039956  acc: 100.0000%(16/16)
Batch[43] - loss: 0.021854  acc: 100.0000%(16/16)
Batch[44] - loss: 0.021412  acc: 100.0000%(16/16)
Batch[45] - loss: 0.019207  acc: 100.0000%(16/16)
Batch[46] - loss: 0.063335  acc: 100.0000%(16/16)
Batch[47] - loss: 0.018101  acc: 100.0000%(16/16)
Batch[48] - loss: 0.017752  acc: 100.0000%(16/16)
Batch[49] - loss: 0.023079  acc: 100.0000%(16/16)
Batch[50] - loss: 0.019800  acc: 100.0000%(16/16)
Batch[51] - loss: 0.034752  acc: 100.0000%(16/16)
Batch[52] - loss: 0.041766  acc: 100.0000%(16/16)
Batch[53] - loss: 0.251450  acc: 93.0000%(15/16)
Batch[54] - loss: 0.069266  acc: 100.0000%(16/16)
Batch[55] - loss: 0.038881  acc: 100.0000%(16/16)
Batch[56] - loss: 0.043641  acc: 100.0000%(16/16)
Batch[57] - loss: 0.017977  acc: 100.0000%(16/16)
Batch[58] - loss: 0.038838  acc: 100.0000%(16/16)
Batch[59] - loss: 0.024680  acc: 100.0000%(16/16)
Batch[60] - loss: 0.037738  acc: 100.0000%(16/16)
Batch[61] - loss: 0.053565  acc: 100.0000%(16/16)
Batch[62] - loss: 0.021371  acc: 100.0000%(13/13)
Average loss:0.035906 average acc:99.000000%
Val set acc: 0.7651006711409396
Best val set acc: 0
              precision    recall  f1-score   support

          NR    0.78125   0.65789   0.71429        38
          FR    0.67500   0.72973   0.70130        37
          TR    0.70732   0.78378   0.74359        37
          UR    0.91667   0.89189   0.90411        37

    accuracy                        0.76510       149
   macro avg    0.77006   0.76583   0.76582       149
weighted avg    0.77013   0.76510   0.76548       149

save model!!!

Epoch  10 / 18
Batch[0] - loss: 0.034418  acc: 100.0000%(16/16)
Batch[1] - loss: 0.034654  acc: 100.0000%(16/16)
Batch[2] - loss: 0.035278  acc: 100.0000%(16/16)
Batch[3] - loss: 0.022721  acc: 100.0000%(16/16)
Batch[4] - loss: 0.019269  acc: 100.0000%(16/16)
Batch[5] - loss: 0.018524  acc: 100.0000%(16/16)
Batch[6] - loss: 0.009589  acc: 100.0000%(16/16)
Batch[7] - loss: 0.028940  acc: 100.0000%(16/16)
Batch[8] - loss: 0.017241  acc: 100.0000%(16/16)
Batch[9] - loss: 0.017929  acc: 100.0000%(16/16)
Batch[10] - loss: 0.263438  acc: 93.0000%(15/16)
Batch[11] - loss: 0.052692  acc: 100.0000%(16/16)
Batch[12] - loss: 0.013146  acc: 100.0000%(16/16)
Batch[13] - loss: 0.012586  acc: 100.0000%(16/16)
Batch[14] - loss: 0.035727  acc: 100.0000%(16/16)
Batch[15] - loss: 0.061486  acc: 100.0000%(16/16)
Batch[16] - loss: 0.016697  acc: 100.0000%(16/16)
Batch[17] - loss: 0.016420  acc: 100.0000%(16/16)
Batch[18] - loss: 0.011872  acc: 100.0000%(16/16)
Batch[19] - loss: 0.038804  acc: 100.0000%(16/16)
Batch[20] - loss: 0.027795  acc: 100.0000%(16/16)
Batch[21] - loss: 0.020326  acc: 100.0000%(16/16)
Batch[22] - loss: 0.010763  acc: 100.0000%(16/16)
Batch[23] - loss: 0.013348  acc: 100.0000%(16/16)
Batch[24] - loss: 0.012323  acc: 100.0000%(16/16)
Batch[25] - loss: 0.020394  acc: 100.0000%(16/16)
Batch[26] - loss: 0.011855  acc: 100.0000%(16/16)
Batch[27] - loss: 0.046808  acc: 100.0000%(16/16)
Batch[28] - loss: 0.021456  acc: 100.0000%(16/16)
Batch[29] - loss: 0.017219  acc: 100.0000%(16/16)
Batch[30] - loss: 0.012178  acc: 100.0000%(16/16)
Batch[31] - loss: 0.014298  acc: 100.0000%(16/16)
Batch[32] - loss: 0.100855  acc: 93.0000%(15/16)
Batch[33] - loss: 0.060343  acc: 100.0000%(16/16)
Batch[34] - loss: 0.075543  acc: 100.0000%(16/16)
Batch[35] - loss: 0.011398  acc: 100.0000%(16/16)
Batch[36] - loss: 0.022993  acc: 100.0000%(16/16)
Batch[37] - loss: 0.023494  acc: 100.0000%(16/16)
Batch[38] - loss: 0.011454  acc: 100.0000%(16/16)
Batch[39] - loss: 0.011000  acc: 100.0000%(16/16)
Batch[40] - loss: 0.012791  acc: 100.0000%(16/16)
Batch[41] - loss: 0.012295  acc: 100.0000%(16/16)
Batch[42] - loss: 0.013599  acc: 100.0000%(16/16)
Batch[43] - loss: 0.053106  acc: 100.0000%(16/16)
Batch[44] - loss: 0.020887  acc: 100.0000%(16/16)
Batch[45] - loss: 0.026049  acc: 100.0000%(16/16)
Batch[46] - loss: 0.018463  acc: 100.0000%(16/16)
Batch[47] - loss: 0.033770  acc: 100.0000%(16/16)
Batch[48] - loss: 0.010083  acc: 100.0000%(16/16)
Batch[49] - loss: 0.028702  acc: 100.0000%(16/16)
Batch[50] - loss: 0.027120  acc: 100.0000%(16/16)
Batch[51] - loss: 0.009424  acc: 100.0000%(16/16)
Batch[52] - loss: 0.025517  acc: 100.0000%(16/16)
Batch[53] - loss: 0.041793  acc: 100.0000%(16/16)
Batch[54] - loss: 0.032099  acc: 100.0000%(16/16)
Batch[55] - loss: 0.011092  acc: 100.0000%(16/16)
Batch[56] - loss: 0.017631  acc: 100.0000%(16/16)
Batch[57] - loss: 0.023868  acc: 100.0000%(16/16)
Batch[58] - loss: 0.021274  acc: 100.0000%(16/16)
Batch[59] - loss: 0.039386  acc: 100.0000%(16/16)
Batch[60] - loss: 0.011025  acc: 100.0000%(16/16)
Batch[61] - loss: 0.018414  acc: 100.0000%(16/16)
Batch[62] - loss: 0.021276  acc: 100.0000%(13/13)
Average loss:0.029158 average acc:99.000000%
Val set acc: 0.7718120805369127
Best val set acc: 0.7651006711409396
              precision    recall  f1-score   support

          NR    0.75758   0.65789   0.70423        38
          FR    0.68293   0.75676   0.71795        37
          TR    0.82353   0.75676   0.78873        37
          UR    0.82927   0.91892   0.87179        37

    accuracy                        0.77181       149
   macro avg    0.77333   0.77258   0.77068       149
weighted avg    0.77322   0.77181   0.77023       149

save model!!!

Epoch  11 / 18
Batch[0] - loss: 0.013812  acc: 100.0000%(16/16)
Batch[1] - loss: 0.016858  acc: 100.0000%(16/16)
Batch[2] - loss: 0.004557  acc: 100.0000%(16/16)
Batch[3] - loss: 0.042903  acc: 100.0000%(16/16)
Batch[4] - loss: 0.023433  acc: 100.0000%(16/16)
Batch[5] - loss: 0.024400  acc: 100.0000%(16/16)
Batch[6] - loss: 0.007310  acc: 100.0000%(16/16)
Batch[7] - loss: 0.007855  acc: 100.0000%(16/16)
Batch[8] - loss: 0.018213  acc: 100.0000%(16/16)
Batch[9] - loss: 0.008269  acc: 100.0000%(16/16)
Batch[10] - loss: 0.026925  acc: 100.0000%(16/16)
Batch[11] - loss: 0.020727  acc: 100.0000%(16/16)
Batch[12] - loss: 0.010506  acc: 100.0000%(16/16)
Batch[13] - loss: 0.006580  acc: 100.0000%(16/16)
Batch[14] - loss: 0.010642  acc: 100.0000%(16/16)
Batch[15] - loss: 0.031648  acc: 100.0000%(16/16)
Batch[16] - loss: 0.008816  acc: 100.0000%(16/16)
Batch[17] - loss: 0.008558  acc: 100.0000%(16/16)
Batch[18] - loss: 0.009044  acc: 100.0000%(16/16)
Batch[19] - loss: 0.018572  acc: 100.0000%(16/16)
Batch[20] - loss: 0.004472  acc: 100.0000%(16/16)
Batch[21] - loss: 0.010989  acc: 100.0000%(16/16)
Batch[22] - loss: 0.012062  acc: 100.0000%(16/16)
Batch[23] - loss: 0.008559  acc: 100.0000%(16/16)
Batch[24] - loss: 0.021279  acc: 100.0000%(16/16)
Batch[25] - loss: 0.013255  acc: 100.0000%(16/16)
Batch[26] - loss: 0.010300  acc: 100.0000%(16/16)
Batch[27] - loss: 0.007762  acc: 100.0000%(16/16)
Batch[28] - loss: 0.010058  acc: 100.0000%(16/16)
Batch[29] - loss: 0.016899  acc: 100.0000%(16/16)
Batch[30] - loss: 0.012171  acc: 100.0000%(16/16)
Batch[31] - loss: 0.005385  acc: 100.0000%(16/16)
Batch[32] - loss: 0.016242  acc: 100.0000%(16/16)
Batch[33] - loss: 0.019506  acc: 100.0000%(16/16)
Batch[34] - loss: 0.020861  acc: 100.0000%(16/16)
Batch[35] - loss: 0.009161  acc: 100.0000%(16/16)
Batch[36] - loss: 0.025695  acc: 100.0000%(16/16)
Batch[37] - loss: 0.012516  acc: 100.0000%(16/16)
Batch[38] - loss: 0.008747  acc: 100.0000%(16/16)
Batch[39] - loss: 0.009193  acc: 100.0000%(16/16)
Batch[40] - loss: 0.020449  acc: 100.0000%(16/16)
Batch[41] - loss: 0.018997  acc: 100.0000%(16/16)
Batch[42] - loss: 0.006031  acc: 100.0000%(16/16)
Batch[43] - loss: 0.007765  acc: 100.0000%(16/16)
Batch[44] - loss: 0.006384  acc: 100.0000%(16/16)
Batch[45] - loss: 0.012330  acc: 100.0000%(16/16)
Batch[46] - loss: 0.006204  acc: 100.0000%(16/16)
Batch[47] - loss: 0.010606  acc: 100.0000%(16/16)
Batch[48] - loss: 0.009287  acc: 100.0000%(16/16)
Batch[49] - loss: 0.015166  acc: 100.0000%(16/16)
Batch[50] - loss: 0.008234  acc: 100.0000%(16/16)
Batch[51] - loss: 0.010731  acc: 100.0000%(16/16)
Batch[52] - loss: 0.008231  acc: 100.0000%(16/16)
Batch[53] - loss: 0.008968  acc: 100.0000%(16/16)
Batch[54] - loss: 0.010344  acc: 100.0000%(16/16)
Batch[55] - loss: 0.012505  acc: 100.0000%(16/16)
Batch[56] - loss: 0.013517  acc: 100.0000%(16/16)
Batch[57] - loss: 0.015936  acc: 100.0000%(16/16)
Batch[58] - loss: 0.008131  acc: 100.0000%(16/16)
Batch[59] - loss: 0.011658  acc: 100.0000%(16/16)
Batch[60] - loss: 0.002076  acc: 100.0000%(16/16)
Batch[61] - loss: 0.007921  acc: 100.0000%(16/16)
Batch[62] - loss: 0.004453  acc: 100.0000%(13/13)
Average loss:0.012868 average acc:100.000000%
Val set acc: 0.7785234899328859
Best val set acc: 0.7718120805369127
              precision    recall  f1-score   support

          NR    0.71053   0.71053   0.71053        38
          FR    0.70000   0.75676   0.72727        37
          TR    0.78378   0.78378   0.78378        37
          UR    0.94118   0.86486   0.90141        37

    accuracy                        0.77852       149
   macro avg    0.78387   0.77898   0.78075       149
weighted avg    0.78338   0.77852   0.78028       149

save model!!!

Epoch  12 / 18
Batch[0] - loss: 0.009216  acc: 100.0000%(16/16)
Batch[1] - loss: 0.007421  acc: 100.0000%(16/16)
Batch[2] - loss: 0.002966  acc: 100.0000%(16/16)
Batch[3] - loss: 0.008564  acc: 100.0000%(16/16)
Batch[4] - loss: 0.009577  acc: 100.0000%(16/16)
Batch[5] - loss: 0.006053  acc: 100.0000%(16/16)
Batch[6] - loss: 0.007977  acc: 100.0000%(16/16)
Batch[7] - loss: 0.003949  acc: 100.0000%(16/16)
Batch[8] - loss: 0.015207  acc: 100.0000%(16/16)
Batch[9] - loss: 0.010684  acc: 100.0000%(16/16)
Batch[10] - loss: 0.018321  acc: 100.0000%(16/16)
Batch[11] - loss: 0.008791  acc: 100.0000%(16/16)
Batch[12] - loss: 0.044277  acc: 100.0000%(16/16)
Batch[13] - loss: 0.010841  acc: 100.0000%(16/16)
Batch[14] - loss: 0.007876  acc: 100.0000%(16/16)
Batch[15] - loss: 0.006391  acc: 100.0000%(16/16)
Batch[16] - loss: 0.003846  acc: 100.0000%(16/16)
Batch[17] - loss: 0.006556  acc: 100.0000%(16/16)
Batch[18] - loss: 0.012650  acc: 100.0000%(16/16)
Batch[19] - loss: 0.021027  acc: 100.0000%(16/16)
Batch[20] - loss: 0.018924  acc: 100.0000%(16/16)
Batch[21] - loss: 0.012830  acc: 100.0000%(16/16)
Batch[22] - loss: 0.009913  acc: 100.0000%(16/16)
Batch[23] - loss: 0.010353  acc: 100.0000%(16/16)
Batch[24] - loss: 0.007232  acc: 100.0000%(16/16)
Batch[25] - loss: 0.012593  acc: 100.0000%(16/16)
Batch[26] - loss: 0.075492  acc: 93.0000%(15/16)
Batch[27] - loss: 0.004818  acc: 100.0000%(16/16)
Batch[28] - loss: 0.005125  acc: 100.0000%(16/16)
Batch[29] - loss: 0.016333  acc: 100.0000%(16/16)
Batch[30] - loss: 0.008624  acc: 100.0000%(16/16)
Batch[31] - loss: 0.011087  acc: 100.0000%(16/16)
Batch[32] - loss: 0.007538  acc: 100.0000%(16/16)
Batch[33] - loss: 0.007613  acc: 100.0000%(16/16)
Batch[34] - loss: 0.014036  acc: 100.0000%(16/16)
Batch[35] - loss: 0.010033  acc: 100.0000%(16/16)
Batch[36] - loss: 0.014228  acc: 100.0000%(16/16)
Batch[37] - loss: 0.006404  acc: 100.0000%(16/16)
Batch[38] - loss: 0.022020  acc: 100.0000%(16/16)
Batch[39] - loss: 0.007172  acc: 100.0000%(16/16)
Batch[40] - loss: 0.026510  acc: 100.0000%(16/16)
Batch[41] - loss: 0.018136  acc: 100.0000%(16/16)
Batch[42] - loss: 0.016278  acc: 100.0000%(16/16)
Batch[43] - loss: 0.011766  acc: 100.0000%(16/16)
Batch[44] - loss: 0.031210  acc: 100.0000%(16/16)
Batch[45] - loss: 0.013906  acc: 100.0000%(16/16)
Batch[46] - loss: 0.003725  acc: 100.0000%(16/16)
Batch[47] - loss: 0.011554  acc: 100.0000%(16/16)
Batch[48] - loss: 0.024665  acc: 100.0000%(16/16)
Batch[49] - loss: 0.007447  acc: 100.0000%(16/16)
Batch[50] - loss: 0.010242  acc: 100.0000%(16/16)
Batch[51] - loss: 0.012691  acc: 100.0000%(16/16)
Batch[52] - loss: 0.006496  acc: 100.0000%(16/16)
Batch[53] - loss: 0.013165  acc: 100.0000%(16/16)
Batch[54] - loss: 0.011677  acc: 100.0000%(16/16)
Batch[55] - loss: 0.008934  acc: 100.0000%(16/16)
Batch[56] - loss: 0.006586  acc: 100.0000%(16/16)
Batch[57] - loss: 0.006465  acc: 100.0000%(16/16)
Batch[58] - loss: 0.013662  acc: 100.0000%(16/16)
Batch[59] - loss: 0.005817  acc: 100.0000%(16/16)
Batch[60] - loss: 0.006015  acc: 100.0000%(16/16)
Batch[61] - loss: 0.006449  acc: 100.0000%(16/16)
Batch[62] - loss: 0.080221  acc: 92.0000%(12/13)
Average loss:0.013622 average acc:99.000000%
Val set acc: 0.7919463087248322
Best val set acc: 0.7785234899328859
              precision    recall  f1-score   support

          NR    0.74286   0.68421   0.71233        38
          FR    0.69767   0.81081   0.75000        37
          TR    0.80556   0.78378   0.79452        37
          UR    0.94286   0.89189   0.91667        37

    accuracy                        0.79195       149
   macro avg    0.79724   0.79267   0.79338       149
weighted avg    0.79687   0.79195   0.79284       149

save model!!!

Epoch  13 / 18
Batch[0] - loss: 0.006652  acc: 100.0000%(16/16)
Batch[1] - loss: 0.023366  acc: 100.0000%(16/16)
Batch[2] - loss: 0.007699  acc: 100.0000%(16/16)
Batch[3] - loss: 0.006999  acc: 100.0000%(16/16)
Batch[4] - loss: 0.006820  acc: 100.0000%(16/16)
Batch[5] - loss: 0.005735  acc: 100.0000%(16/16)
Batch[6] - loss: 0.009482  acc: 100.0000%(16/16)
Batch[7] - loss: 0.005806  acc: 100.0000%(16/16)
Batch[8] - loss: 0.019877  acc: 100.0000%(16/16)
Batch[9] - loss: 0.006452  acc: 100.0000%(16/16)
Batch[10] - loss: 0.005111  acc: 100.0000%(16/16)
Batch[11] - loss: 0.009620  acc: 100.0000%(16/16)
Batch[12] - loss: 0.006109  acc: 100.0000%(16/16)
Batch[13] - loss: 0.004063  acc: 100.0000%(16/16)
Batch[14] - loss: 0.009622  acc: 100.0000%(16/16)
Batch[15] - loss: 0.013472  acc: 100.0000%(16/16)
Batch[16] - loss: 0.008338  acc: 100.0000%(16/16)
Batch[17] - loss: 0.018302  acc: 100.0000%(16/16)
Batch[18] - loss: 0.006576  acc: 100.0000%(16/16)
Batch[19] - loss: 0.007793  acc: 100.0000%(16/16)
Batch[20] - loss: 0.009788  acc: 100.0000%(16/16)
Batch[21] - loss: 0.005192  acc: 100.0000%(16/16)
Batch[22] - loss: 0.010738  acc: 100.0000%(16/16)
Batch[23] - loss: 0.010919  acc: 100.0000%(16/16)
Batch[24] - loss: 0.005425  acc: 100.0000%(16/16)
Batch[25] - loss: 0.010647  acc: 100.0000%(16/16)
Batch[26] - loss: 0.012475  acc: 100.0000%(16/16)
Batch[27] - loss: 0.004139  acc: 100.0000%(16/16)
Batch[28] - loss: 0.016825  acc: 100.0000%(16/16)
Batch[29] - loss: 0.005429  acc: 100.0000%(16/16)
Batch[30] - loss: 0.011278  acc: 100.0000%(16/16)
Batch[31] - loss: 0.012010  acc: 100.0000%(16/16)
Batch[32] - loss: 0.004976  acc: 100.0000%(16/16)
Batch[33] - loss: 0.007564  acc: 100.0000%(16/16)
Batch[34] - loss: 0.008777  acc: 100.0000%(16/16)
Batch[35] - loss: 0.032362  acc: 100.0000%(16/16)
Batch[36] - loss: 0.014612  acc: 100.0000%(16/16)
Batch[37] - loss: 0.006556  acc: 100.0000%(16/16)
Batch[38] - loss: 0.014946  acc: 100.0000%(16/16)
Batch[39] - loss: 0.010996  acc: 100.0000%(16/16)
Batch[40] - loss: 0.004013  acc: 100.0000%(16/16)
Batch[41] - loss: 0.118862  acc: 93.0000%(15/16)
Batch[42] - loss: 0.006594  acc: 100.0000%(16/16)
Batch[43] - loss: 0.007391  acc: 100.0000%(16/16)
Batch[44] - loss: 0.005063  acc: 100.0000%(16/16)
Batch[45] - loss: 0.004069  acc: 100.0000%(16/16)
Batch[46] - loss: 0.011235  acc: 100.0000%(16/16)
Batch[47] - loss: 0.007610  acc: 100.0000%(16/16)
Batch[48] - loss: 0.002785  acc: 100.0000%(16/16)
Batch[49] - loss: 0.007093  acc: 100.0000%(16/16)
Batch[50] - loss: 0.008302  acc: 100.0000%(16/16)
Batch[51] - loss: 0.008203  acc: 100.0000%(16/16)
Batch[52] - loss: 0.002079  acc: 100.0000%(16/16)
Batch[53] - loss: 0.007452  acc: 100.0000%(16/16)
Batch[54] - loss: 0.003984  acc: 100.0000%(16/16)
Batch[55] - loss: 0.006158  acc: 100.0000%(16/16)
Batch[56] - loss: 0.007083  acc: 100.0000%(16/16)
Batch[57] - loss: 0.046264  acc: 100.0000%(16/16)
Batch[58] - loss: 0.006173  acc: 100.0000%(16/16)
Batch[59] - loss: 0.034007  acc: 100.0000%(16/16)
Batch[60] - loss: 0.006460  acc: 100.0000%(16/16)
Batch[61] - loss: 0.010125  acc: 100.0000%(16/16)
Batch[62] - loss: 0.017205  acc: 100.0000%(13/13)
Average loss:0.011774 average acc:99.000000%
Val set acc: 0.785234899328859
Best val set acc: 0.7919463087248322

Epoch  14 / 18
Batch[0] - loss: 0.005643  acc: 100.0000%(16/16)
Batch[1] - loss: 0.007100  acc: 100.0000%(16/16)
Batch[2] - loss: 0.011567  acc: 100.0000%(16/16)
Batch[3] - loss: 0.004128  acc: 100.0000%(16/16)
Batch[4] - loss: 0.005160  acc: 100.0000%(16/16)
Batch[5] - loss: 0.015439  acc: 100.0000%(16/16)
Batch[6] - loss: 0.015561  acc: 100.0000%(16/16)
Batch[7] - loss: 0.006087  acc: 100.0000%(16/16)
Batch[8] - loss: 0.011436  acc: 100.0000%(16/16)
Batch[9] - loss: 0.005462  acc: 100.0000%(16/16)
Batch[10] - loss: 0.002895  acc: 100.0000%(16/16)
Batch[11] - loss: 0.005089  acc: 100.0000%(16/16)
Batch[12] - loss: 0.003286  acc: 100.0000%(16/16)
Batch[13] - loss: 0.012451  acc: 100.0000%(16/16)
Batch[14] - loss: 0.009556  acc: 100.0000%(16/16)
Batch[15] - loss: 0.014649  acc: 100.0000%(16/16)
Batch[16] - loss: 0.003699  acc: 100.0000%(16/16)
Batch[17] - loss: 0.011495  acc: 100.0000%(16/16)
Batch[18] - loss: 0.008799  acc: 100.0000%(16/16)
Batch[19] - loss: 0.002760  acc: 100.0000%(16/16)
Batch[20] - loss: 0.002801  acc: 100.0000%(16/16)
Batch[21] - loss: 0.006940  acc: 100.0000%(16/16)
Batch[22] - loss: 0.007237  acc: 100.0000%(16/16)
Batch[23] - loss: 0.011089  acc: 100.0000%(16/16)
Batch[24] - loss: 0.011586  acc: 100.0000%(16/16)
Batch[25] - loss: 0.002526  acc: 100.0000%(16/16)
Batch[26] - loss: 0.009543  acc: 100.0000%(16/16)
Batch[27] - loss: 0.008742  acc: 100.0000%(16/16)
Batch[28] - loss: 0.004558  acc: 100.0000%(16/16)
Batch[29] - loss: 0.002920  acc: 100.0000%(16/16)
Batch[30] - loss: 0.007398  acc: 100.0000%(16/16)
Batch[31] - loss: 0.008378  acc: 100.0000%(16/16)
Batch[32] - loss: 0.007434  acc: 100.0000%(16/16)
Batch[33] - loss: 0.005863  acc: 100.0000%(16/16)
Batch[34] - loss: 0.003580  acc: 100.0000%(16/16)
Batch[35] - loss: 0.004363  acc: 100.0000%(16/16)
Batch[36] - loss: 0.008045  acc: 100.0000%(16/16)
Batch[37] - loss: 0.008399  acc: 100.0000%(16/16)
Batch[38] - loss: 0.004995  acc: 100.0000%(16/16)
Batch[39] - loss: 0.008608  acc: 100.0000%(16/16)
Batch[40] - loss: 0.021825  acc: 100.0000%(16/16)
Batch[41] - loss: 0.028895  acc: 100.0000%(16/16)
Batch[42] - loss: 0.008086  acc: 100.0000%(16/16)
Batch[43] - loss: 0.011573  acc: 100.0000%(16/16)
Batch[44] - loss: 0.003081  acc: 100.0000%(16/16)
Batch[45] - loss: 0.007910  acc: 100.0000%(16/16)
Batch[46] - loss: 0.012726  acc: 100.0000%(16/16)
Batch[47] - loss: 0.005975  acc: 100.0000%(16/16)
Batch[48] - loss: 0.003794  acc: 100.0000%(16/16)
Batch[49] - loss: 0.023933  acc: 100.0000%(16/16)
Batch[50] - loss: 0.008210  acc: 100.0000%(16/16)
Batch[51] - loss: 0.003258  acc: 100.0000%(16/16)
Batch[52] - loss: 0.002418  acc: 100.0000%(16/16)
Batch[53] - loss: 0.006626  acc: 100.0000%(16/16)
Batch[54] - loss: 0.017537  acc: 100.0000%(16/16)
Batch[55] - loss: 0.006615  acc: 100.0000%(16/16)
Batch[56] - loss: 0.003348  acc: 100.0000%(16/16)
Batch[57] - loss: 0.008938  acc: 100.0000%(16/16)
Batch[58] - loss: 0.009855  acc: 100.0000%(16/16)
Batch[59] - loss: 0.003659  acc: 100.0000%(16/16)
Batch[60] - loss: 0.010121  acc: 100.0000%(16/16)
Batch[61] - loss: 0.006254  acc: 100.0000%(16/16)
Batch[62] - loss: 0.008839  acc: 100.0000%(13/13)
Average loss:0.008266 average acc:100.000000%
Val set acc: 0.7986577181208053
Best val set acc: 0.7919463087248322
              precision    recall  f1-score   support

          NR    0.72500   0.76316   0.74359        38
          FR    0.72500   0.78378   0.75325        37
          TR    0.82857   0.78378   0.80556        37
          UR    0.94118   0.86486   0.90141        37

    accuracy                        0.79866       149
   macro avg    0.80494   0.79890   0.80095       149
weighted avg    0.80440   0.79866   0.80057       149

save model!!!

Epoch  15 / 18
Batch[0] - loss: 0.022854  acc: 100.0000%(16/16)
Batch[1] - loss: 0.004080  acc: 100.0000%(16/16)
Batch[2] - loss: 0.004248  acc: 100.0000%(16/16)
Batch[3] - loss: 0.006450  acc: 100.0000%(16/16)
Batch[4] - loss: 0.003923  acc: 100.0000%(16/16)
Batch[5] - loss: 0.019671  acc: 100.0000%(16/16)
Batch[6] - loss: 0.008011  acc: 100.0000%(16/16)
Batch[7] - loss: 0.005099  acc: 100.0000%(16/16)
Batch[8] - loss: 0.002238  acc: 100.0000%(16/16)
Batch[9] - loss: 0.004762  acc: 100.0000%(16/16)
Batch[10] - loss: 0.002904  acc: 100.0000%(16/16)
Batch[11] - loss: 0.004076  acc: 100.0000%(16/16)
Batch[12] - loss: 0.003933  acc: 100.0000%(16/16)
Batch[13] - loss: 0.006584  acc: 100.0000%(16/16)
Batch[14] - loss: 0.002510  acc: 100.0000%(16/16)
Batch[15] - loss: 0.011540  acc: 100.0000%(16/16)
Batch[16] - loss: 0.003706  acc: 100.0000%(16/16)
Batch[17] - loss: 0.004984  acc: 100.0000%(16/16)
Batch[18] - loss: 0.005963  acc: 100.0000%(16/16)
Batch[19] - loss: 0.006253  acc: 100.0000%(16/16)
Batch[20] - loss: 0.002893  acc: 100.0000%(16/16)
Batch[21] - loss: 0.005315  acc: 100.0000%(16/16)
Batch[22] - loss: 0.012809  acc: 100.0000%(16/16)
Batch[23] - loss: 0.007260  acc: 100.0000%(16/16)
Batch[24] - loss: 0.004172  acc: 100.0000%(16/16)
Batch[25] - loss: 0.005923  acc: 100.0000%(16/16)
Batch[26] - loss: 0.007796  acc: 100.0000%(16/16)
Batch[27] - loss: 0.015647  acc: 100.0000%(16/16)
Batch[28] - loss: 0.017190  acc: 100.0000%(16/16)
Batch[29] - loss: 0.004833  acc: 100.0000%(16/16)
Batch[30] - loss: 0.007061  acc: 100.0000%(16/16)
Batch[31] - loss: 0.005114  acc: 100.0000%(16/16)
Batch[32] - loss: 0.002845  acc: 100.0000%(16/16)
Batch[33] - loss: 0.009057  acc: 100.0000%(16/16)
Batch[34] - loss: 0.006507  acc: 100.0000%(16/16)
Batch[35] - loss: 0.004157  acc: 100.0000%(16/16)
Batch[36] - loss: 0.006590  acc: 100.0000%(16/16)
Batch[37] - loss: 0.005019  acc: 100.0000%(16/16)
Batch[38] - loss: 0.003918  acc: 100.0000%(16/16)
Batch[39] - loss: 0.006671  acc: 100.0000%(16/16)
Batch[40] - loss: 0.008329  acc: 100.0000%(16/16)
Batch[41] - loss: 0.002783  acc: 100.0000%(16/16)
Batch[42] - loss: 0.019333  acc: 100.0000%(16/16)
Batch[43] - loss: 0.002296  acc: 100.0000%(16/16)
Batch[44] - loss: 0.001974  acc: 100.0000%(16/16)
Batch[45] - loss: 0.003289  acc: 100.0000%(16/16)
Batch[46] - loss: 0.006149  acc: 100.0000%(16/16)
Batch[47] - loss: 0.006967  acc: 100.0000%(16/16)
Batch[48] - loss: 0.023389  acc: 100.0000%(16/16)
Batch[49] - loss: 0.000873  acc: 100.0000%(16/16)
Batch[50] - loss: 0.002847  acc: 100.0000%(16/16)
Batch[51] - loss: 0.004687  acc: 100.0000%(16/16)
Batch[52] - loss: 0.007844  acc: 100.0000%(16/16)
Batch[53] - loss: 0.004207  acc: 100.0000%(16/16)
Batch[54] - loss: 0.008813  acc: 100.0000%(16/16)
Batch[55] - loss: 0.005383  acc: 100.0000%(16/16)
Batch[56] - loss: 0.003512  acc: 100.0000%(16/16)
Batch[57] - loss: 0.004219  acc: 100.0000%(16/16)
Batch[58] - loss: 0.006521  acc: 100.0000%(16/16)
Batch[59] - loss: 0.014629  acc: 100.0000%(16/16)
Batch[60] - loss: 0.002033  acc: 100.0000%(16/16)
Batch[61] - loss: 0.004900  acc: 100.0000%(16/16)
Batch[62] - loss: 0.003645  acc: 100.0000%(13/13)
Average loss:0.006717 average acc:100.000000%
Val set acc: 0.7919463087248322
Best val set acc: 0.7986577181208053

Epoch  16 / 18
Batch[0] - loss: 0.005164  acc: 100.0000%(16/16)
Batch[1] - loss: 0.014817  acc: 100.0000%(16/16)
Batch[2] - loss: 0.009753  acc: 100.0000%(16/16)
Batch[3] - loss: 0.007373  acc: 100.0000%(16/16)
Batch[4] - loss: 0.014965  acc: 100.0000%(16/16)
Batch[5] - loss: 0.007911  acc: 100.0000%(16/16)
Batch[6] - loss: 0.006617  acc: 100.0000%(16/16)
Batch[7] - loss: 0.002013  acc: 100.0000%(16/16)
Batch[8] - loss: 0.014216  acc: 100.0000%(16/16)
Batch[9] - loss: 0.001298  acc: 100.0000%(16/16)
Batch[10] - loss: 0.004195  acc: 100.0000%(16/16)
Batch[11] - loss: 0.011818  acc: 100.0000%(16/16)
Batch[12] - loss: 0.052050  acc: 93.0000%(15/16)
Batch[13] - loss: 0.003890  acc: 100.0000%(16/16)
Batch[14] - loss: 0.003751  acc: 100.0000%(16/16)
Batch[15] - loss: 0.003109  acc: 100.0000%(16/16)
Batch[16] - loss: 0.005333  acc: 100.0000%(16/16)
Batch[17] - loss: 0.014085  acc: 100.0000%(16/16)
Batch[18] - loss: 0.003373  acc: 100.0000%(16/16)
Batch[19] - loss: 0.008878  acc: 100.0000%(16/16)
Batch[20] - loss: 0.009390  acc: 100.0000%(16/16)
Batch[21] - loss: 0.018118  acc: 100.0000%(16/16)
Batch[22] - loss: 0.022351  acc: 100.0000%(16/16)
Batch[23] - loss: 0.006595  acc: 100.0000%(16/16)
Batch[24] - loss: 0.001230  acc: 100.0000%(16/16)
Batch[25] - loss: 0.003812  acc: 100.0000%(16/16)
Batch[26] - loss: 0.004745  acc: 100.0000%(16/16)
Batch[27] - loss: 0.009691  acc: 100.0000%(16/16)
Batch[28] - loss: 0.002648  acc: 100.0000%(16/16)
Batch[29] - loss: 0.008968  acc: 100.0000%(16/16)
Batch[30] - loss: 0.007765  acc: 100.0000%(16/16)
Batch[31] - loss: 0.003839  acc: 100.0000%(16/16)
Batch[32] - loss: 0.003120  acc: 100.0000%(16/16)
Batch[33] - loss: 0.006305  acc: 100.0000%(16/16)
Batch[34] - loss: 0.002137  acc: 100.0000%(16/16)
Batch[35] - loss: 0.005890  acc: 100.0000%(16/16)
Batch[36] - loss: 0.014957  acc: 100.0000%(16/16)
Batch[37] - loss: 0.003222  acc: 100.0000%(16/16)
Batch[38] - loss: 0.005079  acc: 100.0000%(16/16)
Batch[39] - loss: 0.006224  acc: 100.0000%(16/16)
Batch[40] - loss: 0.009630  acc: 100.0000%(16/16)
Batch[41] - loss: 0.004852  acc: 100.0000%(16/16)
Batch[42] - loss: 0.004710  acc: 100.0000%(16/16)
Batch[43] - loss: 0.004687  acc: 100.0000%(16/16)
Batch[44] - loss: 0.001488  acc: 100.0000%(16/16)
Batch[45] - loss: 0.004367  acc: 100.0000%(16/16)
Batch[46] - loss: 0.003527  acc: 100.0000%(16/16)
Batch[47] - loss: 0.007715  acc: 100.0000%(16/16)
Batch[48] - loss: 0.002034  acc: 100.0000%(16/16)
Batch[49] - loss: 0.003774  acc: 100.0000%(16/16)
Batch[50] - loss: 0.009797  acc: 100.0000%(16/16)
Batch[51] - loss: 0.004760  acc: 100.0000%(16/16)
Batch[52] - loss: 0.013446  acc: 100.0000%(16/16)
Batch[53] - loss: 0.003902  acc: 100.0000%(16/16)
Batch[54] - loss: 0.007796  acc: 100.0000%(16/16)
Batch[55] - loss: 0.004766  acc: 100.0000%(16/16)
Batch[56] - loss: 0.005468  acc: 100.0000%(16/16)
Batch[57] - loss: 0.001300  acc: 100.0000%(16/16)
Batch[58] - loss: 0.011001  acc: 100.0000%(16/16)
Batch[59] - loss: 0.006044  acc: 100.0000%(16/16)
Batch[60] - loss: 0.003329  acc: 100.0000%(16/16)
Batch[61] - loss: 0.004339  acc: 100.0000%(16/16)
Batch[62] - loss: 0.008252  acc: 100.0000%(13/13)
Average loss:0.007487 average acc:99.000000%
Val set acc: 0.7919463087248322
Best val set acc: 0.7986577181208053

Epoch  17 / 18
Batch[0] - loss: 0.004003  acc: 100.0000%(16/16)
Batch[1] - loss: 0.002575  acc: 100.0000%(16/16)
Batch[2] - loss: 0.002054  acc: 100.0000%(16/16)
Batch[3] - loss: 0.001721  acc: 100.0000%(16/16)
Batch[4] - loss: 0.002711  acc: 100.0000%(16/16)
Batch[5] - loss: 0.006945  acc: 100.0000%(16/16)
Batch[6] - loss: 0.002039  acc: 100.0000%(16/16)
Batch[7] - loss: 0.003332  acc: 100.0000%(16/16)
Batch[8] - loss: 0.004131  acc: 100.0000%(16/16)
Batch[9] - loss: 0.003430  acc: 100.0000%(16/16)
Batch[10] - loss: 0.026926  acc: 100.0000%(16/16)
Batch[11] - loss: 0.004268  acc: 100.0000%(16/16)
Batch[12] - loss: 0.014114  acc: 100.0000%(16/16)
Batch[13] - loss: 0.003171  acc: 100.0000%(16/16)
Batch[14] - loss: 0.004106  acc: 100.0000%(16/16)
Batch[15] - loss: 0.026612  acc: 100.0000%(16/16)
Batch[16] - loss: 0.005989  acc: 100.0000%(16/16)
Batch[17] - loss: 0.008128  acc: 100.0000%(16/16)
Batch[18] - loss: 0.012564  acc: 100.0000%(16/16)
Batch[19] - loss: 0.005942  acc: 100.0000%(16/16)
Batch[20] - loss: 0.003268  acc: 100.0000%(16/16)
Batch[21] - loss: 0.007081  acc: 100.0000%(16/16)
Batch[22] - loss: 0.004416  acc: 100.0000%(16/16)
Batch[23] - loss: 0.000947  acc: 100.0000%(16/16)
Batch[24] - loss: 0.004552  acc: 100.0000%(16/16)
Batch[25] - loss: 0.004329  acc: 100.0000%(16/16)
Batch[26] - loss: 0.010325  acc: 100.0000%(16/16)
Batch[27] - loss: 0.003098  acc: 100.0000%(16/16)
Batch[28] - loss: 0.004663  acc: 100.0000%(16/16)
Batch[29] - loss: 0.007229  acc: 100.0000%(16/16)
Batch[30] - loss: 0.007583  acc: 100.0000%(16/16)
Batch[31] - loss: 0.005416  acc: 100.0000%(16/16)
Batch[32] - loss: 0.003012  acc: 100.0000%(16/16)
Batch[33] - loss: 0.012494  acc: 100.0000%(16/16)
Batch[34] - loss: 0.005928  acc: 100.0000%(16/16)
Batch[35] - loss: 0.001414  acc: 100.0000%(16/16)
Batch[36] - loss: 0.004591  acc: 100.0000%(16/16)
Batch[37] - loss: 0.004887  acc: 100.0000%(16/16)
Batch[38] - loss: 0.002769  acc: 100.0000%(16/16)
Batch[39] - loss: 0.002589  acc: 100.0000%(16/16)
Batch[40] - loss: 0.003969  acc: 100.0000%(16/16)
Batch[41] - loss: 0.006085  acc: 100.0000%(16/16)
Batch[42] - loss: 0.004208  acc: 100.0000%(16/16)
Batch[43] - loss: 0.004123  acc: 100.0000%(16/16)
Batch[44] - loss: 0.002775  acc: 100.0000%(16/16)
Batch[45] - loss: 0.008058  acc: 100.0000%(16/16)
Batch[46] - loss: 0.009246  acc: 100.0000%(16/16)
Batch[47] - loss: 0.001755  acc: 100.0000%(16/16)
Batch[48] - loss: 0.009561  acc: 100.0000%(16/16)
Batch[49] - loss: 0.004730  acc: 100.0000%(16/16)
Batch[50] - loss: 0.002806  acc: 100.0000%(16/16)
Batch[51] - loss: 0.003251  acc: 100.0000%(16/16)
Batch[52] - loss: 0.002490  acc: 100.0000%(16/16)
Batch[53] - loss: 0.011283  acc: 100.0000%(16/16)
Batch[54] - loss: 0.005118  acc: 100.0000%(16/16)
Batch[55] - loss: 0.010828  acc: 100.0000%(16/16)
Batch[56] - loss: 0.014331  acc: 100.0000%(16/16)
Batch[57] - loss: 0.004125  acc: 100.0000%(16/16)
Batch[58] - loss: 0.003333  acc: 100.0000%(16/16)
Batch[59] - loss: 0.001627  acc: 100.0000%(16/16)
Batch[60] - loss: 0.003561  acc: 100.0000%(16/16)
Batch[61] - loss: 0.003453  acc: 100.0000%(16/16)
Batch[62] - loss: 0.005407  acc: 100.0000%(13/13)
Average loss:0.005897 average acc:100.000000%
Val set acc: 0.785234899328859
Best val set acc: 0.7986577181208053

Epoch  18 / 18
Batch[0] - loss: 0.005855  acc: 100.0000%(16/16)
Batch[1] - loss: 0.009624  acc: 100.0000%(16/16)
Batch[2] - loss: 0.004470  acc: 100.0000%(16/16)
Batch[3] - loss: 0.001737  acc: 100.0000%(16/16)
Batch[4] - loss: 0.005365  acc: 100.0000%(16/16)
Batch[5] - loss: 0.002962  acc: 100.0000%(16/16)
Batch[6] - loss: 0.002278  acc: 100.0000%(16/16)
Batch[7] - loss: 0.061190  acc: 100.0000%(16/16)
Batch[8] - loss: 0.002895  acc: 100.0000%(16/16)
Batch[9] - loss: 0.003269  acc: 100.0000%(16/16)
Batch[10] - loss: 0.002691  acc: 100.0000%(16/16)
Batch[11] - loss: 0.002578  acc: 100.0000%(16/16)
Batch[12] - loss: 0.003108  acc: 100.0000%(16/16)
Batch[13] - loss: 0.005917  acc: 100.0000%(16/16)
Batch[14] - loss: 0.034990  acc: 100.0000%(16/16)
Batch[15] - loss: 0.006029  acc: 100.0000%(16/16)
Batch[16] - loss: 0.005771  acc: 100.0000%(16/16)
Batch[17] - loss: 0.007768  acc: 100.0000%(16/16)
Batch[18] - loss: 0.002172  acc: 100.0000%(16/16)
Batch[19] - loss: 0.002048  acc: 100.0000%(16/16)
Batch[20] - loss: 0.003378  acc: 100.0000%(16/16)
Batch[21] - loss: 0.003948  acc: 100.0000%(16/16)
Batch[22] - loss: 0.003759  acc: 100.0000%(16/16)
Batch[23] - loss: 0.002868  acc: 100.0000%(16/16)
Batch[24] - loss: 0.004670  acc: 100.0000%(16/16)
Batch[25] - loss: 0.001600  acc: 100.0000%(16/16)
Batch[26] - loss: 0.003343  acc: 100.0000%(16/16)
Batch[27] - loss: 0.005600  acc: 100.0000%(16/16)
Batch[28] - loss: 0.005077  acc: 100.0000%(16/16)
Batch[29] - loss: 0.012204  acc: 100.0000%(16/16)
Batch[30] - loss: 0.004234  acc: 100.0000%(16/16)
Batch[31] - loss: 0.006638  acc: 100.0000%(16/16)
Batch[32] - loss: 0.002283  acc: 100.0000%(16/16)
Batch[33] - loss: 0.006829  acc: 100.0000%(16/16)
Batch[34] - loss: 0.003705  acc: 100.0000%(16/16)
Batch[35] - loss: 0.005993  acc: 100.0000%(16/16)
Batch[36] - loss: 0.002108  acc: 100.0000%(16/16)
Batch[37] - loss: 0.003296  acc: 100.0000%(16/16)
Batch[38] - loss: 0.006330  acc: 100.0000%(16/16)
Batch[39] - loss: 0.005452  acc: 100.0000%(16/16)
Batch[40] - loss: 0.003197  acc: 100.0000%(16/16)
Batch[41] - loss: 0.007198  acc: 100.0000%(16/16)
Batch[42] - loss: 0.006100  acc: 100.0000%(16/16)
Batch[43] - loss: 0.010384  acc: 100.0000%(16/16)
Batch[44] - loss: 0.003497  acc: 100.0000%(16/16)
Batch[45] - loss: 0.005008  acc: 100.0000%(16/16)
Batch[46] - loss: 0.001251  acc: 100.0000%(16/16)
Batch[47] - loss: 0.007944  acc: 100.0000%(16/16)
Batch[48] - loss: 0.003687  acc: 100.0000%(16/16)
Batch[49] - loss: 0.004282  acc: 100.0000%(16/16)
Batch[50] - loss: 0.005729  acc: 100.0000%(16/16)
Batch[51] - loss: 0.003620  acc: 100.0000%(16/16)
Batch[52] - loss: 0.002373  acc: 100.0000%(16/16)
Batch[53] - loss: 0.002570  acc: 100.0000%(16/16)
Batch[54] - loss: 0.003866  acc: 100.0000%(16/16)
Batch[55] - loss: 0.009092  acc: 100.0000%(16/16)
Batch[56] - loss: 0.002666  acc: 100.0000%(16/16)
Batch[57] - loss: 0.002649  acc: 100.0000%(16/16)
Batch[58] - loss: 0.001691  acc: 100.0000%(16/16)
Batch[59] - loss: 0.010553  acc: 100.0000%(16/16)
Batch[60] - loss: 0.007301  acc: 100.0000%(16/16)
Batch[61] - loss: 0.008743  acc: 100.0000%(16/16)
Batch[62] - loss: 0.004892  acc: 100.0000%(13/13)
Average loss:0.006100 average acc:100.000000%
Reload the best model...
0.0005
Val set acc: 0.7986577181208053
Best val set acc: 0.7986577181208053
================================
              precision    recall  f1-score   support

          NR      0.867     0.929     0.897        84
          FR      0.898     0.940     0.919        84
          TR      0.921     0.833     0.875        84
          UR      0.927     0.905     0.916        84

    accuracy                          0.902       336
   macro avg      0.903     0.902     0.901       336
weighted avg      0.903     0.902     0.901       336

              total        used        free      shared  buff/cache   available
Mem:          92486        1091       89868           1        1526       90768
Swap:             0           0           0
Tue Oct 27 07:02:21 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:07.0 Off |                    0 |
| N/A   31C    P0    37W / 300W |      0MiB / 32480MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
task:  twitter15
{'lr': 0.001, 'reg': 1e-06, 'embeding_size': 100, 'batch_size': 16, 'nb_filters': 100, 'kernel_sizes': [3, 4, 5], 'dropout': 0.5, 'epochs': 18, 'num_classes': 4, 'target_names': ['NR', 'FR', 'TR', 'UR'], 'save_path': 'checkpoint/weights.best.twitter15.pgan', 'maxlen': 50, 'n_heads': 10}
PGAN(
  (word_embedding): Embedding(2246, 300, padding_idx=0)
  (user_embedding): Embedding(2213, 100, padding_idx=0)
  (source_embedding): Embedding(1490, 100)
  (convs): ModuleList(
    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))
    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))
    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))
  )
  (max_poolings): ModuleList(
    (0): MaxPool1d(kernel_size=48, stride=48, padding=0, dilation=1, ceil_mode=False)
    (1): MaxPool1d(kernel_size=47, stride=47, padding=0, dilation=1, ceil_mode=False)
    (2): MaxPool1d(kernel_size=46, stride=46, padding=0, dilation=1, ceil_mode=False)
  )
  (linear): Linear(in_features=400, out_features=200, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
  (relu): ReLU()
  (elu): ELU(alpha=1.0)
  (fc_out): Sequential(
    (0): Linear(in_features=500, out_features=100, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=100, out_features=4, bias=True)
  )
  (fc_user_out): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=100, out_features=3, bias=True)
  )
  (fc_ruser_out): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=100, out_features=3, bias=True)
  )
)

Epoch  1 / 18
Batch[0] - loss: 3.598952  acc: 25.0000%(4/16)
Batch[1] - loss: 3.666010  acc: 18.0000%(3/16)
Batch[2] - loss: 3.561962  acc: 25.0000%(4/16)
Batch[3] - loss: 3.568866  acc: 31.0000%(5/16)
Batch[4] - loss: 3.586249  acc: 43.0000%(7/16)
Batch[5] - loss: 3.539698  acc: 12.0000%(2/16)
Batch[6] - loss: 3.476076  acc: 18.0000%(3/16)
Batch[7] - loss: 3.615313  acc: 25.0000%(4/16)
Batch[8] - loss: 3.425699  acc: 18.0000%(3/16)
Batch[9] - loss: 3.560316  acc: 31.0000%(5/16)
Batch[10] - loss: 3.454654  acc: 37.0000%(6/16)
Batch[11] - loss: 3.616429  acc: 18.0000%(3/16)
Batch[12] - loss: 3.450480  acc: 37.0000%(6/16)
Batch[13] - loss: 3.860938  acc: 12.0000%(2/16)
Batch[14] - loss: 3.276819  acc: 12.0000%(2/16)
Batch[15] - loss: 3.362071  acc: 31.0000%(5/16)
Batch[16] - loss: 3.324218  acc: 56.0000%(9/16)
Batch[17] - loss: 3.471787  acc: 25.0000%(4/16)
Batch[18] - loss: 3.391870  acc: 25.0000%(4/16)
Batch[19] - loss: 3.229395  acc: 18.0000%(3/16)
Batch[20] - loss: 3.209219  acc: 50.0000%(8/16)
Batch[21] - loss: 3.318237  acc: 12.0000%(2/16)
Batch[22] - loss: 3.363994  acc: 25.0000%(4/16)
Batch[23] - loss: 3.210560  acc: 18.0000%(3/16)
Batch[24] - loss: 3.012885  acc: 25.0000%(4/16)
Batch[25] - loss: 3.020851  acc: 37.0000%(6/16)
Batch[26] - loss: 3.310473  acc: 37.0000%(6/16)
Batch[27] - loss: 3.526949  acc: 25.0000%(4/16)
Batch[28] - loss: 3.266018  acc: 31.0000%(5/16)
Batch[29] - loss: 3.129774  acc: 37.0000%(6/16)
Batch[30] - loss: 3.456914  acc: 31.0000%(5/16)
Batch[31] - loss: 3.369586  acc: 12.0000%(2/16)
Batch[32] - loss: 2.802999  acc: 18.0000%(3/16)
Batch[33] - loss: 3.020998  acc: 37.0000%(6/16)
Batch[34] - loss: 3.157057  acc: 43.0000%(7/16)
Batch[35] - loss: 3.515024  acc: 12.0000%(2/16)
Batch[36] - loss: 3.022015  acc: 37.0000%(6/16)
Batch[37] - loss: 3.226677  acc: 18.0000%(3/16)
Batch[38] - loss: 2.991786  acc: 43.0000%(7/16)
Batch[39] - loss: 3.999331  acc: 25.0000%(4/16)
Batch[40] - loss: 2.783630  acc: 43.0000%(7/16)
Batch[41] - loss: 3.010602  acc: 31.0000%(5/16)
Batch[42] - loss: 2.966434  acc: 25.0000%(4/16)
Batch[43] - loss: 3.028388  acc: 31.0000%(5/16)
Batch[44] - loss: 3.369672  acc: 31.0000%(5/16)
Batch[45] - loss: 3.176540  acc: 31.0000%(5/16)
Batch[46] - loss: 3.210073  acc: 37.0000%(6/16)
Batch[47] - loss: 3.296275  acc: 43.0000%(7/16)
Batch[48] - loss: 3.232572  acc: 18.0000%(3/16)
Batch[49] - loss: 3.166708  acc: 43.0000%(7/16)
Batch[50] - loss: 3.106528  acc: 31.0000%(5/16)
Batch[51] - loss: 2.808836  acc: 50.0000%(8/16)
Batch[52] - loss: 3.486006  acc: 37.0000%(6/16)
Batch[53] - loss: 3.459694  acc: 25.0000%(4/16)
Batch[54] - loss: 3.859522  acc: 25.0000%(4/16)
Batch[55] - loss: 3.549587  acc: 18.0000%(3/16)
Batch[56] - loss: 3.615237  acc: 31.0000%(5/16)
Batch[57] - loss: 3.164203  acc: 37.0000%(6/16)
Batch[58] - loss: 3.128538  acc: 43.0000%(7/16)
Batch[59] - loss: 3.663065  acc: 25.0000%(4/16)
Batch[60] - loss: 3.137412  acc: 37.0000%(6/16)
Batch[61] - loss: 2.992869  acc: 62.0000%(10/16)
Batch[62] - loss: 3.115980  acc: 38.0000%(5/13)
Average loss:3.322183 average acc:29.000000%
Val set acc: 0.4899328859060403
Best val set acc: 0

Epoch  2 / 18
Batch[0] - loss: 2.806869  acc: 62.0000%(10/16)
Batch[1] - loss: 2.963289  acc: 50.0000%(8/16)
Batch[2] - loss: 3.648176  acc: 56.0000%(9/16)
Batch[3] - loss: 2.965605  acc: 62.0000%(10/16)
Batch[4] - loss: 2.871807  acc: 43.0000%(7/16)
Batch[5] - loss: 3.015860  acc: 31.0000%(5/16)
Batch[6] - loss: 3.430668  acc: 37.0000%(6/16)
Batch[7] - loss: 3.233132  acc: 68.0000%(11/16)
Batch[8] - loss: 3.077672  acc: 37.0000%(6/16)
Batch[9] - loss: 3.139430  acc: 68.0000%(11/16)
Batch[10] - loss: 2.886522  acc: 56.0000%(9/16)
Batch[11] - loss: 2.873562  acc: 50.0000%(8/16)
Batch[12] - loss: 3.033412  acc: 25.0000%(4/16)
Batch[13] - loss: 2.749943  acc: 31.0000%(5/16)
Batch[14] - loss: 2.967407  acc: 50.0000%(8/16)
Batch[15] - loss: 3.554486  acc: 50.0000%(8/16)
Batch[16] - loss: 2.862295  acc: 50.0000%(8/16)
Batch[17] - loss: 2.586118  acc: 68.0000%(11/16)
Batch[18] - loss: 2.799355  acc: 43.0000%(7/16)
Batch[19] - loss: 2.855549  acc: 43.0000%(7/16)
Batch[20] - loss: 2.938381  acc: 50.0000%(8/16)
Batch[21] - loss: 3.060991  acc: 43.0000%(7/16)
Batch[22] - loss: 2.932846  acc: 50.0000%(8/16)
Batch[23] - loss: 3.137972  acc: 43.0000%(7/16)
Batch[24] - loss: 2.851245  acc: 62.0000%(10/16)
Batch[25] - loss: 3.362011  acc: 31.0000%(5/16)
Batch[26] - loss: 2.897462  acc: 37.0000%(6/16)
Batch[27] - loss: 2.929836  acc: 25.0000%(4/16)
Batch[28] - loss: 2.621291  acc: 37.0000%(6/16)
Batch[29] - loss: 2.380526  acc: 50.0000%(8/16)
Batch[30] - loss: 3.168774  acc: 25.0000%(4/16)
Batch[31] - loss: 2.940467  acc: 56.0000%(9/16)
Batch[32] - loss: 2.691926  acc: 62.0000%(10/16)
Batch[33] - loss: 2.953815  acc: 43.0000%(7/16)
Batch[34] - loss: 2.478623  acc: 56.0000%(9/16)
Batch[35] - loss: 2.820257  acc: 31.0000%(5/16)
Batch[36] - loss: 2.520307  acc: 62.0000%(10/16)
Batch[37] - loss: 2.651132  acc: 62.0000%(10/16)
Batch[38] - loss: 3.040915  acc: 18.0000%(3/16)
Batch[39] - loss: 2.642834  acc: 43.0000%(7/16)
Batch[40] - loss: 2.761404  acc: 56.0000%(9/16)
Batch[41] - loss: 2.483121  acc: 56.0000%(9/16)
Batch[42] - loss: 2.619020  acc: 56.0000%(9/16)
Batch[43] - loss: 2.995579  acc: 56.0000%(9/16)
Batch[44] - loss: 2.638435  acc: 50.0000%(8/16)
Batch[45] - loss: 2.746197  acc: 37.0000%(6/16)
Batch[46] - loss: 2.521685  acc: 68.0000%(11/16)
Batch[47] - loss: 2.534655  acc: 62.0000%(10/16)
Batch[48] - loss: 2.474056  acc: 56.0000%(9/16)
Batch[49] - loss: 2.290527  acc: 56.0000%(9/16)
Batch[50] - loss: 2.343952  acc: 56.0000%(9/16)
Batch[51] - loss: 2.517353  acc: 50.0000%(8/16)
Batch[52] - loss: 2.387545  acc: 56.0000%(9/16)
Batch[53] - loss: 2.836084  acc: 62.0000%(10/16)
Batch[54] - loss: 2.380823  acc: 75.0000%(12/16)
Batch[55] - loss: 2.478201  acc: 43.0000%(7/16)
Batch[56] - loss: 2.716295  acc: 50.0000%(8/16)
Batch[57] - loss: 2.438669  acc: 81.0000%(13/16)
Batch[58] - loss: 2.180409  acc: 50.0000%(8/16)
Batch[59] - loss: 2.532569  acc: 81.0000%(13/16)
Batch[60] - loss: 2.956792  acc: 43.0000%(7/16)
Batch[61] - loss: 2.395563  acc: 56.0000%(9/16)
Batch[62] - loss: 2.641048  acc: 53.0000%(7/13)
Average loss:2.797028 average acc:50.000000%
Val set acc: 0.6308724832214765
Best val set acc: 0

Epoch  3 / 18
Batch[0] - loss: 1.980993  acc: 81.0000%(13/16)
Batch[1] - loss: 2.001003  acc: 75.0000%(12/16)
Batch[2] - loss: 2.256093  acc: 56.0000%(9/16)
Batch[3] - loss: 1.839305  acc: 81.0000%(13/16)
Batch[4] - loss: 2.200382  acc: 81.0000%(13/16)
Batch[5] - loss: 2.270733  acc: 56.0000%(9/16)
Batch[6] - loss: 2.064367  acc: 81.0000%(13/16)
Batch[7] - loss: 2.022921  acc: 87.0000%(14/16)
Batch[8] - loss: 2.019557  acc: 81.0000%(13/16)
Batch[9] - loss: 2.026803  acc: 75.0000%(12/16)
Batch[10] - loss: 1.668408  acc: 81.0000%(13/16)
Batch[11] - loss: 1.797582  acc: 87.0000%(14/16)
Batch[12] - loss: 1.905392  acc: 75.0000%(12/16)
Batch[13] - loss: 1.942493  acc: 68.0000%(11/16)
Batch[14] - loss: 1.881796  acc: 75.0000%(12/16)
Batch[15] - loss: 2.226568  acc: 75.0000%(12/16)
Batch[16] - loss: 1.990632  acc: 75.0000%(12/16)
Batch[17] - loss: 1.326001  acc: 81.0000%(13/16)
Batch[18] - loss: 1.343831  acc: 68.0000%(11/16)
Batch[19] - loss: 2.068247  acc: 62.0000%(10/16)
Batch[20] - loss: 1.706904  acc: 75.0000%(12/16)
Batch[21] - loss: 1.764975  acc: 62.0000%(10/16)
Batch[22] - loss: 1.949209  acc: 56.0000%(9/16)
Batch[23] - loss: 1.473116  acc: 50.0000%(8/16)
Batch[24] - loss: 2.030798  acc: 62.0000%(10/16)
Batch[25] - loss: 1.739118  acc: 75.0000%(12/16)
Batch[26] - loss: 1.587287  acc: 93.0000%(15/16)
Batch[27] - loss: 1.856186  acc: 56.0000%(9/16)
Batch[28] - loss: 1.876934  acc: 81.0000%(13/16)
Batch[29] - loss: 1.624175  acc: 81.0000%(13/16)
Batch[30] - loss: 1.921676  acc: 75.0000%(12/16)
Batch[31] - loss: 1.347628  acc: 87.0000%(14/16)
Batch[32] - loss: 1.655352  acc: 93.0000%(15/16)
Batch[33] - loss: 1.613407  acc: 93.0000%(15/16)
Batch[34] - loss: 1.742092  acc: 81.0000%(13/16)
Batch[35] - loss: 1.475795  acc: 87.0000%(14/16)
Batch[36] - loss: 1.757168  acc: 75.0000%(12/16)
Batch[37] - loss: 1.163451  acc: 100.0000%(16/16)
Batch[38] - loss: 1.746127  acc: 62.0000%(10/16)
Batch[39] - loss: 1.454172  acc: 68.0000%(11/16)
Batch[40] - loss: 1.563528  acc: 87.0000%(14/16)
Batch[41] - loss: 1.542065  acc: 68.0000%(11/16)
Batch[42] - loss: 1.815739  acc: 81.0000%(13/16)
Batch[43] - loss: 1.646621  acc: 81.0000%(13/16)
Batch[44] - loss: 1.481073  acc: 87.0000%(14/16)
Batch[45] - loss: 1.335850  acc: 87.0000%(14/16)
Batch[46] - loss: 1.228052  acc: 87.0000%(14/16)
Batch[47] - loss: 1.481983  acc: 81.0000%(13/16)
Batch[48] - loss: 1.499930  acc: 81.0000%(13/16)
Batch[49] - loss: 1.520137  acc: 87.0000%(14/16)
Batch[50] - loss: 1.754864  acc: 87.0000%(14/16)
Batch[51] - loss: 1.352381  acc: 81.0000%(13/16)
Batch[52] - loss: 1.519662  acc: 75.0000%(12/16)
Batch[53] - loss: 0.916504  acc: 87.0000%(14/16)
Batch[54] - loss: 1.384544  acc: 62.0000%(10/16)
Batch[55] - loss: 1.203497  acc: 87.0000%(14/16)
Batch[56] - loss: 1.544759  acc: 68.0000%(11/16)
Batch[57] - loss: 1.634105  acc: 75.0000%(12/16)
Batch[58] - loss: 1.078983  acc: 93.0000%(15/16)
Batch[59] - loss: 1.959905  acc: 68.0000%(11/16)
Batch[60] - loss: 1.531620  acc: 87.0000%(14/16)
Batch[61] - loss: 1.237574  acc: 81.0000%(13/16)
Batch[62] - loss: 1.210111  acc: 92.0000%(12/13)
Average loss:1.678765 average acc:77.000000%
Val set acc: 0.7449664429530202
Best val set acc: 0

Epoch  4 / 18
Batch[0] - loss: 1.285735  acc: 93.0000%(15/16)
Batch[1] - loss: 0.902624  acc: 87.0000%(14/16)
Batch[2] - loss: 0.827853  acc: 100.0000%(16/16)
Batch[3] - loss: 1.009962  acc: 81.0000%(13/16)
Batch[4] - loss: 0.864575  acc: 93.0000%(15/16)
Batch[5] - loss: 0.771050  acc: 93.0000%(15/16)
Batch[6] - loss: 0.983398  acc: 100.0000%(16/16)
Batch[7] - loss: 0.996453  acc: 87.0000%(14/16)
Batch[8] - loss: 1.067388  acc: 93.0000%(15/16)
Batch[9] - loss: 0.882877  acc: 93.0000%(15/16)
Batch[10] - loss: 1.064203  acc: 81.0000%(13/16)
Batch[11] - loss: 1.077607  acc: 93.0000%(15/16)
Batch[12] - loss: 0.812529  acc: 93.0000%(15/16)
Batch[13] - loss: 0.743126  acc: 93.0000%(15/16)
Batch[14] - loss: 0.822159  acc: 100.0000%(16/16)
Batch[15] - loss: 0.591853  acc: 100.0000%(16/16)
Batch[16] - loss: 0.997840  acc: 87.0000%(14/16)
Batch[17] - loss: 0.872471  acc: 81.0000%(13/16)
Batch[18] - loss: 0.726958  acc: 93.0000%(15/16)
Batch[19] - loss: 0.895991  acc: 87.0000%(14/16)
Batch[20] - loss: 0.677154  acc: 93.0000%(15/16)
Batch[21] - loss: 0.650582  acc: 87.0000%(14/16)
Batch[22] - loss: 1.363502  acc: 81.0000%(13/16)
Batch[23] - loss: 0.652153  acc: 81.0000%(13/16)
Batch[24] - loss: 0.501302  acc: 100.0000%(16/16)
Batch[25] - loss: 1.003963  acc: 93.0000%(15/16)
Batch[26] - loss: 0.660367  acc: 87.0000%(14/16)
Batch[27] - loss: 1.023884  acc: 93.0000%(15/16)
Batch[28] - loss: 0.677883  acc: 100.0000%(16/16)
Batch[29] - loss: 0.763664  acc: 87.0000%(14/16)
Batch[30] - loss: 0.604758  acc: 75.0000%(12/16)
Batch[31] - loss: 0.586607  acc: 93.0000%(15/16)
Batch[32] - loss: 0.428052  acc: 100.0000%(16/16)
Batch[33] - loss: 0.675496  acc: 93.0000%(15/16)
Batch[34] - loss: 0.541628  acc: 93.0000%(15/16)
Batch[35] - loss: 0.498597  acc: 87.0000%(14/16)
Batch[36] - loss: 1.193890  acc: 81.0000%(13/16)
Batch[37] - loss: 0.689166  acc: 93.0000%(15/16)
Batch[38] - loss: 0.543651  acc: 93.0000%(15/16)
Batch[39] - loss: 0.668634  acc: 87.0000%(14/16)
Batch[40] - loss: 0.691554  acc: 93.0000%(15/16)
Batch[41] - loss: 0.412856  acc: 100.0000%(16/16)
Batch[42] - loss: 0.988906  acc: 93.0000%(15/16)
Batch[43] - loss: 0.637593  acc: 93.0000%(15/16)
Batch[44] - loss: 0.458153  acc: 100.0000%(16/16)
Batch[45] - loss: 0.932034  acc: 75.0000%(12/16)
Batch[46] - loss: 0.965599  acc: 93.0000%(15/16)
Batch[47] - loss: 0.648036  acc: 93.0000%(15/16)
Batch[48] - loss: 0.620056  acc: 100.0000%(16/16)
Batch[49] - loss: 0.483879  acc: 100.0000%(16/16)
Batch[50] - loss: 0.525427  acc: 100.0000%(16/16)
Batch[51] - loss: 0.537029  acc: 93.0000%(15/16)
Batch[52] - loss: 0.457346  acc: 100.0000%(16/16)
Batch[53] - loss: 0.405858  acc: 93.0000%(15/16)
Batch[54] - loss: 0.534936  acc: 87.0000%(14/16)
Batch[55] - loss: 0.917480  acc: 87.0000%(14/16)
Batch[56] - loss: 0.611742  acc: 100.0000%(16/16)
Batch[57] - loss: 0.841342  acc: 100.0000%(16/16)
Batch[58] - loss: 1.048469  acc: 87.0000%(14/16)
Batch[59] - loss: 0.503338  acc: 100.0000%(16/16)
Batch[60] - loss: 0.763307  acc: 87.0000%(14/16)
Batch[61] - loss: 0.553667  acc: 93.0000%(15/16)
Batch[62] - loss: 0.740511  acc: 100.0000%(13/13)
Average loss:0.760011 average acc:91.000000%
Val set acc: 0.8120805369127517
Best val set acc: 0

Epoch  5 / 18
Batch[0] - loss: 0.217208  acc: 100.0000%(16/16)
Batch[1] - loss: 0.321252  acc: 93.0000%(15/16)
Batch[2] - loss: 0.212380  acc: 100.0000%(16/16)
Batch[3] - loss: 0.515547  acc: 93.0000%(15/16)
Batch[4] - loss: 0.334582  acc: 93.0000%(15/16)
Batch[5] - loss: 0.523326  acc: 93.0000%(15/16)
Batch[6] - loss: 0.424671  acc: 93.0000%(15/16)
Batch[7] - loss: 0.680091  acc: 100.0000%(16/16)
Batch[8] - loss: 0.424305  acc: 100.0000%(16/16)
Batch[9] - loss: 0.235685  acc: 100.0000%(16/16)
Batch[10] - loss: 0.144405  acc: 100.0000%(16/16)
Batch[11] - loss: 0.279822  acc: 100.0000%(16/16)
Batch[12] - loss: 0.359988  acc: 93.0000%(15/16)
Batch[13] - loss: 0.461820  acc: 100.0000%(16/16)
Batch[14] - loss: 0.417929  acc: 87.0000%(14/16)
Batch[15] - loss: 0.309546  acc: 100.0000%(16/16)
Batch[16] - loss: 0.449560  acc: 93.0000%(15/16)
Batch[17] - loss: 0.422863  acc: 100.0000%(16/16)
Batch[18] - loss: 0.390368  acc: 93.0000%(15/16)
Batch[19] - loss: 0.217978  acc: 93.0000%(15/16)
Batch[20] - loss: 0.416172  acc: 100.0000%(16/16)
Batch[21] - loss: 0.426197  acc: 100.0000%(16/16)
Batch[22] - loss: 0.189665  acc: 100.0000%(16/16)
Batch[23] - loss: 0.401767  acc: 100.0000%(16/16)
Batch[24] - loss: 0.332832  acc: 100.0000%(16/16)
Batch[25] - loss: 0.288599  acc: 100.0000%(16/16)
Batch[26] - loss: 0.252586  acc: 100.0000%(16/16)
Batch[27] - loss: 0.129248  acc: 100.0000%(16/16)
Batch[28] - loss: 0.423281  acc: 93.0000%(15/16)
Batch[29] - loss: 0.287486  acc: 100.0000%(16/16)
Batch[30] - loss: 0.265169  acc: 93.0000%(15/16)
Batch[31] - loss: 0.413543  acc: 100.0000%(16/16)
Batch[32] - loss: 0.314235  acc: 93.0000%(15/16)
Batch[33] - loss: 0.173040  acc: 100.0000%(16/16)
Batch[34] - loss: 0.279237  acc: 100.0000%(16/16)
Batch[35] - loss: 0.207293  acc: 100.0000%(16/16)
Batch[36] - loss: 0.164054  acc: 100.0000%(16/16)
Batch[37] - loss: 0.251782  acc: 100.0000%(16/16)
Batch[38] - loss: 0.258123  acc: 93.0000%(15/16)
Batch[39] - loss: 0.413865  acc: 87.0000%(14/16)
Batch[40] - loss: 0.409918  acc: 100.0000%(16/16)
Batch[41] - loss: 0.424092  acc: 100.0000%(16/16)
Batch[42] - loss: 0.120219  acc: 100.0000%(16/16)
Batch[43] - loss: 0.221168  acc: 100.0000%(16/16)
Batch[44] - loss: 0.314170  acc: 93.0000%(15/16)
Batch[45] - loss: 0.298218  acc: 100.0000%(16/16)
Batch[46] - loss: 0.356591  acc: 93.0000%(15/16)
Batch[47] - loss: 0.285484  acc: 100.0000%(16/16)
Batch[48] - loss: 0.332452  acc: 100.0000%(16/16)
Batch[49] - loss: 0.214840  acc: 93.0000%(15/16)
Batch[50] - loss: 0.214644  acc: 100.0000%(16/16)
Batch[51] - loss: 0.133716  acc: 100.0000%(16/16)
Batch[52] - loss: 0.305888  acc: 100.0000%(16/16)
Batch[53] - loss: 0.295987  acc: 93.0000%(15/16)
Batch[54] - loss: 0.253545  acc: 100.0000%(16/16)
Batch[55] - loss: 0.458350  acc: 100.0000%(16/16)
Batch[56] - loss: 0.287819  acc: 100.0000%(16/16)
Batch[57] - loss: 0.135809  acc: 100.0000%(16/16)
Batch[58] - loss: 0.383660  acc: 87.0000%(14/16)
Batch[59] - loss: 0.253803  acc: 100.0000%(16/16)
Batch[60] - loss: 0.298764  acc: 93.0000%(15/16)
Batch[61] - loss: 0.239003  acc: 93.0000%(15/16)
Batch[62] - loss: 0.544864  acc: 92.0000%(12/13)
Average loss:0.317691 average acc:97.000000%
Val set acc: 0.7718120805369127
Best val set acc: 0

Epoch  6 / 18
Batch[0] - loss: 0.269516  acc: 100.0000%(16/16)
Batch[1] - loss: 0.166517  acc: 100.0000%(16/16)
Batch[2] - loss: 0.161073  acc: 93.0000%(15/16)
Batch[3] - loss: 0.254042  acc: 100.0000%(16/16)
Batch[4] - loss: 0.215333  acc: 93.0000%(15/16)
Batch[5] - loss: 0.146465  acc: 100.0000%(16/16)
Batch[6] - loss: 0.084942  acc: 100.0000%(16/16)
Batch[7] - loss: 0.467546  acc: 93.0000%(15/16)
Batch[8] - loss: 0.172428  acc: 100.0000%(16/16)
Batch[9] - loss: 0.119999  acc: 100.0000%(16/16)
Batch[10] - loss: 0.080368  acc: 100.0000%(16/16)
Batch[11] - loss: 0.082256  acc: 100.0000%(16/16)
Batch[12] - loss: 0.128411  acc: 100.0000%(16/16)
Batch[13] - loss: 0.179756  acc: 100.0000%(16/16)
Batch[14] - loss: 0.176150  acc: 100.0000%(16/16)
Batch[15] - loss: 0.261685  acc: 93.0000%(15/16)
Batch[16] - loss: 0.095779  acc: 100.0000%(16/16)
Batch[17] - loss: 0.171303  acc: 100.0000%(16/16)
Batch[18] - loss: 0.079956  acc: 100.0000%(16/16)
Batch[19] - loss: 0.500416  acc: 93.0000%(15/16)
Batch[20] - loss: 0.124264  acc: 100.0000%(16/16)
Batch[21] - loss: 0.315704  acc: 93.0000%(15/16)
Batch[22] - loss: 0.119461  acc: 100.0000%(16/16)
Batch[23] - loss: 0.180792  acc: 100.0000%(16/16)
Batch[24] - loss: 0.160189  acc: 100.0000%(16/16)
Batch[25] - loss: 0.169311  acc: 100.0000%(16/16)
Batch[26] - loss: 0.076829  acc: 100.0000%(16/16)
Batch[27] - loss: 0.256903  acc: 100.0000%(16/16)
Batch[28] - loss: 0.129731  acc: 100.0000%(16/16)
Batch[29] - loss: 0.161559  acc: 100.0000%(16/16)
Batch[30] - loss: 0.260452  acc: 93.0000%(15/16)
Batch[31] - loss: 0.136199  acc: 100.0000%(16/16)
Batch[32] - loss: 0.247646  acc: 93.0000%(15/16)
Batch[33] - loss: 0.111459  acc: 100.0000%(16/16)
Batch[34] - loss: 0.108630  acc: 100.0000%(16/16)
Batch[35] - loss: 0.156862  acc: 100.0000%(16/16)
Batch[36] - loss: 0.311092  acc: 100.0000%(16/16)
Batch[37] - loss: 0.131900  acc: 100.0000%(16/16)
Batch[38] - loss: 0.130162  acc: 100.0000%(16/16)
Batch[39] - loss: 0.077141  acc: 100.0000%(16/16)
Batch[40] - loss: 0.137732  acc: 100.0000%(16/16)
Batch[41] - loss: 0.094478  acc: 100.0000%(16/16)
Batch[42] - loss: 0.143789  acc: 93.0000%(15/16)
Batch[43] - loss: 0.067884  acc: 100.0000%(16/16)
Batch[44] - loss: 0.069157  acc: 100.0000%(16/16)
Batch[45] - loss: 0.147360  acc: 100.0000%(16/16)
Batch[46] - loss: 0.056315  acc: 100.0000%(16/16)
Batch[47] - loss: 0.115967  acc: 100.0000%(16/16)
Batch[48] - loss: 0.115361  acc: 100.0000%(16/16)
Batch[49] - loss: 0.081668  acc: 100.0000%(16/16)
Batch[50] - loss: 0.076759  acc: 100.0000%(16/16)
Batch[51] - loss: 0.166226  acc: 100.0000%(16/16)
Batch[52] - loss: 0.132483  acc: 100.0000%(16/16)
Batch[53] - loss: 0.095724  acc: 100.0000%(16/16)
Batch[54] - loss: 0.215460  acc: 93.0000%(15/16)
Batch[55] - loss: 0.083921  acc: 100.0000%(16/16)
Batch[56] - loss: 0.093869  acc: 100.0000%(16/16)
Batch[57] - loss: 0.083990  acc: 100.0000%(16/16)
Batch[58] - loss: 0.064346  acc: 100.0000%(16/16)
Batch[59] - loss: 0.120777  acc: 100.0000%(16/16)
Batch[60] - loss: 0.104104  acc: 100.0000%(16/16)
Batch[61] - loss: 0.075976  acc: 100.0000%(16/16)
Batch[62] - loss: 0.185246  acc: 100.0000%(13/13)
Average loss:0.154108 average acc:98.000000%
Val set acc: 0.7651006711409396
Best val set acc: 0

Epoch  7 / 18
Batch[0] - loss: 0.025641  acc: 100.0000%(16/16)
Batch[1] - loss: 0.079730  acc: 100.0000%(16/16)
Batch[2] - loss: 0.092926  acc: 100.0000%(16/16)
Batch[3] - loss: 0.042468  acc: 100.0000%(16/16)
Batch[4] - loss: 0.124055  acc: 100.0000%(16/16)
Batch[5] - loss: 0.162184  acc: 100.0000%(16/16)
Batch[6] - loss: 0.096315  acc: 100.0000%(16/16)
Batch[7] - loss: 0.121884  acc: 100.0000%(16/16)
Batch[8] - loss: 0.082835  acc: 100.0000%(16/16)
Batch[9] - loss: 0.097516  acc: 100.0000%(16/16)
Batch[10] - loss: 0.059490  acc: 100.0000%(16/16)
Batch[11] - loss: 0.091509  acc: 100.0000%(16/16)
Batch[12] - loss: 0.080241  acc: 100.0000%(16/16)
Batch[13] - loss: 0.040978  acc: 100.0000%(16/16)
Batch[14] - loss: 0.080304  acc: 100.0000%(16/16)
Batch[15] - loss: 0.038403  acc: 100.0000%(16/16)
Batch[16] - loss: 0.070879  acc: 100.0000%(16/16)
Batch[17] - loss: 0.069286  acc: 100.0000%(16/16)
Batch[18] - loss: 0.039261  acc: 100.0000%(16/16)
Batch[19] - loss: 0.094382  acc: 100.0000%(16/16)
Batch[20] - loss: 0.097602  acc: 100.0000%(16/16)
Batch[21] - loss: 0.051911  acc: 100.0000%(16/16)
Batch[22] - loss: 0.053379  acc: 100.0000%(16/16)
Batch[23] - loss: 0.036246  acc: 100.0000%(16/16)
Batch[24] - loss: 0.026389  acc: 100.0000%(16/16)
Batch[25] - loss: 0.069852  acc: 100.0000%(16/16)
Batch[26] - loss: 0.045841  acc: 100.0000%(16/16)
Batch[27] - loss: 0.074900  acc: 100.0000%(16/16)
Batch[28] - loss: 0.061013  acc: 100.0000%(16/16)
Batch[29] - loss: 0.067364  acc: 100.0000%(16/16)
Batch[30] - loss: 0.056001  acc: 100.0000%(16/16)
Batch[31] - loss: 0.067887  acc: 100.0000%(16/16)
Batch[32] - loss: 0.048331  acc: 100.0000%(16/16)
Batch[33] - loss: 0.051789  acc: 100.0000%(16/16)
Batch[34] - loss: 0.026776  acc: 100.0000%(16/16)
Batch[35] - loss: 0.057774  acc: 100.0000%(16/16)
Batch[36] - loss: 0.083131  acc: 100.0000%(16/16)
Batch[37] - loss: 0.067593  acc: 100.0000%(16/16)
Batch[38] - loss: 0.130464  acc: 100.0000%(16/16)
Batch[39] - loss: 0.062529  acc: 100.0000%(16/16)
Batch[40] - loss: 0.039212  acc: 100.0000%(16/16)
Batch[41] - loss: 0.083116  acc: 100.0000%(16/16)
Batch[42] - loss: 0.055010  acc: 100.0000%(16/16)
Batch[43] - loss: 0.048782  acc: 100.0000%(16/16)
Batch[44] - loss: 0.118508  acc: 100.0000%(16/16)
Batch[45] - loss: 0.052596  acc: 100.0000%(16/16)
Batch[46] - loss: 0.034979  acc: 100.0000%(16/16)
Batch[47] - loss: 0.033555  acc: 100.0000%(16/16)
Batch[48] - loss: 0.078593  acc: 100.0000%(16/16)
Batch[49] - loss: 0.042874  acc: 100.0000%(16/16)
Batch[50] - loss: 0.041290  acc: 100.0000%(16/16)
Batch[51] - loss: 0.060965  acc: 100.0000%(16/16)
Batch[52] - loss: 0.042657  acc: 100.0000%(16/16)
Batch[53] - loss: 0.119564  acc: 100.0000%(16/16)
Batch[54] - loss: 0.068886  acc: 100.0000%(16/16)
Batch[55] - loss: 0.051825  acc: 100.0000%(16/16)
Batch[56] - loss: 0.035045  acc: 100.0000%(16/16)
Batch[57] - loss: 0.046052  acc: 100.0000%(16/16)
Batch[58] - loss: 0.077057  acc: 100.0000%(16/16)
Batch[59] - loss: 0.054286  acc: 100.0000%(16/16)
Batch[60] - loss: 0.066467  acc: 100.0000%(16/16)
Batch[61] - loss: 0.034215  acc: 100.0000%(16/16)
Batch[62] - loss: 0.030594  acc: 100.0000%(13/13)
Average loss:0.065765 average acc:100.000000%
Val set acc: 0.7785234899328859
Best val set acc: 0

Epoch  8 / 18
Batch[0] - loss: 0.091667  acc: 100.0000%(16/16)
Batch[1] - loss: 0.081089  acc: 100.0000%(16/16)
Batch[2] - loss: 0.078409  acc: 100.0000%(16/16)
Batch[3] - loss: 0.040595  acc: 100.0000%(16/16)
Batch[4] - loss: 0.048340  acc: 100.0000%(16/16)
Batch[5] - loss: 0.039468  acc: 100.0000%(16/16)
Batch[6] - loss: 0.039303  acc: 100.0000%(16/16)
Batch[7] - loss: 0.018258  acc: 100.0000%(16/16)
Batch[8] - loss: 0.023573  acc: 100.0000%(16/16)
Batch[9] - loss: 0.017543  acc: 100.0000%(16/16)
Batch[10] - loss: 0.038001  acc: 100.0000%(16/16)
Batch[11] - loss: 0.088022  acc: 93.0000%(15/16)
Batch[12] - loss: 0.023568  acc: 100.0000%(16/16)
Batch[13] - loss: 0.044107  acc: 100.0000%(16/16)
Batch[14] - loss: 0.051728  acc: 100.0000%(16/16)
Batch[15] - loss: 0.039293  acc: 100.0000%(16/16)
Batch[16] - loss: 0.054656  acc: 100.0000%(16/16)
Batch[17] - loss: 0.061709  acc: 100.0000%(16/16)
Batch[18] - loss: 0.032956  acc: 100.0000%(16/16)
Batch[19] - loss: 0.040412  acc: 100.0000%(16/16)
Batch[20] - loss: 0.068689  acc: 100.0000%(16/16)
Batch[21] - loss: 0.027327  acc: 100.0000%(16/16)
Batch[22] - loss: 0.039291  acc: 100.0000%(16/16)
Batch[23] - loss: 0.070219  acc: 100.0000%(16/16)
Batch[24] - loss: 0.020179  acc: 100.0000%(16/16)
Batch[25] - loss: 0.051624  acc: 100.0000%(16/16)
Batch[26] - loss: 0.028069  acc: 100.0000%(16/16)
Batch[27] - loss: 0.020847  acc: 100.0000%(16/16)
Batch[28] - loss: 0.025619  acc: 100.0000%(16/16)
Batch[29] - loss: 0.087417  acc: 100.0000%(16/16)
Batch[30] - loss: 0.019717  acc: 100.0000%(16/16)
Batch[31] - loss: 0.048781  acc: 100.0000%(16/16)
Batch[32] - loss: 0.036505  acc: 100.0000%(16/16)
Batch[33] - loss: 0.027611  acc: 100.0000%(16/16)
Batch[34] - loss: 0.031642  acc: 100.0000%(16/16)
Batch[35] - loss: 0.024632  acc: 100.0000%(16/16)
Batch[36] - loss: 0.018995  acc: 100.0000%(16/16)
Batch[37] - loss: 0.026061  acc: 100.0000%(16/16)
Batch[38] - loss: 0.043914  acc: 100.0000%(16/16)
Batch[39] - loss: 0.042138  acc: 100.0000%(16/16)
Batch[40] - loss: 0.048554  acc: 100.0000%(16/16)
Batch[41] - loss: 0.327386  acc: 87.0000%(14/16)
Batch[42] - loss: 0.046378  acc: 100.0000%(16/16)
Batch[43] - loss: 0.028182  acc: 100.0000%(16/16)
Batch[44] - loss: 0.027700  acc: 100.0000%(16/16)
Batch[45] - loss: 0.022734  acc: 100.0000%(16/16)
Batch[46] - loss: 0.038026  acc: 100.0000%(16/16)
Batch[47] - loss: 0.060412  acc: 100.0000%(16/16)
Batch[48] - loss: 0.037604  acc: 100.0000%(16/16)
Batch[49] - loss: 0.037549  acc: 100.0000%(16/16)
Batch[50] - loss: 0.025302  acc: 100.0000%(16/16)
Batch[51] - loss: 0.015349  acc: 100.0000%(16/16)
Batch[52] - loss: 0.051452  acc: 100.0000%(16/16)
Batch[53] - loss: 0.058232  acc: 100.0000%(16/16)
Batch[54] - loss: 0.061064  acc: 100.0000%(16/16)
Batch[55] - loss: 0.024573  acc: 100.0000%(16/16)
Batch[56] - loss: 0.014167  acc: 100.0000%(16/16)
Batch[57] - loss: 0.056376  acc: 100.0000%(16/16)
Batch[58] - loss: 0.054114  acc: 100.0000%(16/16)
Batch[59] - loss: 0.035684  acc: 100.0000%(16/16)
Batch[60] - loss: 0.022553  acc: 100.0000%(16/16)
Batch[61] - loss: 0.019244  acc: 100.0000%(16/16)
Batch[62] - loss: 0.023783  acc: 100.0000%(13/13)
Average loss:0.045213 average acc:99.000000%
Val set acc: 0.7651006711409396
Best val set acc: 0

Epoch  9 / 18
Batch[0] - loss: 0.050387  acc: 100.0000%(16/16)
Batch[1] - loss: 0.035354  acc: 100.0000%(16/16)
Batch[2] - loss: 0.029940  acc: 100.0000%(16/16)
Batch[3] - loss: 0.016100  acc: 100.0000%(16/16)
Batch[4] - loss: 0.028282  acc: 100.0000%(16/16)
Batch[5] - loss: 0.055320  acc: 100.0000%(16/16)
Batch[6] - loss: 0.042270  acc: 100.0000%(16/16)
Batch[7] - loss: 0.011106  acc: 100.0000%(16/16)
Batch[8] - loss: 0.026399  acc: 100.0000%(16/16)
Batch[9] - loss: 0.010438  acc: 100.0000%(16/16)
Batch[10] - loss: 0.022457  acc: 100.0000%(16/16)
Batch[11] - loss: 0.029696  acc: 100.0000%(16/16)
Batch[12] - loss: 0.019685  acc: 100.0000%(16/16)
Batch[13] - loss: 0.039617  acc: 100.0000%(16/16)
Batch[14] - loss: 0.023614  acc: 100.0000%(16/16)
Batch[15] - loss: 0.029057  acc: 100.0000%(16/16)
Batch[16] - loss: 0.028147  acc: 100.0000%(16/16)
Batch[17] - loss: 0.019594  acc: 100.0000%(16/16)
Batch[18] - loss: 0.016321  acc: 100.0000%(16/16)
Batch[19] - loss: 0.030457  acc: 100.0000%(16/16)
Batch[20] - loss: 0.014352  acc: 100.0000%(16/16)
Batch[21] - loss: 0.047283  acc: 100.0000%(16/16)
Batch[22] - loss: 0.040917  acc: 100.0000%(16/16)
Batch[23] - loss: 0.020495  acc: 100.0000%(16/16)
Batch[24] - loss: 0.018863  acc: 100.0000%(16/16)
Batch[25] - loss: 0.017411  acc: 100.0000%(16/16)
Batch[26] - loss: 0.019965  acc: 100.0000%(16/16)
Batch[27] - loss: 0.013782  acc: 100.0000%(16/16)
Batch[28] - loss: 0.026353  acc: 100.0000%(16/16)
Batch[29] - loss: 0.102676  acc: 93.0000%(15/16)
Batch[30] - loss: 0.045166  acc: 100.0000%(16/16)
Batch[31] - loss: 0.039124  acc: 100.0000%(16/16)
Batch[32] - loss: 0.031689  acc: 100.0000%(16/16)
Batch[33] - loss: 0.032489  acc: 100.0000%(16/16)
Batch[34] - loss: 0.022753  acc: 100.0000%(16/16)
Batch[35] - loss: 0.025569  acc: 100.0000%(16/16)
Batch[36] - loss: 0.050670  acc: 100.0000%(16/16)
Batch[37] - loss: 0.020269  acc: 100.0000%(16/16)
Batch[38] - loss: 0.041943  acc: 100.0000%(16/16)
Batch[39] - loss: 0.010356  acc: 100.0000%(16/16)
Batch[40] - loss: 0.033970  acc: 100.0000%(16/16)
Batch[41] - loss: 0.018721  acc: 100.0000%(16/16)
Batch[42] - loss: 0.017834  acc: 100.0000%(16/16)
Batch[43] - loss: 0.016798  acc: 100.0000%(16/16)
Batch[44] - loss: 0.031443  acc: 100.0000%(16/16)
Batch[45] - loss: 0.019427  acc: 100.0000%(16/16)
Batch[46] - loss: 0.012079  acc: 100.0000%(16/16)
Batch[47] - loss: 0.032022  acc: 100.0000%(16/16)
Batch[48] - loss: 0.021133  acc: 100.0000%(16/16)
Batch[49] - loss: 0.010427  acc: 100.0000%(16/16)
Batch[50] - loss: 0.024943  acc: 100.0000%(16/16)
Batch[51] - loss: 0.016814  acc: 100.0000%(16/16)
Batch[52] - loss: 0.019595  acc: 100.0000%(16/16)
Batch[53] - loss: 0.010333  acc: 100.0000%(16/16)
Batch[54] - loss: 0.016525  acc: 100.0000%(16/16)
Batch[55] - loss: 0.020759  acc: 100.0000%(16/16)
Batch[56] - loss: 0.086880  acc: 93.0000%(15/16)
Batch[57] - loss: 0.009555  acc: 100.0000%(16/16)
Batch[58] - loss: 0.035029  acc: 100.0000%(16/16)
Batch[59] - loss: 0.035398  acc: 100.0000%(16/16)
Batch[60] - loss: 0.021216  acc: 100.0000%(16/16)
Batch[61] - loss: 0.051105  acc: 100.0000%(16/16)
Batch[62] - loss: 0.024709  acc: 100.0000%(13/13)
Average loss:0.028462 average acc:99.000000%
Val set acc: 0.7986577181208053
Best val set acc: 0
              precision    recall  f1-score   support

          NR    0.68889   0.81579   0.74699        38
          FR    0.79412   0.72973   0.76056        37
          TR    0.81081   0.81081   0.81081        37
          UR    0.93939   0.83784   0.88571        37

    accuracy                        0.79866       149
   macro avg    0.80830   0.79854   0.80102       149
weighted avg    0.80750   0.79866   0.80066       149

save model!!!

Epoch  10 / 18
Batch[0] - loss: 0.010014  acc: 100.0000%(16/16)
Batch[1] - loss: 0.029514  acc: 100.0000%(16/16)
Batch[2] - loss: 0.034175  acc: 100.0000%(16/16)
Batch[3] - loss: 0.007745  acc: 100.0000%(16/16)
Batch[4] - loss: 0.017799  acc: 100.0000%(16/16)
Batch[5] - loss: 0.011003  acc: 100.0000%(16/16)
Batch[6] - loss: 0.020106  acc: 100.0000%(16/16)
Batch[7] - loss: 0.189499  acc: 93.0000%(15/16)
Batch[8] - loss: 0.009252  acc: 100.0000%(16/16)
Batch[9] - loss: 0.013688  acc: 100.0000%(16/16)
Batch[10] - loss: 0.090492  acc: 93.0000%(15/16)
Batch[11] - loss: 0.016175  acc: 100.0000%(16/16)
Batch[12] - loss: 0.024274  acc: 100.0000%(16/16)
Batch[13] - loss: 0.011467  acc: 100.0000%(16/16)
Batch[14] - loss: 0.047362  acc: 100.0000%(16/16)
Batch[15] - loss: 0.034493  acc: 100.0000%(16/16)
Batch[16] - loss: 0.020213  acc: 100.0000%(16/16)
Batch[17] - loss: 0.089201  acc: 100.0000%(16/16)
Batch[18] - loss: 0.017923  acc: 100.0000%(16/16)
Batch[19] - loss: 0.111568  acc: 93.0000%(15/16)
Batch[20] - loss: 0.041135  acc: 100.0000%(16/16)
Batch[21] - loss: 0.014716  acc: 100.0000%(16/16)
Batch[22] - loss: 0.015847  acc: 100.0000%(16/16)
Batch[23] - loss: 0.011460  acc: 100.0000%(16/16)
Batch[24] - loss: 0.012218  acc: 100.0000%(16/16)
Batch[25] - loss: 0.019970  acc: 100.0000%(16/16)
Batch[26] - loss: 0.023885  acc: 100.0000%(16/16)
Batch[27] - loss: 0.055458  acc: 100.0000%(16/16)
Batch[28] - loss: 0.019079  acc: 100.0000%(16/16)
Batch[29] - loss: 0.026882  acc: 100.0000%(16/16)
Batch[30] - loss: 0.025613  acc: 100.0000%(16/16)
Batch[31] - loss: 0.023446  acc: 100.0000%(16/16)
Batch[32] - loss: 0.013675  acc: 100.0000%(16/16)
Batch[33] - loss: 0.015570  acc: 100.0000%(16/16)
Batch[34] - loss: 0.037797  acc: 100.0000%(16/16)
Batch[35] - loss: 0.116090  acc: 93.0000%(15/16)
Batch[36] - loss: 0.022705  acc: 100.0000%(16/16)
Batch[37] - loss: 0.009210  acc: 100.0000%(16/16)
Batch[38] - loss: 0.010968  acc: 100.0000%(16/16)
Batch[39] - loss: 0.011785  acc: 100.0000%(16/16)
Batch[40] - loss: 0.009042  acc: 100.0000%(16/16)
Batch[41] - loss: 0.042708  acc: 100.0000%(16/16)
Batch[42] - loss: 0.021335  acc: 100.0000%(16/16)
Batch[43] - loss: 0.068207  acc: 100.0000%(16/16)
Batch[44] - loss: 0.015240  acc: 100.0000%(16/16)
Batch[45] - loss: 0.021518  acc: 100.0000%(16/16)
Batch[46] - loss: 0.010536  acc: 100.0000%(16/16)
Batch[47] - loss: 0.029302  acc: 100.0000%(16/16)
Batch[48] - loss: 0.017620  acc: 100.0000%(16/16)
Batch[49] - loss: 0.025295  acc: 100.0000%(16/16)
Batch[50] - loss: 0.019442  acc: 100.0000%(16/16)
Batch[51] - loss: 0.008904  acc: 100.0000%(16/16)
Batch[52] - loss: 0.025209  acc: 100.0000%(16/16)
Batch[53] - loss: 0.017746  acc: 100.0000%(16/16)
Batch[54] - loss: 0.007016  acc: 100.0000%(16/16)
Batch[55] - loss: 0.011524  acc: 100.0000%(16/16)
Batch[56] - loss: 0.027711  acc: 100.0000%(16/16)
Batch[57] - loss: 0.011961  acc: 100.0000%(16/16)
Batch[58] - loss: 0.014841  acc: 100.0000%(16/16)
Batch[59] - loss: 0.031980  acc: 100.0000%(16/16)
Batch[60] - loss: 0.007524  acc: 100.0000%(16/16)
Batch[61] - loss: 0.031052  acc: 100.0000%(16/16)
Batch[62] - loss: 0.014354  acc: 100.0000%(13/13)
Average loss:0.028945 average acc:99.000000%
Val set acc: 0.7919463087248322
Best val set acc: 0.7986577181208053

Epoch  11 / 18
Batch[0] - loss: 0.034210  acc: 100.0000%(16/16)
Batch[1] - loss: 0.042258  acc: 100.0000%(16/16)
Batch[2] - loss: 0.006274  acc: 100.0000%(16/16)
Batch[3] - loss: 0.025305  acc: 100.0000%(16/16)
Batch[4] - loss: 0.016234  acc: 100.0000%(16/16)
Batch[5] - loss: 0.013133  acc: 100.0000%(16/16)
Batch[6] - loss: 0.010883  acc: 100.0000%(16/16)
Batch[7] - loss: 0.010537  acc: 100.0000%(16/16)
Batch[8] - loss: 0.014039  acc: 100.0000%(16/16)
Batch[9] - loss: 0.021368  acc: 100.0000%(16/16)
Batch[10] - loss: 0.035560  acc: 100.0000%(16/16)
Batch[11] - loss: 0.010436  acc: 100.0000%(16/16)
Batch[12] - loss: 0.011902  acc: 100.0000%(16/16)
Batch[13] - loss: 0.007651  acc: 100.0000%(16/16)
Batch[14] - loss: 0.072444  acc: 100.0000%(16/16)
Batch[15] - loss: 0.025108  acc: 100.0000%(16/16)
Batch[16] - loss: 0.013817  acc: 100.0000%(16/16)
Batch[17] - loss: 0.012223  acc: 100.0000%(16/16)
Batch[18] - loss: 0.009221  acc: 100.0000%(16/16)
Batch[19] - loss: 0.014072  acc: 100.0000%(16/16)
Batch[20] - loss: 0.017215  acc: 100.0000%(16/16)
Batch[21] - loss: 0.015819  acc: 100.0000%(16/16)
Batch[22] - loss: 0.008562  acc: 100.0000%(16/16)
Batch[23] - loss: 0.010913  acc: 100.0000%(16/16)
Batch[24] - loss: 0.009146  acc: 100.0000%(16/16)
Batch[25] - loss: 0.011833  acc: 100.0000%(16/16)
Batch[26] - loss: 0.024730  acc: 100.0000%(16/16)
Batch[27] - loss: 0.016386  acc: 100.0000%(16/16)
Batch[28] - loss: 0.014445  acc: 100.0000%(16/16)
Batch[29] - loss: 0.035786  acc: 100.0000%(16/16)
Batch[30] - loss: 0.007864  acc: 100.0000%(16/16)
Batch[31] - loss: 0.007829  acc: 100.0000%(16/16)
Batch[32] - loss: 0.016725  acc: 100.0000%(16/16)
Batch[33] - loss: 0.021450  acc: 100.0000%(16/16)
Batch[34] - loss: 0.017434  acc: 100.0000%(16/16)
Batch[35] - loss: 0.024546  acc: 100.0000%(16/16)
Batch[36] - loss: 0.014491  acc: 100.0000%(16/16)
Batch[37] - loss: 0.008422  acc: 100.0000%(16/16)
Batch[38] - loss: 0.007480  acc: 100.0000%(16/16)
Batch[39] - loss: 0.068067  acc: 93.0000%(15/16)
Batch[40] - loss: 0.029901  acc: 100.0000%(16/16)
Batch[41] - loss: 0.021415  acc: 100.0000%(16/16)
Batch[42] - loss: 0.011890  acc: 100.0000%(16/16)
Batch[43] - loss: 0.012125  acc: 100.0000%(16/16)
Batch[44] - loss: 0.007380  acc: 100.0000%(16/16)
Batch[45] - loss: 0.018665  acc: 100.0000%(16/16)
Batch[46] - loss: 0.008411  acc: 100.0000%(16/16)
Batch[47] - loss: 0.008899  acc: 100.0000%(16/16)
Batch[48] - loss: 0.013988  acc: 100.0000%(16/16)
Batch[49] - loss: 0.014441  acc: 100.0000%(16/16)
Batch[50] - loss: 0.010542  acc: 100.0000%(16/16)
Batch[51] - loss: 0.014071  acc: 100.0000%(16/16)
Batch[52] - loss: 0.043622  acc: 100.0000%(16/16)
Batch[53] - loss: 0.056085  acc: 100.0000%(16/16)
Batch[54] - loss: 0.005943  acc: 100.0000%(16/16)
Batch[55] - loss: 0.009568  acc: 100.0000%(16/16)
Batch[56] - loss: 0.015452  acc: 100.0000%(16/16)
Batch[57] - loss: 0.014625  acc: 100.0000%(16/16)
Batch[58] - loss: 0.037363  acc: 100.0000%(16/16)
Batch[59] - loss: 0.010898  acc: 100.0000%(16/16)
Batch[60] - loss: 0.002457  acc: 100.0000%(16/16)
Batch[61] - loss: 0.010083  acc: 100.0000%(16/16)
Batch[62] - loss: 0.005722  acc: 100.0000%(13/13)
Average loss:0.018244 average acc:99.000000%
Val set acc: 0.7986577181208053
Best val set acc: 0.7986577181208053

Epoch  12 / 18
Batch[0] - loss: 0.042586  acc: 100.0000%(16/16)
Batch[1] - loss: 0.017470  acc: 100.0000%(16/16)
Batch[2] - loss: 0.015906  acc: 100.0000%(16/16)
Batch[3] - loss: 0.007457  acc: 100.0000%(16/16)
Batch[4] - loss: 0.008203  acc: 100.0000%(16/16)
Batch[5] - loss: 0.008644  acc: 100.0000%(16/16)
Batch[6] - loss: 0.015577  acc: 100.0000%(16/16)
Batch[7] - loss: 0.006599  acc: 100.0000%(16/16)
Batch[8] - loss: 0.013220  acc: 100.0000%(16/16)
Batch[9] - loss: 0.004837  acc: 100.0000%(16/16)
Batch[10] - loss: 0.009410  acc: 100.0000%(16/16)
Batch[11] - loss: 0.009391  acc: 100.0000%(16/16)
Batch[12] - loss: 0.020738  acc: 100.0000%(16/16)
Batch[13] - loss: 0.011042  acc: 100.0000%(16/16)
Batch[14] - loss: 0.020625  acc: 100.0000%(16/16)
Batch[15] - loss: 0.013576  acc: 100.0000%(16/16)
Batch[16] - loss: 0.011112  acc: 100.0000%(16/16)
Batch[17] - loss: 0.081363  acc: 93.0000%(15/16)
Batch[18] - loss: 0.012012  acc: 100.0000%(16/16)
Batch[19] - loss: 0.017980  acc: 100.0000%(16/16)
Batch[20] - loss: 0.024696  acc: 100.0000%(16/16)
Batch[21] - loss: 0.013067  acc: 100.0000%(16/16)
Batch[22] - loss: 0.009479  acc: 100.0000%(16/16)
Batch[23] - loss: 0.024188  acc: 100.0000%(16/16)
Batch[24] - loss: 0.007336  acc: 100.0000%(16/16)
Batch[25] - loss: 0.015463  acc: 100.0000%(16/16)
Batch[26] - loss: 0.017794  acc: 100.0000%(16/16)
Batch[27] - loss: 0.032996  acc: 100.0000%(16/16)
Batch[28] - loss: 0.008159  acc: 100.0000%(16/16)
Batch[29] - loss: 0.007530  acc: 100.0000%(16/16)
Batch[30] - loss: 0.011981  acc: 100.0000%(16/16)
Batch[31] - loss: 0.010799  acc: 100.0000%(16/16)
Batch[32] - loss: 0.030894  acc: 100.0000%(16/16)
Batch[33] - loss: 0.007456  acc: 100.0000%(16/16)
Batch[34] - loss: 0.017788  acc: 100.0000%(16/16)
Batch[35] - loss: 0.010125  acc: 100.0000%(16/16)
Batch[36] - loss: 0.010795  acc: 100.0000%(16/16)
Batch[37] - loss: 0.018198  acc: 100.0000%(16/16)
Batch[38] - loss: 0.008726  acc: 100.0000%(16/16)
Batch[39] - loss: 0.011793  acc: 100.0000%(16/16)
Batch[40] - loss: 0.004510  acc: 100.0000%(16/16)
Batch[41] - loss: 0.021411  acc: 100.0000%(16/16)
Batch[42] - loss: 0.008545  acc: 100.0000%(16/16)
Batch[43] - loss: 0.013359  acc: 100.0000%(16/16)
Batch[44] - loss: 0.012923  acc: 100.0000%(16/16)
Batch[45] - loss: 0.005506  acc: 100.0000%(16/16)
Batch[46] - loss: 0.012376  acc: 100.0000%(16/16)
Batch[47] - loss: 0.032993  acc: 100.0000%(16/16)
Batch[48] - loss: 0.086604  acc: 93.0000%(15/16)
Batch[49] - loss: 0.013924  acc: 100.0000%(16/16)
Batch[50] - loss: 0.012158  acc: 100.0000%(16/16)
Batch[51] - loss: 0.015675  acc: 100.0000%(16/16)
Batch[52] - loss: 0.004435  acc: 100.0000%(16/16)
Batch[53] - loss: 0.056518  acc: 93.0000%(15/16)
Batch[54] - loss: 0.009111  acc: 100.0000%(16/16)
Batch[55] - loss: 0.009020  acc: 100.0000%(16/16)
Batch[56] - loss: 0.005999  acc: 100.0000%(16/16)
Batch[57] - loss: 0.005025  acc: 100.0000%(16/16)
Batch[58] - loss: 0.013251  acc: 100.0000%(16/16)
Batch[59] - loss: 0.006850  acc: 100.0000%(16/16)
Batch[60] - loss: 0.002888  acc: 100.0000%(16/16)
Batch[61] - loss: 0.011156  acc: 100.0000%(16/16)
Batch[62] - loss: 0.010729  acc: 100.0000%(13/13)
Average loss:0.016222 average acc:99.000000%
Val set acc: 0.7986577181208053
Best val set acc: 0.7986577181208053

Epoch  13 / 18
Batch[0] - loss: 0.026615  acc: 100.0000%(16/16)
Batch[1] - loss: 0.007864  acc: 100.0000%(16/16)
Batch[2] - loss: 0.020372  acc: 100.0000%(16/16)
Batch[3] - loss: 0.007617  acc: 100.0000%(16/16)
Batch[4] - loss: 0.015443  acc: 100.0000%(16/16)
Batch[5] - loss: 0.007015  acc: 100.0000%(16/16)
Batch[6] - loss: 0.007472  acc: 100.0000%(16/16)
Batch[7] - loss: 0.015236  acc: 100.0000%(16/16)
Batch[8] - loss: 0.009337  acc: 100.0000%(16/16)
Batch[9] - loss: 0.018113  acc: 100.0000%(16/16)
Batch[10] - loss: 0.004355  acc: 100.0000%(16/16)
Batch[11] - loss: 0.011366  acc: 100.0000%(16/16)
Batch[12] - loss: 0.011141  acc: 100.0000%(16/16)
Batch[13] - loss: 0.008751  acc: 100.0000%(16/16)
Batch[14] - loss: 0.007270  acc: 100.0000%(16/16)
Batch[15] - loss: 0.006032  acc: 100.0000%(16/16)
Batch[16] - loss: 0.004908  acc: 100.0000%(16/16)
Batch[17] - loss: 0.011883  acc: 100.0000%(16/16)
Batch[18] - loss: 0.007246  acc: 100.0000%(16/16)
Batch[19] - loss: 0.036205  acc: 100.0000%(16/16)
Batch[20] - loss: 0.008946  acc: 100.0000%(16/16)
Batch[21] - loss: 0.007279  acc: 100.0000%(16/16)
Batch[22] - loss: 0.026526  acc: 100.0000%(16/16)
Batch[23] - loss: 0.010524  acc: 100.0000%(16/16)
Batch[24] - loss: 0.005410  acc: 100.0000%(16/16)
Batch[25] - loss: 0.009416  acc: 100.0000%(16/16)
Batch[26] - loss: 0.011322  acc: 100.0000%(16/16)
Batch[27] - loss: 0.015925  acc: 100.0000%(16/16)
Batch[28] - loss: 0.004707  acc: 100.0000%(16/16)
Batch[29] - loss: 0.009538  acc: 100.0000%(16/16)
Batch[30] - loss: 0.008996  acc: 100.0000%(16/16)
Batch[31] - loss: 0.014935  acc: 100.0000%(16/16)
Batch[32] - loss: 0.009419  acc: 100.0000%(16/16)
Batch[33] - loss: 0.015836  acc: 100.0000%(16/16)
Batch[34] - loss: 0.009602  acc: 100.0000%(16/16)
Batch[35] - loss: 0.027035  acc: 100.0000%(16/16)
Batch[36] - loss: 0.010152  acc: 100.0000%(16/16)
Batch[37] - loss: 0.006714  acc: 100.0000%(16/16)
Batch[38] - loss: 0.008423  acc: 100.0000%(16/16)
Batch[39] - loss: 0.006779  acc: 100.0000%(16/16)
Batch[40] - loss: 0.009215  acc: 100.0000%(16/16)
Batch[41] - loss: 0.005265  acc: 100.0000%(16/16)
Batch[42] - loss: 0.010044  acc: 100.0000%(16/16)
Batch[43] - loss: 0.017535  acc: 100.0000%(16/16)
Batch[44] - loss: 0.011954  acc: 100.0000%(16/16)
Batch[45] - loss: 0.006174  acc: 100.0000%(16/16)
Batch[46] - loss: 0.006083  acc: 100.0000%(16/16)
Batch[47] - loss: 0.010585  acc: 100.0000%(16/16)
Batch[48] - loss: 0.008464  acc: 100.0000%(16/16)
Batch[49] - loss: 0.010609  acc: 100.0000%(16/16)
Batch[50] - loss: 0.005665  acc: 100.0000%(16/16)
Batch[51] - loss: 0.011404  acc: 100.0000%(16/16)
Batch[52] - loss: 0.005555  acc: 100.0000%(16/16)
Batch[53] - loss: 0.005405  acc: 100.0000%(16/16)
Batch[54] - loss: 0.006095  acc: 100.0000%(16/16)
Batch[55] - loss: 0.011055  acc: 100.0000%(16/16)
Batch[56] - loss: 0.008649  acc: 100.0000%(16/16)
Batch[57] - loss: 0.005330  acc: 100.0000%(16/16)
Batch[58] - loss: 0.004402  acc: 100.0000%(16/16)
Batch[59] - loss: 0.004022  acc: 100.0000%(16/16)
Batch[60] - loss: 0.010547  acc: 100.0000%(16/16)
Batch[61] - loss: 0.008029  acc: 100.0000%(16/16)
Batch[62] - loss: 0.007305  acc: 100.0000%(13/13)
Average loss:0.010494 average acc:100.000000%
Reload the best model...
0.0005
Val set acc: 0.7986577181208053
Best val set acc: 0.7986577181208053

Epoch  14 / 18
Batch[0] - loss: 0.057665  acc: 100.0000%(16/16)
Batch[1] - loss: 0.023288  acc: 100.0000%(16/16)
Batch[2] - loss: 0.016353  acc: 100.0000%(16/16)
Batch[3] - loss: 0.018710  acc: 100.0000%(16/16)
Batch[4] - loss: 0.019034  acc: 100.0000%(16/16)
Batch[5] - loss: 0.020916  acc: 100.0000%(16/16)
Batch[6] - loss: 0.023437  acc: 100.0000%(16/16)
Batch[7] - loss: 0.017777  acc: 100.0000%(16/16)
Batch[8] - loss: 0.024401  acc: 100.0000%(16/16)
Batch[9] - loss: 0.036306  acc: 100.0000%(16/16)
Batch[10] - loss: 0.045410  acc: 100.0000%(16/16)
Batch[11] - loss: 0.013404  acc: 100.0000%(16/16)
Batch[12] - loss: 0.032011  acc: 100.0000%(16/16)
Batch[13] - loss: 0.038664  acc: 100.0000%(16/16)
Batch[14] - loss: 0.025550  acc: 100.0000%(16/16)
Batch[15] - loss: 0.021879  acc: 100.0000%(16/16)
Batch[16] - loss: 0.014601  acc: 100.0000%(16/16)
Batch[17] - loss: 0.013480  acc: 100.0000%(16/16)
Batch[18] - loss: 0.027805  acc: 100.0000%(16/16)
Batch[19] - loss: 0.024155  acc: 100.0000%(16/16)
Batch[20] - loss: 0.041677  acc: 100.0000%(16/16)
Batch[21] - loss: 0.017331  acc: 100.0000%(16/16)
Batch[22] - loss: 0.017788  acc: 100.0000%(16/16)
Batch[23] - loss: 0.020008  acc: 100.0000%(16/16)
Batch[24] - loss: 0.027920  acc: 100.0000%(16/16)
Batch[25] - loss: 0.015935  acc: 100.0000%(16/16)
Batch[26] - loss: 0.010976  acc: 100.0000%(16/16)
Batch[27] - loss: 0.019166  acc: 100.0000%(16/16)
Batch[28] - loss: 0.033694  acc: 100.0000%(16/16)
Batch[29] - loss: 0.041911  acc: 100.0000%(16/16)
Batch[30] - loss: 0.012170  acc: 100.0000%(16/16)
Batch[31] - loss: 0.010914  acc: 100.0000%(16/16)
Batch[32] - loss: 0.021212  acc: 100.0000%(16/16)
Batch[33] - loss: 0.031544  acc: 100.0000%(16/16)
Batch[34] - loss: 0.010392  acc: 100.0000%(16/16)
Batch[35] - loss: 0.014817  acc: 100.0000%(16/16)
Batch[36] - loss: 0.027433  acc: 100.0000%(16/16)
Batch[37] - loss: 0.025870  acc: 100.0000%(16/16)
Batch[38] - loss: 0.009041  acc: 100.0000%(16/16)
Batch[39] - loss: 0.014903  acc: 100.0000%(16/16)
Batch[40] - loss: 0.012985  acc: 100.0000%(16/16)
Batch[41] - loss: 0.011628  acc: 100.0000%(16/16)
Batch[42] - loss: 0.040116  acc: 100.0000%(16/16)
Batch[43] - loss: 0.014559  acc: 100.0000%(16/16)
Batch[44] - loss: 0.015379  acc: 100.0000%(16/16)
Batch[45] - loss: 0.017239  acc: 100.0000%(16/16)
Batch[46] - loss: 0.015247  acc: 100.0000%(16/16)
Batch[47] - loss: 0.021567  acc: 100.0000%(16/16)
Batch[48] - loss: 0.021203  acc: 100.0000%(16/16)
Batch[49] - loss: 0.055825  acc: 100.0000%(16/16)
Batch[50] - loss: 0.020501  acc: 100.0000%(16/16)
Batch[51] - loss: 0.034944  acc: 100.0000%(16/16)
Batch[52] - loss: 0.009829  acc: 100.0000%(16/16)
Batch[53] - loss: 0.018979  acc: 100.0000%(16/16)
Batch[54] - loss: 0.019991  acc: 100.0000%(16/16)
Batch[55] - loss: 0.012609  acc: 100.0000%(16/16)
Batch[56] - loss: 0.021198  acc: 100.0000%(16/16)
Batch[57] - loss: 0.010376  acc: 100.0000%(16/16)
Batch[58] - loss: 0.025292  acc: 100.0000%(16/16)
Batch[59] - loss: 0.017602  acc: 100.0000%(16/16)
Batch[60] - loss: 0.010324  acc: 100.0000%(16/16)
Batch[61] - loss: 0.008019  acc: 100.0000%(16/16)
Batch[62] - loss: 0.015091  acc: 100.0000%(13/13)
Average loss:0.022064 average acc:100.000000%
Val set acc: 0.7919463087248322
Best val set acc: 0.7986577181208053

Epoch  15 / 18
Batch[0] - loss: 0.012141  acc: 100.0000%(16/16)
Batch[1] - loss: 0.031116  acc: 100.0000%(16/16)
Batch[2] - loss: 0.023626  acc: 100.0000%(16/16)
Batch[3] - loss: 0.009993  acc: 100.0000%(16/16)
Batch[4] - loss: 0.008777  acc: 100.0000%(16/16)
Batch[5] - loss: 0.021284  acc: 100.0000%(16/16)
Batch[6] - loss: 0.018431  acc: 100.0000%(16/16)
Batch[7] - loss: 0.013372  acc: 100.0000%(16/16)
Batch[8] - loss: 0.015971  acc: 100.0000%(16/16)
Batch[9] - loss: 0.008466  acc: 100.0000%(16/16)
Batch[10] - loss: 0.011117  acc: 100.0000%(16/16)
Batch[11] - loss: 0.007014  acc: 100.0000%(16/16)
Batch[12] - loss: 0.010832  acc: 100.0000%(16/16)
Batch[13] - loss: 0.020811  acc: 100.0000%(16/16)
Batch[14] - loss: 0.004162  acc: 100.0000%(16/16)
Batch[15] - loss: 0.017183  acc: 100.0000%(16/16)
Batch[16] - loss: 0.020196  acc: 100.0000%(16/16)
Batch[17] - loss: 0.009303  acc: 100.0000%(16/16)
Batch[18] - loss: 0.030103  acc: 100.0000%(16/16)
Batch[19] - loss: 0.026164  acc: 100.0000%(16/16)
Batch[20] - loss: 0.008332  acc: 100.0000%(16/16)
Batch[21] - loss: 0.017145  acc: 100.0000%(16/16)
Batch[22] - loss: 0.014273  acc: 100.0000%(16/16)
Batch[23] - loss: 0.012873  acc: 100.0000%(16/16)
Batch[24] - loss: 0.017330  acc: 100.0000%(16/16)
Batch[25] - loss: 0.010365  acc: 100.0000%(16/16)
Batch[26] - loss: 0.025705  acc: 100.0000%(16/16)
Batch[27] - loss: 0.026652  acc: 100.0000%(16/16)
Batch[28] - loss: 0.018940  acc: 100.0000%(16/16)
Batch[29] - loss: 0.008739  acc: 100.0000%(16/16)
Batch[30] - loss: 0.015524  acc: 100.0000%(16/16)
Batch[31] - loss: 0.010575  acc: 100.0000%(16/16)
Batch[32] - loss: 0.010798  acc: 100.0000%(16/16)
Batch[33] - loss: 0.021656  acc: 100.0000%(16/16)
Batch[34] - loss: 0.006763  acc: 100.0000%(16/16)
Batch[35] - loss: 0.014675  acc: 100.0000%(16/16)
Batch[36] - loss: 0.076399  acc: 100.0000%(16/16)
Batch[37] - loss: 0.012676  acc: 100.0000%(16/16)
Batch[38] - loss: 0.012572  acc: 100.0000%(16/16)
Batch[39] - loss: 0.017899  acc: 100.0000%(16/16)
Batch[40] - loss: 0.030231  acc: 100.0000%(16/16)
Batch[41] - loss: 0.006963  acc: 100.0000%(16/16)
Batch[42] - loss: 0.030641  acc: 100.0000%(16/16)
Batch[43] - loss: 0.017974  acc: 100.0000%(16/16)
Batch[44] - loss: 0.046473  acc: 100.0000%(16/16)
Batch[45] - loss: 0.008840  acc: 100.0000%(16/16)
Batch[46] - loss: 0.008943  acc: 100.0000%(16/16)
Batch[47] - loss: 0.019693  acc: 100.0000%(16/16)
Batch[48] - loss: 0.028891  acc: 100.0000%(16/16)
Batch[49] - loss: 0.012114  acc: 100.0000%(16/16)
Batch[50] - loss: 0.015592  acc: 100.0000%(16/16)
Batch[51] - loss: 0.010951  acc: 100.0000%(16/16)
Batch[52] - loss: 0.037587  acc: 100.0000%(16/16)
Batch[53] - loss: 0.004212  acc: 100.0000%(16/16)
Batch[54] - loss: 0.018897  acc: 100.0000%(16/16)
Batch[55] - loss: 0.013070  acc: 100.0000%(16/16)
Batch[56] - loss: 0.021093  acc: 100.0000%(16/16)
Batch[57] - loss: 0.045623  acc: 100.0000%(16/16)
Batch[58] - loss: 0.015458  acc: 100.0000%(16/16)
Batch[59] - loss: 0.015572  acc: 100.0000%(16/16)
Batch[60] - loss: 0.018619  acc: 100.0000%(16/16)
Batch[61] - loss: 0.009265  acc: 100.0000%(16/16)
Batch[62] - loss: 0.043776  acc: 100.0000%(13/13)
Average loss:0.018420 average acc:100.000000%
Val set acc: 0.7919463087248322
Best val set acc: 0.7986577181208053

Epoch  16 / 18
Batch[0] - loss: 0.022730  acc: 100.0000%(16/16)
Batch[1] - loss: 0.013017  acc: 100.0000%(16/16)
Batch[2] - loss: 0.172845  acc: 93.0000%(15/16)
Batch[3] - loss: 0.008807  acc: 100.0000%(16/16)
Batch[4] - loss: 0.028070  acc: 100.0000%(16/16)
Batch[5] - loss: 0.008326  acc: 100.0000%(16/16)
Batch[6] - loss: 0.012958  acc: 100.0000%(16/16)
Batch[7] - loss: 0.010649  acc: 100.0000%(16/16)
Batch[8] - loss: 0.013606  acc: 100.0000%(16/16)
Batch[9] - loss: 0.009256  acc: 100.0000%(16/16)
Batch[10] - loss: 0.009706  acc: 100.0000%(16/16)
Batch[11] - loss: 0.028322  acc: 100.0000%(16/16)
Batch[12] - loss: 0.019796  acc: 100.0000%(16/16)
Batch[13] - loss: 0.007744  acc: 100.0000%(16/16)
Batch[14] - loss: 0.125362  acc: 87.0000%(14/16)
Batch[15] - loss: 0.008293  acc: 100.0000%(16/16)
Batch[16] - loss: 0.012157  acc: 100.0000%(16/16)
Batch[17] - loss: 0.020875  acc: 100.0000%(16/16)
Batch[18] - loss: 0.006836  acc: 100.0000%(16/16)
Batch[19] - loss: 0.029761  acc: 100.0000%(16/16)
Batch[20] - loss: 0.016091  acc: 100.0000%(16/16)
Batch[21] - loss: 0.012913  acc: 100.0000%(16/16)
Batch[22] - loss: 0.027021  acc: 100.0000%(16/16)
Batch[23] - loss: 0.013001  acc: 100.0000%(16/16)
Batch[24] - loss: 0.006368  acc: 100.0000%(16/16)
Batch[25] - loss: 0.017606  acc: 100.0000%(16/16)
Batch[26] - loss: 0.022572  acc: 100.0000%(16/16)
Batch[27] - loss: 0.014941  acc: 100.0000%(16/16)
Batch[28] - loss: 0.010591  acc: 100.0000%(16/16)
Batch[29] - loss: 0.008338  acc: 100.0000%(16/16)
Batch[30] - loss: 0.044524  acc: 100.0000%(16/16)
Batch[31] - loss: 0.037748  acc: 100.0000%(16/16)
Batch[32] - loss: 0.005464  acc: 100.0000%(16/16)
Batch[33] - loss: 0.012726  acc: 100.0000%(16/16)
Batch[34] - loss: 0.009059  acc: 100.0000%(16/16)
Batch[35] - loss: 0.008950  acc: 100.0000%(16/16)
Batch[36] - loss: 0.029333  acc: 100.0000%(16/16)
Batch[37] - loss: 0.008094  acc: 100.0000%(16/16)
Batch[38] - loss: 0.010558  acc: 100.0000%(16/16)
Batch[39] - loss: 0.014678  acc: 100.0000%(16/16)
Batch[40] - loss: 0.017192  acc: 100.0000%(16/16)
Batch[41] - loss: 0.031193  acc: 100.0000%(16/16)
Batch[42] - loss: 0.016825  acc: 100.0000%(16/16)
Batch[43] - loss: 0.012245  acc: 100.0000%(16/16)
Batch[44] - loss: 0.007192  acc: 100.0000%(16/16)
Batch[45] - loss: 0.022633  acc: 100.0000%(16/16)
Batch[46] - loss: 0.011398  acc: 100.0000%(16/16)
Batch[47] - loss: 0.006985  acc: 100.0000%(16/16)
Batch[48] - loss: 0.032533  acc: 100.0000%(16/16)
Batch[49] - loss: 0.011282  acc: 100.0000%(16/16)
Batch[50] - loss: 0.016337  acc: 100.0000%(16/16)
Batch[51] - loss: 0.011150  acc: 100.0000%(16/16)
Batch[52] - loss: 0.024590  acc: 100.0000%(16/16)
Batch[53] - loss: 0.015809  acc: 100.0000%(16/16)
Batch[54] - loss: 0.015368  acc: 100.0000%(16/16)
Batch[55] - loss: 0.009292  acc: 100.0000%(16/16)
Batch[56] - loss: 0.015350  acc: 100.0000%(16/16)
Batch[57] - loss: 0.017570  acc: 100.0000%(16/16)
Batch[58] - loss: 0.013882  acc: 100.0000%(16/16)
Batch[59] - loss: 0.015793  acc: 100.0000%(16/16)
Batch[60] - loss: 0.013739  acc: 100.0000%(16/16)
Batch[61] - loss: 0.022319  acc: 100.0000%(16/16)
Batch[62] - loss: 0.010103  acc: 100.0000%(13/13)
Average loss:0.020166 average acc:99.000000%
Reload the best model...
0.00025
Val set acc: 0.7986577181208053
Best val set acc: 0.7986577181208053

Epoch  17 / 18
Batch[0] - loss: 0.084368  acc: 93.0000%(15/16)
Batch[1] - loss: 0.020394  acc: 100.0000%(16/16)
Batch[2] - loss: 0.009392  acc: 100.0000%(16/16)
Batch[3] - loss: 0.009737  acc: 100.0000%(16/16)
Batch[4] - loss: 0.021389  acc: 100.0000%(16/16)
Batch[5] - loss: 0.113681  acc: 93.0000%(15/16)
Batch[6] - loss: 0.018946  acc: 100.0000%(16/16)
Batch[7] - loss: 0.021605  acc: 100.0000%(16/16)
Batch[8] - loss: 0.010668  acc: 100.0000%(16/16)
Batch[9] - loss: 0.015554  acc: 100.0000%(16/16)
Batch[10] - loss: 0.026214  acc: 100.0000%(16/16)
Batch[11] - loss: 0.019602  acc: 100.0000%(16/16)
Batch[12] - loss: 0.022354  acc: 100.0000%(16/16)
Batch[13] - loss: 0.008331  acc: 100.0000%(16/16)
Batch[14] - loss: 0.041719  acc: 100.0000%(16/16)
Batch[15] - loss: 0.023015  acc: 100.0000%(16/16)
Batch[16] - loss: 0.011166  acc: 100.0000%(16/16)
Batch[17] - loss: 0.034492  acc: 100.0000%(16/16)
Batch[18] - loss: 0.039604  acc: 100.0000%(16/16)
Batch[19] - loss: 0.016271  acc: 100.0000%(16/16)
Batch[20] - loss: 0.013814  acc: 100.0000%(16/16)
Batch[21] - loss: 0.028029  acc: 100.0000%(16/16)
Batch[22] - loss: 0.007853  acc: 100.0000%(16/16)
Batch[23] - loss: 0.010273  acc: 100.0000%(16/16)
Batch[24] - loss: 0.009386  acc: 100.0000%(16/16)
Batch[25] - loss: 0.014706  acc: 100.0000%(16/16)
Batch[26] - loss: 0.010670  acc: 100.0000%(16/16)
Batch[27] - loss: 0.014253  acc: 100.0000%(16/16)
Batch[28] - loss: 0.025258  acc: 100.0000%(16/16)
Batch[29] - loss: 0.009710  acc: 100.0000%(16/16)
Batch[30] - loss: 0.034674  acc: 100.0000%(16/16)
Batch[31] - loss: 0.019298  acc: 100.0000%(16/16)
Batch[32] - loss: 0.024205  acc: 100.0000%(16/16)
Batch[33] - loss: 0.027942  acc: 100.0000%(16/16)
Batch[34] - loss: 0.018576  acc: 100.0000%(16/16)
Batch[35] - loss: 0.012403  acc: 100.0000%(16/16)
Batch[36] - loss: 0.019389  acc: 100.0000%(16/16)
Batch[37] - loss: 0.028156  acc: 100.0000%(16/16)
Batch[38] - loss: 0.014601  acc: 100.0000%(16/16)
Batch[39] - loss: 0.044702  acc: 100.0000%(16/16)
Batch[40] - loss: 0.081900  acc: 93.0000%(15/16)
Batch[41] - loss: 0.012388  acc: 100.0000%(16/16)
Batch[42] - loss: 0.012613  acc: 100.0000%(16/16)
Batch[43] - loss: 0.009019  acc: 100.0000%(16/16)
Batch[44] - loss: 0.020666  acc: 100.0000%(16/16)
Batch[45] - loss: 0.018353  acc: 100.0000%(16/16)
Batch[46] - loss: 0.018045  acc: 100.0000%(16/16)
Batch[47] - loss: 0.020063  acc: 100.0000%(16/16)
Batch[48] - loss: 0.030355  acc: 100.0000%(16/16)
Batch[49] - loss: 0.019647  acc: 100.0000%(16/16)
Batch[50] - loss: 0.013977  acc: 100.0000%(16/16)
Batch[51] - loss: 0.043176  acc: 100.0000%(16/16)
Batch[52] - loss: 0.013658  acc: 100.0000%(16/16)
Batch[53] - loss: 0.040599  acc: 100.0000%(16/16)
Batch[54] - loss: 0.020346  acc: 100.0000%(16/16)
Batch[55] - loss: 0.036123  acc: 100.0000%(16/16)
Batch[56] - loss: 0.013034  acc: 100.0000%(16/16)
Batch[57] - loss: 0.037699  acc: 100.0000%(16/16)
Batch[58] - loss: 0.036651  acc: 100.0000%(16/16)
Batch[59] - loss: 0.044982  acc: 100.0000%(16/16)
Batch[60] - loss: 0.014928  acc: 100.0000%(16/16)
Batch[61] - loss: 0.022536  acc: 100.0000%(16/16)
Batch[62] - loss: 0.006961  acc: 100.0000%(13/13)
Average loss:0.024510 average acc:99.000000%
Val set acc: 0.7919463087248322
Best val set acc: 0.7986577181208053

Epoch  18 / 18
Batch[0] - loss: 0.028383  acc: 100.0000%(16/16)
Batch[1] - loss: 0.006735  acc: 100.0000%(16/16)
Batch[2] - loss: 0.013405  acc: 100.0000%(16/16)
Batch[3] - loss: 0.009318  acc: 100.0000%(16/16)
Batch[4] - loss: 0.017547  acc: 100.0000%(16/16)
Batch[5] - loss: 0.013967  acc: 100.0000%(16/16)
Batch[6] - loss: 0.022065  acc: 100.0000%(16/16)
Batch[7] - loss: 0.030376  acc: 100.0000%(16/16)
Batch[8] - loss: 0.019749  acc: 100.0000%(16/16)
Batch[9] - loss: 0.010765  acc: 100.0000%(16/16)
Batch[10] - loss: 0.017296  acc: 100.0000%(16/16)
Batch[11] - loss: 0.014878  acc: 100.0000%(16/16)
Batch[12] - loss: 0.016206  acc: 100.0000%(16/16)
Batch[13] - loss: 0.021757  acc: 100.0000%(16/16)
Batch[14] - loss: 0.016718  acc: 100.0000%(16/16)
Batch[15] - loss: 0.008364  acc: 100.0000%(16/16)
Batch[16] - loss: 0.015890  acc: 100.0000%(16/16)
Batch[17] - loss: 0.025620  acc: 100.0000%(16/16)
Batch[18] - loss: 0.009113  acc: 100.0000%(16/16)
Batch[19] - loss: 0.022384  acc: 100.0000%(16/16)
Batch[20] - loss: 0.016840  acc: 100.0000%(16/16)
Batch[21] - loss: 0.012030  acc: 100.0000%(16/16)
Batch[22] - loss: 0.009438  acc: 100.0000%(16/16)
Batch[23] - loss: 0.032006  acc: 100.0000%(16/16)
Batch[24] - loss: 0.005967  acc: 100.0000%(16/16)
Batch[25] - loss: 0.018921  acc: 100.0000%(16/16)
Batch[26] - loss: 0.017364  acc: 100.0000%(16/16)
Batch[27] - loss: 0.010219  acc: 100.0000%(16/16)
Batch[28] - loss: 0.031815  acc: 100.0000%(16/16)
Batch[29] - loss: 0.010003  acc: 100.0000%(16/16)
Batch[30] - loss: 0.012069  acc: 100.0000%(16/16)
Batch[31] - loss: 0.057538  acc: 100.0000%(16/16)
Batch[32] - loss: 0.013489  acc: 100.0000%(16/16)
Batch[33] - loss: 0.160454  acc: 93.0000%(15/16)
Batch[34] - loss: 0.022579  acc: 100.0000%(16/16)
Batch[35] - loss: 0.030767  acc: 100.0000%(16/16)
Batch[36] - loss: 0.005371  acc: 100.0000%(16/16)
Batch[37] - loss: 0.016093  acc: 100.0000%(16/16)
Batch[38] - loss: 0.020473  acc: 100.0000%(16/16)
Batch[39] - loss: 0.036720  acc: 100.0000%(16/16)
Batch[40] - loss: 0.007342  acc: 100.0000%(16/16)
Batch[41] - loss: 0.015777  acc: 100.0000%(16/16)
Batch[42] - loss: 0.051510  acc: 100.0000%(16/16)
Batch[43] - loss: 0.016710  acc: 100.0000%(16/16)
Batch[44] - loss: 0.016280  acc: 100.0000%(16/16)
Batch[45] - loss: 0.025312  acc: 100.0000%(16/16)
Batch[46] - loss: 0.007331  acc: 100.0000%(16/16)
Batch[47] - loss: 0.016786  acc: 100.0000%(16/16)
Batch[48] - loss: 0.018558  acc: 100.0000%(16/16)
Batch[49] - loss: 0.011804  acc: 100.0000%(16/16)
Batch[50] - loss: 0.017533  acc: 100.0000%(16/16)
Batch[51] - loss: 0.058365  acc: 100.0000%(16/16)
Batch[52] - loss: 0.016486  acc: 100.0000%(16/16)
Batch[53] - loss: 0.012343  acc: 100.0000%(16/16)
Batch[54] - loss: 0.008311  acc: 100.0000%(16/16)
Batch[55] - loss: 0.011531  acc: 100.0000%(16/16)
Batch[56] - loss: 0.027806  acc: 100.0000%(16/16)
Batch[57] - loss: 0.014242  acc: 100.0000%(16/16)
Batch[58] - loss: 0.014365  acc: 100.0000%(16/16)
Batch[59] - loss: 0.026383  acc: 100.0000%(16/16)
Batch[60] - loss: 0.027501  acc: 100.0000%(16/16)
Batch[61] - loss: 0.013886  acc: 100.0000%(16/16)
Batch[62] - loss: 0.013254  acc: 100.0000%(13/13)
Average loss:0.021113 average acc:99.000000%
Val set acc: 0.7785234899328859
Best val set acc: 0.7986577181208053
================================
              precision    recall  f1-score   support

          NR      0.865     0.988     0.922        84
          FR      0.975     0.917     0.945        84
          TR      0.938     0.893     0.915        84
          UR      0.951     0.917     0.933        84

    accuracy                          0.929       336
   macro avg      0.932     0.929     0.929       336
weighted avg      0.932     0.929     0.929       336

